======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/__init__.py ---
======================================================================

# enkibot/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file intentionally left blank.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/app.py ---
======================================================================

# enkibot/app.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import logging
from datetime import timedelta, time as dtime
from telegram import Update
from telegram.ext import Application, ContextTypes

from enkibot import config
from enkibot.utils.database import DatabaseManager
from enkibot.core.llm_services import LLMServices
from enkibot.core.language_service import LanguageService
from enkibot.modules.intent_recognizer import IntentRecognizer
from enkibot.modules.profile_manager import ProfileManager
from enkibot.modules.api_router import ApiRouter
from enkibot.modules.response_generator import ResponseGenerator
from enkibot.core.telegram_handlers import TelegramHandlerService
from enkibot.modules.karma_manager import KarmaManager
from enkibot.modules.spam_detector import SpamDetector
from enkibot.modules.stats_manager import StatsManager
from enkibot.modules.community_moderation import CommunityModerationService
from enkibot.modules.fact_check import (
    FactChecker,
    FactCheckBot,
    SatireDetector,
    StanceModel,
    OpenAIWebFetcher,
)
from enkibot.modules.primary_source_hunter import PrimarySourceHunter

logger = logging.getLogger(__name__)

class EnkiBotApplication:
    def __init__(self, ptb_application: Application):
        logger.info("EnkiBotApplication initializing...")
        self.ptb_application = ptb_application

        # Initialize core services
        self.db_manager = DatabaseManager(config.DB_CONNECTION_STRING)
        self.llm_services = LLMServices(
            openai_api_key=config.OPENAI_API_KEY, openai_model_id=config.OPENAI_MODEL_ID,
            groq_api_key=config.GROQ_API_KEY, groq_model_id=config.GROQ_MODEL_ID, groq_endpoint_url=config.GROQ_ENDPOINT_URL,
            openrouter_api_key=config.OPENROUTER_API_KEY, openrouter_model_id=config.OPENROUTER_MODEL_ID, openrouter_endpoint_url=config.OPENROUTER_ENDPOINT_URL,
            google_ai_api_key=config.GOOGLE_AI_API_KEY, google_ai_model_id=config.GOOGLE_AI_MODEL_ID
        )
        self.language_service = LanguageService(
            llm_services=self.llm_services, 
            db_manager=self.db_manager # Pass db_manager for fetching chat history
        )
        
        # Initialize functional modules/services
        self.intent_recognizer = IntentRecognizer(self.llm_services)
        self.profile_manager = ProfileManager(self.llm_services, self.db_manager)
        self.api_router = ApiRouter(
            weather_api_key=config.WEATHER_API_KEY,
            news_api_key=config.NEWS_API_KEY,
            llm_services=self.llm_services,
            db_manager=self.db_manager,
        )
        self.response_generator = ResponseGenerator(
            self.llm_services,
            self.db_manager,
            self.intent_recognizer
        )
        self.spam_detector = SpamDetector(
            self.llm_services,
            self.db_manager,
            self.language_service,
            enabled=config.ENABLE_SPAM_DETECTION,
        )
        self.stats_manager = StatsManager(self.db_manager)
        self.karma_manager = KarmaManager(self.db_manager)
        self.community_moderation = CommunityModerationService(
            self.language_service,
            admin_chat_id=config.REPORTS_CHANNEL_ID,
        )

        # Initialize Telegram handlers, passing all necessary services
        self.handler_service = TelegramHandlerService(
            application=self.ptb_application,
            db_manager=self.db_manager,
            llm_services=self.llm_services,
            intent_recognizer=self.intent_recognizer,
            profile_manager=self.profile_manager,
            api_router=self.api_router,
            response_generator=self.response_generator,
            language_service=self.language_service,
            spam_detector=self.spam_detector,
            stats_manager=self.stats_manager,
            karma_manager=self.karma_manager,
            community_moderation=self.community_moderation,
            allowed_group_ids=config.ALLOWED_GROUP_IDS, # Pass as set
            bot_nicknames=config.BOT_NICKNAMES_TO_CHECK # Pass as list
        )

        # Allow the spam detector to trigger captcha challenges via the
        # Telegram handler service.
        self.spam_detector.set_captcha_callback(
            self.handler_service.start_captcha
        )

        # ------------------------------------------------------------------
        # Fact checking subsystem (skeleton implementation)
        # ------------------------------------------------------------------
        self.fact_checker = FactChecker(
            fetcher=OpenAIWebFetcher(),
            stance=StanceModel(),
            llm_services=self.llm_services,
            primary_hunter=PrimarySourceHunter(),
        )

        def _default_fact_cfg(_chat_id: int) -> dict:
            return {"satire": {"enabled": False}, "auto": {"auto_check_news": True}}

        self.fact_check_bot = FactCheckBot(
            app=self.ptb_application,
            fc=self.fact_checker,
            satire_detector=SatireDetector(_default_fact_cfg),
            cfg_reader=_default_fact_cfg,
            db_manager=self.db_manager,
            language_service=self.language_service,
        )
        self.fact_check_bot.register()

        async def _refresh_news_channels_job(_: ContextTypes.DEFAULT_TYPE) -> None:
            logger.info("News channel refresh job started")
            await self.db_manager.refresh_news_channels()
            logger.info("News channel refresh job completed")

        if getattr(self.ptb_application, "job_queue", None):
            logger.info("Scheduling news channel refresh job")
            self.ptb_application.job_queue.run_once(
                _refresh_news_channels_job, when=0
            )
            # Run the refresh on the first day of each month at midnight.
            self.ptb_application.job_queue.run_monthly(
                _refresh_news_channels_job,
                when=dtime(hour=0, minute=0),
                day=1,
            )
        else:
            logger.warning(
                "JobQueue is not available. Running one-off news channel refresh."
            )
            import asyncio

            asyncio.run(self.db_manager.refresh_news_channels())

        logger.info("EnkiBotApplication initialized all services.")

    def register_handlers(self):
        """Registers all Telegram handlers."""
        self.handler_service.register_all_handlers()
        logger.info("EnkiBotApplication: All handlers registered with PTB Application.")

    def run(self):
        """Starts the bot polling."""
        logger.info("EnkiBotApplication: Starting PTB Application polling...")
        self.ptb_application.run_polling(allowed_updates=Update.ALL_TYPES)
        logger.info("EnkiBotApplication: Polling stopped.")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/combine_files.py ---
======================================================================

# enkibot/combine_files.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
import os
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import logging

# --- Configuration ---
# The root directory of your project source code.
# IMPORTANT: Please ensure this path is correct for your system.
PROJECT_ROOT = r'c:\Projects\EnkiBot\EnkiBot\EnkiBot'
# The name of the file that will contain all the combined code.
OUTPUT_FILENAME = 'combined_enkibot_python_source.txt' # Changed name to reflect content
# --- End Configuration ---

# Setup basic logging for the script itself.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_all_python_files(root_dir):
    """
    Walks through the directory structure and collects paths of .py files.
    
    Args:
        root_dir (str): The starting directory to walk.
        
    Returns:
        list: A sorted list of all .py file paths found.
    """
    file_paths = []
    for dirpath, _, filenames in os.walk(root_dir):
        # Exclude __pycache__ directories
        if '__pycache__' in dirpath:
            continue
        for filename in filenames:
            # --- MODIFICATION IS HERE ---
            # Only include files that end with the .py extension.
            if filename.endswith(('.py','.json')):
                file_paths.append(os.path.join(dirpath, filename))
                
    return sorted(file_paths)

def combine_project_files(root_dir, output_file):
    """
    Reads all .py files from a project directory and writes their contents
    into a single output file, with headers for each file.
    
    Args:
        root_dir (str): The root directory of the project to combine.
        output_file (str): The path to the single output file.
    """
    logging.info(f"Starting to combine only .py files from project root: '{root_dir}'")
    
    all_files = get_all_python_files(root_dir)
    
    if not all_files:
        logging.error(f"No .py files found in '{root_dir}'. Please check the PROJECT_ROOT path.")
        return

    try:
        # Open the output file in write mode with UTF-8 encoding
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for file_path in all_files:
                # Normalize path for consistent display
                normalized_path = file_path.replace('\\', '/')
                header = f"======================================================================\n"
                header += f"--- File: {normalized_path} ---\n"
                header += f"======================================================================\n\n"
                
                outfile.write(header)
                logging.info(f"Processing: {normalized_path}")
                
                try:
                    # Open the source file in read mode with UTF-8 encoding
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(infile.read())
                    # Add newlines for separation between files
                    outfile.write("\n\n\n")
                except Exception as e:
                    # Handle potential read errors
                    error_message = f"*** ERROR: Could not read file. Reason: {e} ***\n\n\n"
                    outfile.write(error_message)
                    logging.warning(f"Could not read {file_path}: {e}")

    except IOError as e:
        logging.error(f"Fatal error writing to output file '{output_file}': {e}")
        return

    logging.info(f"Successfully combined {len(all_files)} .py files into '{output_file}'.")

if __name__ == '__main__':
    if not os.path.isdir(PROJECT_ROOT):
        print(f"Error: The project directory '{PROJECT_ROOT}' was not found.")
        print("Please make sure the PROJECT_ROOT path is correct and you are running this script from a location that can access it.")
    else:
        combine_project_files(PROJECT_ROOT, OUTPUT_FILENAME)



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/config.py ---
======================================================================

﻿# enkibot/config.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Configuration ===
# ==================================================================================================
# Central configuration file for API keys, model IDs, database settings, and other constants.
# It is best practice to load sensitive information from environment variables.
# ==================================================================================================

import os
import logging

# --- Core Bot Settings ---
TELEGRAM_BOT_TOKEN = os.getenv('ENKI_BOT_TOKEN')
# A list of bot nicknames that trigger a response in group chats.
BOT_NICKNAMES_TO_CHECK = ["enki", "энки", "енки"]

# Feature toggles
ENABLE_SPAM_DETECTION = os.getenv('ENKI_BOT_ENABLE_SPAM_DETECTION', 'true').lower() == 'true'
CAPTCHA_TIMEOUT_SECONDS = int(os.getenv('ENKI_BOT_CAPTCHA_TIMEOUT_SECONDS', '60'))
CAPTCHA_MAX_ATTEMPTS = int(os.getenv('ENKI_BOT_CAPTCHA_MAX_ATTEMPTS', '3'))

# Usage quotas
DAILY_LLM_QUOTA = int(os.getenv('ENKI_BOT_DAILY_LLM_QUOTA', '100'))
DAILY_IMAGE_QUOTA = int(os.getenv('ENKI_BOT_DAILY_IMAGE_QUOTA', '20'))

# Zero-Trust moderation configuration. These values control how the spam
# detector treats new or unverified users and how it scores their first
# messages. All values can be overridden with environment variables.
ZERO_TRUST_SETTINGS = {
    "watch_new_user_window_sec": int(os.getenv('ENKI_BOT_ZT_NEW_USER_WINDOW', '3600')),
    "watch_first_messages": int(os.getenv('ENKI_BOT_ZT_FIRST_MESSAGES', '5')),
    "global_thresholds": {
        "delete": float(os.getenv('ENKI_BOT_ZT_DELETE_THRESHOLD', '0.75')),
        "ban": float(os.getenv('ENKI_BOT_ZT_BAN_THRESHOLD', '0.95')),
        "mute_then_captcha": float(os.getenv('ENKI_BOT_ZT_MUTE_THRESHOLD', '0.80')),
    },
    "heuristics": {
        "url_in_first_msg": float(os.getenv('ENKI_BOT_ZT_URL_RISK', '0.10')),
        "many_mentions": float(os.getenv('ENKI_BOT_ZT_MANY_MENTIONS_RISK', '0.10')),
        "all_caps_long": float(os.getenv('ENKI_BOT_ZT_ALL_CAPS_RISK', '0.10')),
        "repeated_chars": float(os.getenv('ENKI_BOT_ZT_REPEAT_RISK', '0.05')),
        "keyword_hits": float(os.getenv('ENKI_BOT_ZT_KEYWORD_RISK', '0.15')),
    },
    "lists": {
        "domain_blocklist": set(filter(None, os.getenv(
            'ENKI_BOT_ZT_DOMAIN_BLOCKLIST', 't.me/joinchat,bit.ly,cutt.ly'
        ).split(','))),
        "keyword_blocklist": set(filter(None, os.getenv(
            'ENKI_BOT_ZT_KEYWORD_BLOCKLIST',
            'free crypto,giveaway,xxx,adult cam,telegram view,loan approval,work from home $,forex vip'
        ).split(','))),
    },
    "logging": {
        "admin_chat_id": int(os.getenv('ENKI_BOT_ZT_ADMIN_CHAT_ID'))
        if os.getenv('ENKI_BOT_ZT_ADMIN_CHAT_ID')
        else None,
    },
}

# Community moderation settings
DEFAULT_SPAM_VOTE_THRESHOLD = int(os.getenv('ENKI_BOT_SPAM_VOTE_THRESHOLD', '3'))
SPAM_VOTE_TIME_WINDOW_MINUTES = int(os.getenv('ENKI_BOT_SPAM_VOTE_WINDOW_MINUTES', '60'))
REPORTS_CHANNEL_ID = int(os.getenv('ENKI_BOT_REPORTS_CHANNEL_ID')) if os.getenv('ENKI_BOT_REPORTS_CHANNEL_ID') else None

# NSFW filtering
NSFW_FILTER_DEFAULT_ENABLED = os.getenv('ENKI_BOT_NSFW_FILTER_DEFAULT', 'false').lower() == 'true'
NSFW_DETECTION_THRESHOLD = float(os.getenv('ENKI_BOT_NSFW_THRESHOLD', '0.8'))

# Timeout settings for Telegram HTTP requests (in seconds).
TELEGRAM_CONNECT_TIMEOUT = float(os.getenv('ENKI_BOT_TELEGRAM_CONNECT_TIMEOUT', '30'))
TELEGRAM_READ_TIMEOUT = float(os.getenv('ENKI_BOT_TELEGRAM_READ_TIMEOUT', '30'))
# The write timeout is rarely distinct from the read timeout, but httpx
# requires it to be provided when specifying individual timeouts. Use the
# read timeout as the default if no explicit environment variable is set.
TELEGRAM_WRITE_TIMEOUT = float(
    os.getenv('ENKI_BOT_TELEGRAM_WRITE_TIMEOUT', str(TELEGRAM_READ_TIMEOUT))
)
# httpx also expects a pool timeout when customising timeouts. Default to the
# connect timeout so existing behaviour is preserved without requiring an
# additional setting from the user.
TELEGRAM_POOL_TIMEOUT = float(
    os.getenv('ENKI_BOT_TELEGRAM_POOL_TIMEOUT', str(TELEGRAM_CONNECT_TIMEOUT))
)

# --- Database Configuration (MS SQL Server) ---
SQL_SERVER_NAME = os.getenv('ENKI_BOT_SQL_SERVER_NAME')
SQL_DATABASE_NAME = os.getenv('ENKI_BOT_SQL_DATABASE_NAME')
DB_CONNECTION_STRING = (
    f"DRIVER={{ODBC Driver 17 for SQL Server}};"
    f"SERVER={SQL_SERVER_NAME};"
    f"DATABASE={SQL_DATABASE_NAME};"
    f"Trusted_Connection=yes;"
) if SQL_SERVER_NAME and SQL_DATABASE_NAME else None

# --- LLM Provider API Keys & Models ---
# OpenAI
OPENAI_API_KEY = os.getenv('ENKI_BOT_OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MODEL_ID', 'gpt-4.1-mini')                 # General orchestrator
# Default deep‑research model. ``gpt-4o-mini`` works for most accounts with
# built‑in web search. Override with ``o3-deep-research`` or another supported
# model via the environment variable if you have access.
OPENAI_DEEP_RESEARCH_MODEL_ID = os.getenv(
    'ENKI_BOT_OPENAI_DEEP_RESEARCH_MODEL_ID', 'gpt-4o-mini'
)
OPENAI_EMBEDDING_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_EMBEDDING_MODEL_ID', 'text-embedding-3-large')
OPENAI_CLASSIFICATION_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_CLASSIFICATION_MODEL_ID', 'gpt-3.5-turbo') # For faster tasks like intent classification
OPENAI_TRANSLATION_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_TRANSLATION_MODEL_ID', 'gpt-4o-mini')      # For language pack creation
OPENAI_MULTIMODAL_IMAGE_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MULTIMODAL_IMAGE_MODEL_ID', 'gpt-4o')
OPENAI_DALLE_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_DALLE_MODEL_ID', 'dall-e-3')
OPENAI_WHISPER_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_WHISPER_MODEL_ID', 'whisper-1')
OPENAI_SEARCH_CONTEXT_SIZE = os.getenv('ENKI_BOT_OPENAI_SEARCH_CONTEXT_SIZE')
OPENAI_SEARCH_USER_LOCATION = os.getenv('ENKI_BOT_OPENAI_SEARCH_USER_LOCATION')
DEFAULT_IMAGE_N = 1
DEFAULT_IMAGE_SIZE = "1024x1024"
DEFAULT_IMAGE_QUALITY = "standard"
# Groq
GROQ_API_KEY = os.getenv('ENKI_BOT_GROQ_API_KEY')
GROQ_MODEL_ID = os.getenv('ENKI_BOT_GROQ_MODEL_ID', 'llama-3.3-70b-versatile')
GROQ_ENDPOINT_URL = "https://api.groq.com/openai/v1/chat/completions"

# OpenRouter
OPENROUTER_API_KEY = os.getenv('ENKI_BOT_OPENROUTER_API_KEY')
OPENROUTER_MODEL_ID = os.getenv('ENKI_BOT_OPENROUTER_MODEL_ID', 'mistralai/mistral-7b-instruct:free')
OPENROUTER_ENDPOINT_URL = "https://openrouter.ai/api/v1/chat/completions"

# Google AI
GOOGLE_AI_API_KEY = os.getenv('ENKI_BOT_GOOGLE_AI_API_KEY')
GOOGLE_AI_MODEL_ID = os.getenv('ENKI_BOT_GOOGLE_AI_MODEL_ID', 'gemini-1.5-flash-latest')

# Estimated cost per 1K tokens for tracking purposes (USD)
OPENAI_COST_PER_1K_TOKENS = float(os.getenv('ENKI_BOT_OPENAI_COST_PER_1K_TOKENS', '0'))
GROQ_COST_PER_1K_TOKENS = float(os.getenv('ENKI_BOT_GROQ_COST_PER_1K_TOKENS', '0'))
OPENROUTER_COST_PER_1K_TOKENS = float(os.getenv('ENKI_BOT_OPENROUTER_COST_PER_1K_TOKENS', '0'))

# --- External Service API Keys ---
NEWS_API_KEY = os.getenv('ENKI_BOT_NEWS_API_KEY')
WEATHER_API_KEY = os.getenv('ENKI_BOT_WEATHER_API_KEY')

# --- Group Chat Access Control ---
ALLOWED_GROUP_IDS_STR = os.getenv('ENKI_BOT_ALLOWED_GROUP_IDS')
ALLOWED_GROUP_IDS = set()
if ALLOWED_GROUP_IDS_STR:
    try:
        ALLOWED_GROUP_IDS = set(int(id_str.strip()) for id_str in ALLOWED_GROUP_IDS_STR.split(','))
        if ALLOWED_GROUP_IDS:
            logging.info(f"Bot access is restricted to group IDs: {ALLOWED_GROUP_IDS}")
    except ValueError:
        logging.error(f"Invalid format for ENKI_BOT_ALLOWED_GROUP_IDS: '{ALLOWED_GROUP_IDS_STR}'. No group restrictions applied.")
else:
    logging.info("ENKI_BOT_ALLOWED_GROUP_IDS is not set. The bot will operate in all groups.")

# --- Language and Prompts Configuration ---
# The default language to use if detection fails.
DEFAULT_LANGUAGE = "en"
# The directory where language-specific prompt files (e.g., en.json, ru.json) are stored.
LANGUAGE_PACKS_DIR = os.path.join(os.path.dirname(__file__), 'lang')



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/__init__.py ---
======================================================================

# enkibot/core/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# This file makes the 'core' directory a Python package.



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/general_handler.py ---
======================================================================


# enkibot/core/intent_handlers/general_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------

import logging
from typing import Optional, TYPE_CHECKING

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
from telegram.constants import ChatAction

from enkibot.utils.message_utils import is_forwarded_message, clean_output_text
from enkibot.utils.quota_middleware import enforce_user_quota
from enkibot.utils.text_splitter import split_text_into_chunks

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.response_generator import ResponseGenerator
    # from enkibot.utils.database import DatabaseManager # If direct DB access is needed later

logger = logging.getLogger(__name__)

class GeneralIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 response_generator: 'ResponseGenerator'
                 # db_manager: Optional['DatabaseManager'] = None # Add if needed
                 ):
        logger.info("GeneralIntentHandler __init__ STARTING")
        self.language_service = language_service
        self.response_generator = response_generator
        # self.db_manager = db_manager
        logger.info("GeneralIntentHandler __init__ COMPLETED")

    async def handle_request(self, 
                             update: Update, 
                             context: ContextTypes.DEFAULT_TYPE, 
                             user_msg_txt: str, 
                             master_intent: str) -> None:
        """
        Handles USER_PROFILE_QUERY, GENERAL_CHAT, and UNKNOWN_INTENT,
        and can serve as a fallback for other intents if needed.
        """
        if not update.message or not update.effective_chat or not update.effective_user:
            logger.warning("GeneralIntentHandler.handle_request called with invalid update/context.")
            return

        logger.info(f"GeneralIntentHandler: Handling intent '{master_intent}' for: '{user_msg_txt[:70]}...'")
        if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
            await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
            return
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)

        if is_forwarded_message(update.message):
            analyzer_prompts = self.language_service.get_llm_prompt_set("forwarded_news_fact_checker")
            if analyzer_prompts and "system" in analyzer_prompts:
                forwarded_text = update.message.text or update.message.caption or ""
                # Detect the language of the forwarded text so the research happens in that language
                await self.language_service.determine_language_context(
                    forwarded_text, update.effective_chat.id
                )

                question = user_msg_txt
                if question == forwarded_text or len(question.strip()) < 5:
                    question = self.language_service.get_response_string(
                        "replied_message_default_question"
                    )
                fact_check_result = await self.response_generator.fact_check_forwarded_message(
                    forwarded_text=forwarded_text,
                    user_question=question,
                    system_prompt=analyzer_prompts["system"],
                    user_prompt_template=analyzer_prompts.get("user_template"),
                    fallback_text=self.language_service.get_response_string(
                        "analysis_unavailable", "Analysis unavailable."
                    ),
                )
                fact_check_result = clean_output_text(fact_check_result)
                if not fact_check_result:
                    fact_check_result = self.language_service.get_response_string(
                        "analysis_unavailable", "Analysis unavailable."
                    )
                for chunk in split_text_into_chunks(fact_check_result):
                    await update.message.reply_text(chunk)
            else:
                logger.error("Prompt set for forwarded news fact-check is missing or malformed.")
                await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return

        main_orchestrator_prompts = self.language_service.get_llm_prompt_set("main_orchestrator")
        system_prompt_override = "You are EnkiBot, a helpful and friendly AI assistant."
        
        if main_orchestrator_prompts and "system" in main_orchestrator_prompts:
            system_prompt_template = main_orchestrator_prompts["system"]
            language_name_for_prompt = self.language_service.current_lang 
            try: 
                from babel import Locale
                locale = Locale.parse(self.language_service.current_lang)
                language_name_for_prompt = locale.get_display_name('en') # Get language name in English
            except ImportError: 
                logger.debug("Babel not installed, using ISO code as language_name in main_orchestrator prompt.")
            except Exception as e: 
                logger.warning(f"Could not get display name for lang {self.language_service.current_lang}: {e}")

            try:
                system_prompt_override = system_prompt_template.format(
                    language_name=language_name_for_prompt, 
                    lang_code=self.language_service.current_lang
                )
            except KeyError as ke:
                logger.error(f"KeyError formatting main_orchestrator system prompt: {ke}. Using template as is: '{system_prompt_template}'")
                system_prompt_override = system_prompt_template # Use unformatted if keys missing
            
            logger.info(f"GeneralIntentHandler: Using system prompt with lang instruction for {self.language_service.current_lang}")
        
        logger.info(f"GeneralIntentHandler: FINAL SYSTEM PROMPT for get_orchestrated_llm_response (lang: {self.language_service.current_lang}): '{system_prompt_override[:100]}...'")
        
        reply = await self.response_generator.get_orchestrated_llm_response(
            prompt_text=user_msg_txt, 
            chat_id=update.effective_chat.id, 
            user_id=update.effective_user.id, 
            message_id=update.message.message_id, 
            context=context, 
            lang_code=self.language_service.current_lang,
            system_prompt_override=system_prompt_override, 
            user_search_ambiguous_response_template=self.language_service.get_response_string("user_search_ambiguous_clarification"),
            user_search_not_found_response_template=self.language_service.get_response_string("user_search_not_found_in_db") 
        )
        
        if reply:

            reply = clean_output_text(reply) or reply

            keyboard = InlineKeyboardMarkup(
                [[
                    InlineKeyboardButton("\U0001F504 Regenerate", callback_data="refine:regenerate"),
                    InlineKeyboardButton("\u2795 Expand", callback_data="refine:expand"),
                    InlineKeyboardButton("\U0001F4DD Summarize", callback_data="refine:summary"),
                ]]
            )
            reply_chunks = split_text_into_chunks(reply)
            for idx, chunk in enumerate(reply_chunks):
                await update.message.reply_text(chunk, reply_markup=keyboard if idx == 0 else None)
        else:
            await update.message.reply_text(self.language_service.get_response_string("llm_error_fallback"))



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/image_generation_handler.py ---
======================================================================

﻿# enkibot/core/intent_handlers/image_generation_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import logging 
import base64 
import re
from typing import Optional, TYPE_CHECKING

from telegram import Update
# InputFile might not be strictly necessary if only sending URLs or bytes for photos
from telegram.ext import ContextTypes 
from telegram.constants import ChatAction

from enkibot import config # Import config directly
from enkibot.utils.quota_middleware import enforce_user_quota

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.core.llm_services import LLMServices
    from enkibot.utils.database import DatabaseManager

logger = logging.getLogger(__name__)

class ImageGenerationIntentHandler:
    def __init__(self,
                 language_service: 'LanguageService',
                 intent_recognizer: 'IntentRecognizer',
                 llm_services: 'LLMServices',
                 db_manager: 'DatabaseManager'):
        logger.info("ImageGenerationIntentHandler initialized.")
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer
        self.llm_services = llm_services
        self.db_manager = db_manager

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        if not update.message or not update.effective_chat or not update.effective_user:
            return

        if not await enforce_user_quota(self.db_manager, update.effective_user.id, "image"):
            await update.message.reply_text(self.language_service.get_response_string("image_quota_exceeded"))
            return

        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.UPLOAD_PHOTO)
        preliminary_reply = await update.message.reply_text(
            self.language_service.get_response_string("image_generation_start")
        )

        extractor_prompts = self.language_service.get_llm_prompt_set("image_generation_prompt_extractor")
        clean_prompt: Optional[str] = None
        
        if extractor_prompts and "system" in extractor_prompts and extractor_prompts.get("user_template"):
            clean_prompt = await self.intent_recognizer.extract_image_prompt_with_llm(
                text=user_msg_txt,
                lang_code=self.language_service.current_lang,
                system_prompt=extractor_prompts["system"],
                user_prompt_template=extractor_prompts["user_template"]
            )
        else:
            logger.error("Image generation prompt extractor prompts are missing or malformed (expecting 'user_template')!")
            # Basic fallback prompt cleaning
            bot_nicknames_to_check = config.BOT_NICKNAMES_TO_CHECK 
            bot_name_pattern = r"(?i)\b(?:{})\b\s*[:,]?\s*".format("|".join(re.escape(name) for name in bot_nicknames_to_check))
            cleaned_text_intermediate = re.sub(bot_name_pattern, "", user_msg_txt, count=1).strip()
            triggers_to_remove = ["draw", "create", "generate", "image of", "picture of", "сделай картинку", "нарисуй", "создай", "сгенерируй", "картинку", "изображение"]
            for trigger in triggers_to_remove:
                cleaned_text_intermediate = re.sub(r'(?i)\b' + re.escape(trigger) + r'\b\s*', '', cleaned_text_intermediate, count=1).strip()
            cleaned_text_intermediate = re.sub(r"^(can you|could you|please|i want|i need|дай мне|хочу)\s*", "", cleaned_text_intermediate, flags=re.IGNORECASE).strip()
            cleaned_text_intermediate = re.sub(r"[?.!]$", "", cleaned_text_intermediate).strip()
            if cleaned_text_intermediate and len(cleaned_text_intermediate) > 2: 
                 clean_prompt = cleaned_text_intermediate
            else:
                 logger.warning("Fallback prompt cleaning also resulted in no usable prompt.")


        if not clean_prompt:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=preliminary_reply.message_id,
                text=self.language_service.get_response_string("image_generation_no_prompt")
            )
            return

        try:
            generated_images_data = await self.llm_services.generate_image_openai(
                prompt=clean_prompt,
                n=config.DEFAULT_IMAGE_N,
                size=config.DEFAULT_IMAGE_SIZE,
                quality=config.DEFAULT_IMAGE_QUALITY,
                response_format="url"
            )
            
            await context.bot.delete_message(chat_id=update.effective_chat.id, message_id=preliminary_reply.message_id)

            if generated_images_data:
                caption_key = "image_generation_success_single" if len(generated_images_data) == 1 else "image_generation_success_multiple"
                
                for i, img_data in enumerate(generated_images_data):
                    current_caption = self.language_service.get_response_string(caption_key, image_prompt=clean_prompt) if i == 0 else None
                    if img_data.get("url"):
                        logger.info(f"DALL-E API returned a URL. Sending photo to user.")
                        await update.message.reply_photo(photo=img_data["url"], caption=current_caption)
                    elif img_data.get("b64_json"): 
                        logger.info("Image data returned as b64_json. Decoding and sending photo.")
                        image_bytes = base64.b64decode(img_data["b64_json"])
                        await update.message.reply_photo(photo=image_bytes, caption=current_caption)
                    else: 
                        logger.error(f"Image generation successful but no URL or b64_json data found for prompt: {clean_prompt}")
                        await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))

            else: 
                logger.error(f"Image generation failed for prompt: {clean_prompt} (no data returned from service or service call failed)")
                await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))

        except Exception as e:
            logger.error(f"An exception occurred while handling image generation: {e}", exc_info=True)
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=preliminary_reply.message_id,
                    text=self.language_service.get_response_string("image_generation_error")
                )
            except Exception as edit_e: 
                logger.error(f"Failed to edit preliminary message during image error handling: {edit_e}")
                await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/news_handler.py ---
======================================================================

# enkibot/core/intent_handlers/news_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import logging
from typing import Optional, Dict, Any, List, TYPE_CHECKING

from telegram import Update
from telegram.ext import ContextTypes, ConversationHandler
from telegram.constants import ChatAction
from enkibot.utils.quota_middleware import enforce_user_quota
from enkibot.utils.text_splitter import split_text_into_chunks
from enkibot.utils.message_utils import clean_output_text

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator

logger = logging.getLogger(__name__)

# This state constant should ideally be defined in or imported from a central location
# to avoid magic numbers. For now, ensure it matches telegram_handlers.py.
ASK_NEWS_TOPIC = 2 

class NewsIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer',
                 api_router: 'ApiRouter',
                 response_generator: 'ResponseGenerator',
                 pending_action_data_ref: Dict[int, Dict[str, Any]]):
        logger.info("NewsIntentHandler initialized.")
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer
        self.api_router = api_router
        self.response_generator = response_generator
        self.pending_action_data = pending_action_data_ref # Reference to shared dict

    async def _process_news_request(self, update: Update, context: ContextTypes.DEFAULT_TYPE, topic: Optional[str], original_message_id: Optional[int] = None) -> int:
        """
        Fetches, compiles, and sends news. Central logic after topic is known.
        Returns ConversationHandler.END.
        """
        if not update.message or not update.effective_chat: 
            logger.warning("NewsHandler._process_news_request called with invalid update/context.")
            return ConversationHandler.END

        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        logger.info(f"NewsHandler: Processing news for topic: '{topic if topic else 'General'}'")
        reply_to_id = original_message_id or update.message.message_id
        
        try:
            articles_structured = await self.api_router.get_latest_news_structured(
                query=topic, 
                lang_code=self.language_service.current_lang
            )

            if articles_structured is None: # API call itself failed
                logger.error(f"NewsHandler: API call to get_latest_news_structured failed for topic '{topic}'.")
                await update.message.reply_text(self.language_service.get_response_string("news_api_error"), reply_to_message_id=reply_to_id)
            elif not articles_structured: # API call succeeded but no articles found
                no_articles_key = "news_api_no_articles" if topic else "news_api_no_general_articles"
                await update.message.reply_text(self.language_service.get_response_string(no_articles_key, query=topic or ""), reply_to_message_id=reply_to_id)
            else: # Articles found, proceed to compile
                compiler_prompts = self.language_service.get_llm_prompt_set("news_compiler")
                if not (compiler_prompts and "system" in compiler_prompts and compiler_prompts.get("user_template")):
                    logger.error("NewsHandler: News compiler LLM prompts are missing or malformed (expecting 'user_template').")
                    await update.message.reply_text(self.language_service.get_response_string("news_api_data_error"), reply_to_message_id=reply_to_id)
                else:
                    if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
                        await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
                        return ConversationHandler.END
                    compiled_response = await self.response_generator.compile_news_response(
                        articles_structured=articles_structured, topic=topic,
                        lang_code=self.language_service.current_lang,
                        system_prompt=compiler_prompts["system"],
                        user_prompt_template=compiler_prompts["user_template"]
                    )
                    compiled_response = clean_output_text(compiled_response)
                    if not compiled_response:
                        compiled_response = self.language_service.get_response_string("news_api_data_error")
                    for idx, chunk in enumerate(split_text_into_chunks(compiled_response)):
                        await update.message.reply_text(
                            chunk,
                            disable_web_page_preview=True,
                            reply_to_message_id=reply_to_id if idx == 0 else None,
                        )
        
        except Exception as e:
            logger.error(f"NewsHandler: Unhandled exception in _process_news_request for topic '{topic}': {e}", exc_info=True)
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"), reply_to_message_id=reply_to_id)
        
        finally: 
            # Clean up conversation state regardless of success or failure in processing
            chat_id = update.effective_chat.id
            if chat_id in self.pending_action_data and \
               self.pending_action_data[chat_id].get("action_type") == "ask_news_topic":
                del self.pending_action_data[chat_id]
            if context.user_data and context.user_data.get('conversation_state') == ASK_NEWS_TOPIC:
                context.user_data.pop('conversation_state')
            return ConversationHandler.END

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        """
        Handles an initial news intent (triggered by master_intent == NEWS_QUERY).
        Extracts topic or asks for it.
        """
        logger.info(f"NewsHandler: Handling initial NEWS_QUERY: '{user_msg_txt}'")
        if not update.message or not update.effective_chat: return ConversationHandler.END

        if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
            await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
            return ConversationHandler.END

        news_topic_prompts = self.language_service.get_llm_prompt_set("news_topic_extractor")
        if not (news_topic_prompts and "system" in news_topic_prompts and news_topic_prompts.get("user_template")):
            logger.error("NewsHandler: News topic extractor LLM prompts missing or malformed (expecting 'user_template').")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END
        
        topic = await self.intent_recognizer.extract_news_topic_with_llm( 
            text=user_msg_txt, 
            lang_code=self.language_service.current_lang, 
            system_prompt=news_topic_prompts["system"], 
            user_prompt_template=news_topic_prompts["user_template"] 
        )

        if topic: 
            return await self._process_news_request(update, context, topic, update.message.message_id)
        else: 
            logger.info("NewsHandler: No topic identified from initial query, asking user.")
            self.pending_action_data[update.effective_chat.id] = {
                "action_type": "ask_news_topic",
                "original_message_id": update.message.message_id
            }
            context.user_data['conversation_state'] = ASK_NEWS_TOPIC # Ensure state is set in PTB
            await update.message.reply_text(self.language_service.get_response_string("news_ask_topic"))
            return ASK_NEWS_TOPIC

    async def handle_command_entry(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        """
        Handles the /news command. This will always ask for a topic.
        """
        logger.info(f"NewsHandler: /news command received by user {update.effective_user.id if update.effective_user else 'Unknown'}")
        if not update.message or not update.effective_chat or not update.effective_user: 
            return ConversationHandler.END 
        
        # For /news, directly ask for topic.
        self.pending_action_data[update.effective_chat.id] = {
            "action_type": "ask_news_topic",
            "original_message_id": update.message.message_id
        }
        context.user_data['conversation_state'] = ASK_NEWS_TOPIC # Ensure state is set in PTB
        await update.message.reply_text(self.language_service.get_response_string("news_ask_topic"))
        return ASK_NEWS_TOPIC

    async def handle_topic_response(self, update: Update, context: ContextTypes.DEFAULT_TYPE, original_message_id: Optional[int]) -> int:
        """
        Handles the user's response when they provide a news topic after being asked.
        """
        if not update.message or not update.message.text:
            # If user sends something non-text (e.g., sticker) or empty message
            await update.message.reply_text(self.language_service.get_response_string("news_ask_topic")) # Re-ask
            return ASK_NEWS_TOPIC # Remain in the same state

        user_reply_topic_text = update.message.text.strip()
        logger.info(f"NewsHandler: Received news topic reply '{user_reply_topic_text}' in ASK_NEWS_TOPIC state.")

        extractor_prompts = self.language_service.get_llm_prompt_set("news_topic_reply_extractor")
        logger.debug(f"NewsHandler: news_topic_reply_extractor prompts: {extractor_prompts}") 
        
        if not (extractor_prompts and "system" in extractor_prompts and extractor_prompts.get("user_template")):
            logger.error("NewsHandler: News topic reply extractor LLM prompts are missing or malformed (expecting 'user_template').")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END

        if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
            await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
            return ConversationHandler.END

        extracted_topic = await self.intent_recognizer.extract_topic_from_reply(
            text=user_reply_topic_text,
            lang_code=self.language_service.current_lang,
            system_prompt=extractor_prompts["system"],
            user_prompt_template=extractor_prompts["user_template"]
        )

        # Use the extracted topic if successful, otherwise fall back to the user's raw input.
        # This handles cases where the LLM might return "None" or fail, but the user's text is still a valid topic.
        final_topic = extracted_topic or user_reply_topic_text
        logger.info(f"NewsHandler: Final topic for processing: '{final_topic}'")
        
        return await self._process_news_request(update, context, final_topic, original_message_id)



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/weather_handler.py ---
======================================================================


# enkibot/core/intent_handlers/weather_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import logging
from typing import Optional, Dict, Any, TYPE_CHECKING
from telegram import Update
from telegram.ext import ContextTypes, ConversationHandler
from telegram.constants import ChatAction
from enkibot.utils.quota_middleware import enforce_user_quota
from enkibot.utils.text_splitter import split_text_into_chunks

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator

logger = logging.getLogger(__name__)
ASK_CITY = 1 # Define state here or import from a central states definition

class WeatherIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer',
                 api_router: 'ApiRouter',
                 response_generator: 'ResponseGenerator',
                 pending_action_data_ref: Dict[int, Dict[str, Any]]): # Reference to the main pending_action_data
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer
        self.api_router = api_router
        self.response_generator = response_generator
        self.pending_action_data = pending_action_data_ref # Use the shared dictionary

    async def _process_weather_request(self, update: Update, context: ContextTypes.DEFAULT_TYPE, city: str, original_message_id: Optional[int] = None) -> int:
        # This is the _process_weather_request method moved from TelegramHandlerService
        if not update.message or not update.effective_chat: return ConversationHandler.END
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        logger.info(f"WeatherHandler: Processing weather for city: '{city}'")
        
        forecast_data = await self.api_router.get_weather_data_structured(
            location=city, 
            lang_pack_full=self.language_service.current_lang_pack_full
        )
        reply_to_id = original_message_id or update.message.message_id
        if forecast_data:
            compiler_prompts = self.language_service.get_llm_prompt_set("weather_forecast_compiler")
            if not (compiler_prompts and "system" in compiler_prompts and "user_template" in compiler_prompts):
                logger.error("Weather forecast compiler LLM prompts are missing.")
                await update.message.reply_text(self.language_service.get_response_string("weather_api_data_error", location=city), reply_to_message_id=reply_to_id)
            else:
                if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
                    await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
                    return ConversationHandler.END
                compiled_response = await self.response_generator.compile_weather_forecast_response(
                    forecast_data_structured=forecast_data,
                    lang_code=self.language_service.current_lang,
                    system_prompt=compiler_prompts["system"],
                    user_prompt_template=compiler_prompts["user_template"]
                )
                for idx, chunk in enumerate(split_text_into_chunks(compiled_response)):
                    await update.message.reply_text(chunk, reply_to_message_id=reply_to_id if idx == 0 else None)
        else:
            await update.message.reply_text(self.language_service.get_response_string("weather_city_not_found", location=city), reply_to_message_id=reply_to_id)
        return ConversationHandler.END

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        # This is the _handle_weather_intent method logic
        logger.info(f"WeatherHandler: Initial WEATHER_QUERY: '{user_msg_txt}'")
        if not update.message or not update.effective_chat: return ConversationHandler.END

        if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
            await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
            return ConversationHandler.END

        location_extract_prompts = self.language_service.get_llm_prompt_set("location_extractor")
        if not (location_extract_prompts and "system" in location_extract_prompts):
            logger.error("Location extractor LLM prompts missing.")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END

        city = await self.intent_recognizer.extract_location_with_llm(
            text=user_msg_txt, lang_code=self.language_service.current_lang,
            system_prompt=location_extract_prompts["system"], 
            user_prompt_template=location_extract_prompts.get("user","{text}")
        )
        if city: # This will be false because extract_location_with_llm returns None
            return await self._process_weather_request(update, context, city, update.message.message_id)
        else:
            logger.info("WeatherHandler: No city identified, asking user.")
            self.pending_action_data[update.effective_chat.id] = {
                "action_type": "ask_city_weather", 
                "original_message_id": update.message.message_id 
            }
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city"))
            return ASK_CITY 

    async def handle_city_response(self, update: Update, context: ContextTypes.DEFAULT_TYPE, original_message_id: Optional[int]) -> int:
        """
        Handles the user's response when they provide a city name after being asked.
        """
        if not update.message or not update.message.text:
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city"))
            return ASK_CITY # Re-ask

        user_reply_city_text = update.message.text.strip()
        logger.info(f"WeatherHandler: Received city reply '{user_reply_city_text}' in ASK_CITY state.")

        extractor_prompts = self.language_service.get_llm_prompt_set("location_reply_extractor")
        if not (extractor_prompts and "system" in extractor_prompts and "user_template" in extractor_prompts):
            logger.error("Location reply extractor LLM prompts are missing or malformed.")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END

        if not await enforce_user_quota(self.response_generator.db_manager, update.effective_user.id, "llm"):
            await update.message.reply_text(self.language_service.get_response_string("llm_quota_exceeded"))
            return ConversationHandler.END

        extracted_city = await self.intent_recognizer.extract_location_from_reply(
            text=user_reply_city_text,
            lang_code=self.language_service.current_lang,
            system_prompt=extractor_prompts["system"],
            user_prompt_template=extractor_prompts["user_template"]
        )

        if extracted_city:
            return await self._process_weather_request(update, context, extracted_city, original_message_id)
        else:
            # LLM couldn't extract a clear city from the reply
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city_failed_extraction", 
                                                                                      "Sorry, I didn't quite catch the city name. Could you please tell me the city again?"))
            return ASK_CITY # Remain in the same state, re-prompting



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/language_service.py ---
======================================================================

# enkibot/core/language_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------

import logging
import json
import os
import re
from typing import Dict, Any, Optional, List

from telegram import Update 

from enkibot import config 
from enkibot.core.llm_services import LLMServices 
from enkibot.utils.database import DatabaseManager 

logger = logging.getLogger(__name__)

class LanguageService:
    def __init__(self, 
                 llm_services: LLMServices, 
                 db_manager: DatabaseManager,
                 lang_packs_dir: str = config.LANGUAGE_PACKS_DIR, 
                 default_lang: str = config.DEFAULT_LANGUAGE):
        
        logger.info("LanguageService __init__ STARTING")
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.lang_packs_dir = lang_packs_dir
        self.default_language = default_lang
        self.primary_fallback_lang = default_lang 
        self.secondary_fallback_lang = "ru"

        self.language_packs: Dict[str, Dict[str, Any]] = {}
        self.llm_prompt_sets: Dict[str, Dict[str, Dict[str, str]]] = {}
        self.response_strings: Dict[str, Dict[str, str]] = {}
        
        self.current_lang: str = self.default_language
        self.current_llm_prompt_sets: Dict[str, Dict[str, str]] = {}
        self.current_response_strings: Dict[str, str] = {}
        self.current_lang_pack_full: Dict[str, Any] = {}

        self._load_language_packs() 
        logger.info("LanguageService __init__ COMPLETED")

    def _load_language_packs(self):
        self.language_packs = {}
        self.llm_prompt_sets = {}
        self.response_strings = {}
        if not os.path.exists(self.lang_packs_dir):
            logger.error(f"Language packs directory not found: {self.lang_packs_dir}")
            try:
                os.makedirs(self.lang_packs_dir, exist_ok=True)
                logger.info(f"Created language packs directory: {self.lang_packs_dir}")
            except OSError as e:
                logger.error(f"Could not create language packs directory {self.lang_packs_dir}: {e}")
        
        for lang_file in os.listdir(self.lang_packs_dir):
            if lang_file.endswith(".json"):
                lang_code = lang_file[:-5]
                file_path = os.path.join(self.lang_packs_dir, lang_file)
                try:
                    with open(file_path, 'r', encoding='utf-8-sig') as f: 
                        pack_content = json.load(f)
                        self.language_packs[lang_code] = pack_content
                        self.llm_prompt_sets[lang_code] = pack_content.get("prompts", {})
                        self.response_strings[lang_code] = pack_content.get("responses", {})
                        logger.info(f"Successfully loaded language pack: {lang_code} from {file_path}")
                except json.JSONDecodeError as jde:
                    logger.error(f"Error decoding JSON from language file: {lang_file}. Error: {jde.msg} at L{jde.lineno} C{jde.colno} (char {jde.pos})", exc_info=False)
                except Exception as e:
                    logger.error(f"Error loading language file {lang_file}: {e}", exc_info=True)
        
        self._set_current_language_internals(self.default_language)

    def _set_current_language_internals(self, lang_code_to_set: str):
        chosen_lang_code = lang_code_to_set
        if chosen_lang_code not in self.language_packs:
            logger.warning(f"Language pack for initially requested '{chosen_lang_code}' not found.")
            if self.primary_fallback_lang in self.language_packs:
                logger.info(f"Falling back to primary fallback: '{self.primary_fallback_lang}'.")
                chosen_lang_code = self.primary_fallback_lang
            elif self.secondary_fallback_lang in self.language_packs:
                logger.info(f"Primary fallback '{self.primary_fallback_lang}' not found. Falling back to secondary: '{self.secondary_fallback_lang}'.")
                chosen_lang_code = self.secondary_fallback_lang
            elif self.language_packs: 
                first_available = next(iter(self.language_packs))
                logger.error(f"Fallbacks ('{self.primary_fallback_lang}', '{self.secondary_fallback_lang}') not found. Using first available: '{first_available}'.")
                chosen_lang_code = first_available
            else: 
                logger.critical("CRITICAL: No language packs loaded at all. Service may be impaired.")
                self.current_lang = "none" 
                self.current_lang_pack_full = {}
                self.current_llm_prompt_sets = {}
                self.current_response_strings = {}
                return

        self.current_lang = chosen_lang_code
        self.current_lang_pack_full = self.language_packs.get(chosen_lang_code, {})
        self.current_llm_prompt_sets = self.llm_prompt_sets.get(chosen_lang_code, {})
        self.current_response_strings = self.response_strings.get(chosen_lang_code, {})
        
        if not self.current_llm_prompt_sets and not self.current_response_strings:
             logger.error(f"Language '{self.current_lang}' pack loaded, but it seems empty (no 'prompts' or 'responses').")
        else:
            logger.info(f"LanguageService: Successfully set current language context to: '{self.current_lang}'")

    async def _create_and_load_language_pack(self, new_lang_code: str, update_context: Optional[Update] = None) -> bool:
        logger.info(f"LanguageService: Attempting to create language pack for new language: {new_lang_code}")
        english_pack_key = "en"
        if english_pack_key not in self.language_packs:
            logger.error(f"Cannot create new language pack: Source English ('{english_pack_key}') pack not found.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback", 
                                                                                           "My apologies, I'm having trouble setting up support for this language right now (missing base files)."))
            return False

        english_pack_content_str = json.dumps(self.language_packs[english_pack_key], ensure_ascii=False, indent=2)
        target_language_name = new_lang_code 

        translation_system_prompt = (
             f"You are an expert translation AI. Your task is to translate a complete JSON language pack from English to {target_language_name} (language code: {new_lang_code}).\n"
            "You MUST maintain the original JSON structure and all original keys (e.g., \"prompts\", \"responses\", \"weather_conditions_map\", \"days_of_week\", and all keys within them). Only translate the string values associated with the keys.\n"
            "The output MUST be a single, valid JSON object and nothing else. Do not add any explanatory text, comments, or markdown before or after the JSON.\n"
            "Ensure all translated strings are appropriate for a friendly AI assistant and are natural-sounding in the target language. Pay special attention to escaping characters within JSON strings if necessary (e.g. double quotes inside a string should be \\\", newlines as \\n)."
        )
        translation_user_prompt = f"Translate the following English JSON language pack to {target_language_name} ({new_lang_code}):\n\n{english_pack_content_str}"
        
        messages_for_api = [{"role": "system", "content": translation_system_prompt}, {"role": "user", "content": translation_user_prompt}]
        response_format_arg = {"response_format": {"type": "json_object"}}
        
        translated_content_str: Optional[str] = None
        try:
            translator_model_id = config.OPENAI_TRANSLATION_MODEL_ID 
            logger.info(f"Using model {translator_model_id} for language pack translation to {new_lang_code}")
            translated_content_str = await self.llm_services.call_openai_llm(
                messages_for_api, model_id=translator_model_id, 
                temperature=0.1, max_tokens=4000, **response_format_arg
            )
        except Exception as e:
            logger.error(f"LLM call itself failed during translation for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False

        if not translated_content_str:
            logger.error(f"LLM failed to provide a translation string for {new_lang_code}.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False
        
        clean_response = translated_content_str.strip() 
        try:
            match = re.search(r"```json\s*(.*?)\s*```", clean_response, re.DOTALL | re.IGNORECASE)
            if match: clean_response = match.group(1).strip()
            else:
                if clean_response.startswith("```json"): clean_response = clean_response[7:]
                if clean_response.endswith("```"): clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            logger.debug(f"Attempting to parse cleaned LLM translation for {new_lang_code}: '{clean_response[:300]}...'")
            translated_pack_content = json.loads(clean_response) 
            
            if not all(k in translated_pack_content for k in ["prompts", "responses", "weather_conditions_map", "days_of_week"]):
                logger.error(f"Translated pack for {new_lang_code} is missing core top-level keys. Aborting save.")
                raise ValueError("Translated JSON missing core keys.")

            new_pack_path = os.path.join(self.lang_packs_dir, f"{new_lang_code}.json")
            with open(new_pack_path, 'w', encoding='utf-8') as f: 
                json.dump(translated_pack_content, f, ensure_ascii=False, indent=2)
            logger.info(f"Successfully created and saved new language pack: {new_lang_code}.json")
            
            self.language_packs[new_lang_code] = translated_pack_content
            self.llm_prompt_sets[new_lang_code] = translated_pack_content.get("prompts", {})
            self.response_strings[new_lang_code] = translated_pack_content.get("responses", {})
            logger.info(f"New language pack for {new_lang_code} is now available at runtime.")
            return True
        except json.JSONDecodeError as jde:
            logger.error(
                f"Failed to decode LLM translation JSON for {new_lang_code}. Error: {jde.msg} "
                f"at L{jde.lineno} C{jde.colno} (char {jde.pos}). "
                f"Nearby: '{clean_response[max(0, jde.pos-50):jde.pos+50]}'", 
                exc_info=False 
            )
            log_limit = 3000
            full_resp_to_log = translated_content_str 
            if len(full_resp_to_log) < log_limit: logger.debug(f"Full problematic translated content for {new_lang_code}:\n{full_resp_to_log}")
            else: logger.debug(f"Problematic translated content (first {log_limit} chars) for {new_lang_code}:\n{full_resp_to_log[:log_limit]}")
            if update_context and update_context.effective_message:
                await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False
        except Exception as e:
            logger.error(f"Error processing/saving new lang pack for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message: 
                await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False

    async def determine_language_context(self,
                                         current_message_text: Optional[str],
                                         chat_id: Optional[int],
                                         update_context: Optional[Update] = None) -> str:
        if current_message_text and self._is_non_language_text(current_message_text):
            logger.info("Input appears to be non-language text (e.g., URL); using default language without detection.")
            self._set_current_language_internals(self.default_language)
            return self.current_lang

        LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD = 0.70
        NUM_RECENT_MESSAGES_FOR_CONTEXT = 2
        MIN_MESSAGE_LENGTH_FOR_LLM_INPUT = 5
        MIN_AGGREGATED_TEXT_LENGTH_FOR_LLM = 15

        final_candidate_lang_code = self.current_lang if self.current_lang != "none" else self.default_language
        
        lang_detector_prompts = self.get_llm_prompt_set("language_detector_llm")

        if not (lang_detector_prompts and "system" in lang_detector_prompts and \
                lang_detector_prompts.get("user_template_full_context") and \
                lang_detector_prompts.get("user_template_latest_only") ):
            logger.error("LLM language detector prompts are incomplete/missing. Using current/default logic for language.")
            # No LLM detection possible, just ensure current lang is set via fallbacks
            self._set_current_language_internals(final_candidate_lang_code)
            return self.current_lang

        history_context_str = ""
        latest_message_payload = current_message_text or "" 

        if chat_id and (not latest_message_payload.strip() or len(latest_message_payload.strip()) < MIN_MESSAGE_LENGTH_FOR_LLM_INPUT):
            logger.debug(f"Current msg short or absent, fetching {NUM_RECENT_MESSAGES_FOR_CONTEXT} recent msgs from chat {chat_id}.")
            try:
                if self.db_manager:
                    recent_messages = await self.db_manager.get_recent_chat_texts(chat_id, limit=NUM_RECENT_MESSAGES_FOR_CONTEXT)
                    if recent_messages:
                        history_context_str = "\n".join(recent_messages)
                        logger.debug(f"Fetched {len(recent_messages)} messages for lang detection context.")
                else: logger.warning("db_manager not available in determine_language_context for history fetch.")
            except Exception as e: logger.error(f"Error fetching recent chat texts for lang detection: {e}", exc_info=True)
        
        user_prompt_template_key = "user_template_full_context" if history_context_str else "user_template_latest_only"
        user_prompt_template = lang_detector_prompts[user_prompt_template_key] # We checked existence above
            
        user_prompt_for_llm_detector = user_prompt_template.format(
            latest_message=latest_message_payload, 
            history_context=history_context_str 
        )
            
        messages_for_llm_detector = [
            {"role": "system", "content": lang_detector_prompts["system"]},
            {"role": "user", "content": user_prompt_for_llm_detector}
        ]

        llm_detected_primary_lang: Optional[str] = None
        llm_detected_confidence: float = 0.0
        
        aggregated_text_for_llm_prompt_check = f"{history_context_str}\n{latest_message_payload}".strip()

        if aggregated_text_for_llm_prompt_check and len(aggregated_text_for_llm_prompt_check) >= MIN_AGGREGATED_TEXT_LENGTH_FOR_LLM:
            try:
                detection_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID 
                logger.info(f"Requesting LLM language detection with model {detection_model_id} for text: '{aggregated_text_for_llm_prompt_check[:70]}...'")
                
                completion_str = await self.llm_services.call_openai_llm(
                    messages_for_llm_detector, model_id=detection_model_id,
                    temperature=0.0, max_tokens=150, response_format={"type": "json_object"}
                )
                if completion_str:
                    try:
                        detection_result = json.loads(completion_str)
                        logger.info(f"LLM language detection response: {detection_result}")
                        llm_detected_primary_lang = str(detection_result.get("primary_lang", "")).lower()
                        confidence_val = detection_result.get("confidence", 0.0)
                        try: llm_detected_confidence = float(confidence_val)
                        except (ValueError, TypeError): llm_detected_confidence = 0.0
                    except json.JSONDecodeError as e:
                         logger.error(f"Failed to decode JSON from LLM lang detector: {e}. Raw: {completion_str}")
                else:
                    logger.warning("LLM language detector returned no content.")
            except Exception as e:
                logger.error(f"Error calling LLM for language detection: {e}", exc_info=True)
        else:
            logger.info(f"Not enough aggregated text ('{aggregated_text_for_llm_prompt_check[:50]}...') for LLM language detection. Using current/default logic.")

        # Logic to use LLM detection result
        if llm_detected_primary_lang and llm_detected_confidence >= LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD:
            logger.info(
                f"LLM confidently detected primary lang '{llm_detected_primary_lang}' "
                f"(conf: {llm_detected_confidence:.2f})."
            )
            final_candidate_lang_code = llm_detected_primary_lang
        else:
            if llm_detected_primary_lang:
                logger.warning(
                    f"LLM detected lang '{llm_detected_primary_lang}' but confidence "
                    f"({llm_detected_confidence:.2f}) < threshold "
                    f"({LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD}). Using current/default: {final_candidate_lang_code}"
                )

            # Heuristic detection prioritising the latest message.
            def _contains_cyrillic(text: str) -> bool:
                return bool(re.search(r"[\u0400-\u04FF]", text))

            def _contains_uk_chars(text: str) -> bool:
                return bool(re.search(r"[ґєії]", text.lower()))

            def _contains_latin(text: str) -> bool:
                return bool(re.search(r"[A-Za-z]", text))

            def _looks_like_translit_ru(text: str) -> bool:
                translit_words = {
                    "privet",
                    "poka",
                    "spasibo",
                    "kak",
                    "dela",
                    "zdravstv",
                }
                t = text.lower()
                return any(word in t for word in translit_words)

            latest_only = latest_message_payload or ""

            if _contains_cyrillic(latest_only):
                final_candidate_lang_code = "uk" if _contains_uk_chars(latest_only) else "ru"
            elif _looks_like_translit_ru(latest_only):
                final_candidate_lang_code = "ru"
            elif _contains_latin(latest_only):
                final_candidate_lang_code = "en"
            elif _contains_cyrillic(aggregated_text_for_llm_prompt_check):
                final_candidate_lang_code = (
                    "uk"
                    if _contains_uk_chars(aggregated_text_for_llm_prompt_check)
                    else "ru"
                )
            elif _contains_latin(aggregated_text_for_llm_prompt_check):
                final_candidate_lang_code = "en"

        if update_context and update_context.effective_user:
            user_lang = getattr(update_context.effective_user, "language_code", None)
            if (
                user_lang
                and final_candidate_lang_code == self.default_language
                and user_lang in self.language_packs
            ):
                final_candidate_lang_code = user_lang

        unsupported_codes = ("", "und", "undefined", "none")
        if final_candidate_lang_code in unsupported_codes:
            logger.warning(
                f"LLM returned unsupported language code '{final_candidate_lang_code}'. "
                f"Using default language '{self.default_language}' instead."
            )
            final_candidate_lang_code = self.default_language

        # Set language context based on final_candidate_lang_code
        if final_candidate_lang_code not in self.language_packs:
            logger.warning(f"Language pack for candidate '{final_candidate_lang_code}' not found. Attempting creation.")
            if await self._create_and_load_language_pack(final_candidate_lang_code, update_context=update_context):
                self._set_current_language_internals(final_candidate_lang_code)
            else: 
                logger.warning(f"Failed to create pack for '{final_candidate_lang_code}'. Applying prioritized fallbacks.")
                self._set_current_language_internals(self.default_language) 
        else: 
            self._set_current_language_internals(final_candidate_lang_code)
            
        return self.current_lang

    @staticmethod
    def _is_non_language_text(text: str) -> bool:
        """Return True if text likely isn't natural language (e.g., URLs or code)."""
        if not text:
            return True

        stripped = text.strip()

        url_patterns = [
            r'^(?:https?://|www\.)\S+$',
            r'^[\w.-]+\.[a-z]{2,}(?:/\S*)?$'
        ]
        for pattern in url_patterns:
            if re.match(pattern, stripped, re.IGNORECASE):
                return True

        if not any(ch.isalpha() for ch in stripped):
            return True

        return False
    
    def get_llm_prompt_set(self, key: str) -> Optional[Dict[str, str]]:
        current_prompts_to_check = self.current_llm_prompt_sets
        prompt_set = current_prompts_to_check.get(key)
        primary_fallback_lang = self.default_language
        secondary_fallback_lang = "ru"

        if not prompt_set: 
            logger.debug(f"LLM prompt set key '{key}' not in current lang '{self.current_lang}'. Trying '{primary_fallback_lang}'.")
            current_prompts_to_check = self.llm_prompt_sets.get(primary_fallback_lang, {})
            prompt_set = current_prompts_to_check.get(key)
            if not prompt_set and primary_fallback_lang != secondary_fallback_lang: 
                 logger.debug(f"LLM prompt set key '{key}' not in '{primary_fallback_lang}'. Trying '{secondary_fallback_lang}'.")
                 current_prompts_to_check = self.llm_prompt_sets.get(secondary_fallback_lang, {})
                 prompt_set = current_prompts_to_check.get(key)
        
        if not prompt_set:
            logger.error(f"LLM prompt set for key '{key}' ultimately not found.")
            return None
        if not isinstance(prompt_set, dict) or "system" not in prompt_set: 
            logger.error(f"LLM prompt set for key '{key}' (found in lang or fallback) is malformed: {prompt_set}")
            return None
        return prompt_set

    def get_response_string(self, key: str, default_value: Optional[str] = None, **kwargs) -> str:
        raw_string = self.current_response_strings.get(key)
        lang_tried = self.current_lang
        primary_fallback_lang = self.default_language
        secondary_fallback_lang = "ru"

        if raw_string is None: 
            lang_tried = primary_fallback_lang
            raw_string = self.response_strings.get(primary_fallback_lang, {}).get(key)
            if raw_string is None and primary_fallback_lang != secondary_fallback_lang: 
                lang_tried = secondary_fallback_lang
                raw_string = self.response_strings.get(secondary_fallback_lang, {}).get(key)

        if raw_string is None:
            if default_value is not None:
                raw_string = default_value
            else:
                logger.error(
                    f"Response string for key '{key}' ultimately not found",
                    extra={
                        "event_type": "i18n.missing_key",
                        "key": key,
                        "lang": lang_tried,
                    },
                )
                raw_string = f"[[Missing response: {key}]]"
        
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError as e:
            logger.error(f"Missing format key '{e}' in response string for key '{key}' (lang tried: {lang_tried}, raw: '{raw_string}')")
            english_raw = self.response_strings.get("en", {}).get(key, f"[[Format error & missing English for key: {key}]]")
            try: return english_raw.format(**kwargs) if kwargs else english_raw
            except KeyError: return f"[[Formatting error for response key: {key} - check placeholders/English pack]]"



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/llm_services.py ---
======================================================================

# enkibot/core/llm_services.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# ==================================================================================================
# === EnkiBot LLM Services ===
# ==================================================================================================
# enkibot/core/llm_services.py
# (Your GPLv3 Header)

import logging
import httpx
from types import SimpleNamespace
try:  # pragma: no cover - optional dependency
    import openai
except Exception:  # pragma: no cover
    openai = SimpleNamespace(AsyncOpenAI=None)
import asyncio
import time
from typing import List, Dict, Optional, Any, Tuple

from enkibot import config
from enkibot.utils.provider_metrics import ProviderMetrics

logger = logging.getLogger(__name__)

class LLMServices:
    def __init__(self, openai_api_key: Optional[str], openai_model_id: str,
                 groq_api_key: Optional[str], groq_model_id: str, groq_endpoint_url: str,
                 openrouter_api_key: Optional[str], openrouter_model_id: str, openrouter_endpoint_url: str,
                 google_ai_api_key: Optional[str], google_ai_model_id: str):
        
        logger.info("LLMServices __init__ STARTING")
        
        self.openai_api_key = openai_api_key
        self.openai_model_id = openai_model_id
        self.openai_deep_research_model_id = config.OPENAI_DEEP_RESEARCH_MODEL_ID
        self.openai_embedding_model_id = config.OPENAI_EMBEDDING_MODEL_ID
        self.openai_classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        self.openai_translation_model_id = config.OPENAI_TRANSLATION_MODEL_ID
        self.openai_dalle_model_id = config.OPENAI_DALLE_MODEL_ID
        self.openai_whisper_model_id = config.OPENAI_WHISPER_MODEL_ID # New

        self.openai_async_client: Optional[openai.AsyncOpenAI] = None
        if self.openai_api_key:
            try:
                self.openai_async_client = openai.AsyncOpenAI(api_key=self.openai_api_key)
                logger.info("OpenAI AsyncClient initialized successfully.")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI AsyncClient: {e}")
        else:
            logger.warning("OpenAI API key not provided. OpenAI calls will be disabled.")

        self.groq_api_key = groq_api_key
        self.groq_model_id = groq_model_id
        self.groq_endpoint_url = groq_endpoint_url

        self.openrouter_api_key = openrouter_api_key
        self.openrouter_model_id = openrouter_model_id
        self.openrouter_endpoint_url = openrouter_endpoint_url

        self.google_ai_api_key = google_ai_api_key
        self.google_ai_model_id = google_ai_model_id

        # Persistent HTTP client for provider calls
        self.http_client = httpx.AsyncClient()

        # Metrics tracking structures
        self.metrics: Dict[str, ProviderMetrics] = {
            "OpenAI": ProviderMetrics(),
            "Groq": ProviderMetrics(),
            "OpenRouter": ProviderMetrics(),
        }

        self.cost_per_1k_tokens = {
            "OpenAI": config.OPENAI_COST_PER_1K_TOKENS,
            "Groq": config.GROQ_COST_PER_1K_TOKENS,
            "OpenRouter": config.OPENROUTER_COST_PER_1K_TOKENS,
        }

        logger.info("LLMServices __init__ COMPLETED")

    # ... (is_provider_configured, call_openai_llm, call_llm_api, race_llm_calls, generate_image_openai methods remain the same) ...
    def is_provider_configured(self, provider_name: str) -> bool:
        provider_name_lower = provider_name.lower()
        if provider_name_lower == "openai":
            return bool(self.openai_async_client and self.openai_api_key)
        elif provider_name_lower == "groq":
            return bool(self.groq_api_key and self.groq_endpoint_url and self.groq_model_id)
        elif provider_name_lower == "openrouter":
            return bool(self.openrouter_api_key and self.openrouter_endpoint_url and self.openrouter_model_id)
        return False

    def _record_metrics(self, provider: str, latency: float, tokens: int = 0) -> None:
        metrics = self.metrics.setdefault(provider, ProviderMetrics())
        cost_per_1k = self.cost_per_1k_tokens.get(provider, 0.0)
        metrics.record(latency, tokens, cost_per_1k)

    async def call_openai_llm(self, messages: List[Dict[str, str]],
                              model_id: Optional[str] = None,
                              temperature: float = 0.7,
                              max_tokens: int = 2000,
                              **kwargs) -> Optional[str]:
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not initialized or API key missing. Cannot make call.")
            return None
        actual_model_id = model_id or self.openai_model_id
        logger.info(f"Calling OpenAI (model: {actual_model_id}) with {len(messages)} messages.")
        logger.debug(f"OpenAI messages: {messages}")
        call_params = { "model": actual_model_id, "messages": messages, "temperature": temperature, "max_tokens": max_tokens, **kwargs }
        try:
            start = time.perf_counter()
            completion = await self.openai_async_client.chat.completions.create(**call_params)
            latency = time.perf_counter() - start
            tokens = 0
            if getattr(completion, "usage", None):
                tokens = getattr(completion.usage, "total_tokens", 0)
            self._record_metrics("OpenAI", latency, tokens)
            logger.debug(f"OpenAI raw response: {completion}")
            if completion.choices and completion.choices[0].message and completion.choices[0].message.content:
                return completion.choices[0].message.content.strip()
            logger.warning(f"OpenAI call to {actual_model_id} returned no content or unexpected structure.")
            return None
        except openai.OpenAIError as e:
            logger.error(
                f"OpenAI API Error (model: {actual_model_id}): {e}",
                exc_info=False,
            )
        except Exception as e:
            logger.error(f"Unexpected error with OpenAI API (model: {actual_model_id}): {e}", exc_info=True)
        return None

    async def call_openai_deep_research(self, messages: List[Dict[str, str]],
                                        max_output_tokens: int = 1000) -> Optional[str]:
        """Call OpenAI's deep research model with web search enabled.

        This uses the Responses API so the model can issue `web_search` tool
        calls when checking claims. It returns the aggregated text response or
        ``None`` if the call fails.
        """
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not initialized or API key missing. Cannot make deep research call.")
            return None
        try:
            start = time.perf_counter()
            response = await self.openai_async_client.responses.create(
                model=self.openai_deep_research_model_id,
                input=messages,
                tools=[{"type": "web_search"}],
                tool_choice="auto",
                max_output_tokens=max_output_tokens,
            )
            latency = time.perf_counter() - start
            usage = getattr(response, "usage", None)
            tokens = getattr(usage, "total_tokens", 0)
            self._record_metrics("OpenAI", latency, tokens)
            return getattr(response, "output_text", None)
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API Error (deep research): {e}", exc_info=False)
            # If the dedicated deep-research model is unavailable (e.g. 403
            # model_not_found), fall back to the standard chat completion
            # model so the bot still provides a best-effort answer.
            try:
                logger.info(
                    "Falling back to standard OpenAI model %s", self.openai_model_id
                )
                return await self.call_openai_llm(
                    messages,
                    model_id=self.openai_model_id,
                    temperature=0.0,
                    max_tokens=max_output_tokens,
                )
            except Exception as fallback_err:  # pragma: no cover - rare
                logger.error(
                    "Fallback to standard OpenAI model failed: %s", fallback_err, exc_info=True
                )
        except Exception as e:
            logger.error(f"Unexpected error with OpenAI deep research: {e}", exc_info=True)
        return None

    async def embed_texts_openai(self, texts: List[str], model_id: Optional[str] = None) -> Optional[List[List[float]]]:
        """Return embeddings for ``texts`` using OpenAI's embedding endpoint.

        Falls back to ``None`` if OpenAI is not configured or an error occurs.
        """
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not initialized or API key missing. Cannot create embeddings.")
            return None
        actual_model_id = model_id or self.openai_embedding_model_id
        try:
            start = time.perf_counter()
            response = await self.openai_async_client.embeddings.create(model=actual_model_id, input=texts)
            latency = time.perf_counter() - start
            usage = getattr(response, "usage", None)
            tokens = getattr(usage, "total_tokens", 0)
            self._record_metrics("OpenAI", latency, tokens)
            if response.data:
                return [item.embedding for item in response.data]
        except openai.OpenAIError as e:
            logger.error(
                f"OpenAI Embedding API Error (model: {actual_model_id}): {e}",
            )
        except Exception as e:
            logger.error(f"Unexpected error during OpenAI embedding call (model: {actual_model_id}): {e}", exc_info=True)
        return None

    async def call_llm_api(self, provider_name: str, api_key: Optional[str], endpoint_url: Optional[str], 
                           model_id: str, messages: List[Dict[str, str]], 
                           temperature: float = 0.7, max_tokens: int = 2000,
                           **kwargs) -> Optional[str]:
        if not api_key or not endpoint_url:
            logger.warning(f"{provider_name} not configured. Skipping call.")
            return None
        logger.info(f"Calling {provider_name} (model: {model_id}) with {len(messages)} messages.")
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        if provider_name.lower() == "openrouter":
            headers.update({"HTTP-Referer": "http://localhost:8000", "X-Title": "EnkiBot"})
        payload = {"model": model_id, "messages": messages, "max_tokens": max_tokens, "temperature": temperature, **kwargs}
        try:
            start = time.perf_counter()
            resp = await self.http_client.post(endpoint_url, json=payload, headers=headers, timeout=30.0)
            latency = time.perf_counter() - start
            resp.raise_for_status()
            data = resp.json()
            tokens = data.get("usage", {}).get("total_tokens", 0)
            self._record_metrics(provider_name, latency, tokens)
            if data.get("choices") and data["choices"][0].get("message") and data["choices"][0]["message"].get("content"):
                return data["choices"][0]["message"]["content"].strip()
            logger.warning(f"{provider_name} call to {model_id} returned no content or unexpected structure.")
            return None
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error for {provider_name} ({model_id}): {e.response.status_code} - Response: {e.response.text[:500]}...", exc_info=False)
        except Exception as e:
            logger.error(f"Unexpected error with {provider_name} API ({model_id}): {e}", exc_info=True)
        return None

    async def race_llm_calls(self, messages: List[Dict[str, str]]) -> Optional[str]:
        task_info: List[Tuple[asyncio.Task, str]] = []
        if self.is_provider_configured("openai"):
            task = asyncio.create_task(self.call_openai_llm(messages, model_id=self.openai_model_id))
            task_info.append((task, "OpenAI"))
        if self.is_provider_configured("groq"):
            task = asyncio.create_task(self.call_llm_api("Groq", self.groq_api_key, self.groq_endpoint_url, self.groq_model_id, messages))
            task_info.append((task, "Groq"))
        if self.is_provider_configured("openrouter"):
            task = asyncio.create_task(self.call_llm_api("OpenRouter", self.openrouter_api_key, self.openrouter_endpoint_url, self.openrouter_model_id, messages))
            task_info.append((task, "OpenRouter"))
        if not task_info:
            logger.warning("No LLM providers configured for racing calls.")
            return None
        logger.info(f"Racing LLM calls to: {[name for _, name in task_info]}")
        task_to_provider_map = {task_obj: name for task_obj, name in task_info}
        tasks_only = [task_obj for task_obj, _ in task_info]
        for future in asyncio.as_completed(tasks_only):
            provider_name_for_log = task_to_provider_map.get(future, "UnknownProvider") 
            try:
                result = await future
                if result and result.strip():
                    logger.info(f"Successful response from {provider_name_for_log} in race.")
                    return result.strip()
                else:
                    logger.warning(f"{provider_name_for_log} returned no content in race.")
            except Exception as e: 
                logger.warning(f"Provider {provider_name_for_log} task raised an exception during race: {type(e).__name__} - {e}")
        logger.error("All LLM providers failed or returned no content in race_llm_calls.")
        return None

    async def generate_image_openai(self, prompt: str, n: int = 1, size: str = "1024x1024", quality: str = "standard", response_format: str = "url") -> Optional[List[Dict[str, str]]]:
        if not self.is_provider_configured("openai"):
            logger.error("OpenAI client not configured. Cannot generate image.")
            return None
        model_to_use = self.openai_dalle_model_id 
        logger.info(f"Requesting DALL-E image generation with prompt: '{prompt[:70]}...' using model {model_to_use}")
        try:
            response = await self.openai_async_client.images.generate(
                model=model_to_use, prompt=prompt, n=n, size=size, quality=quality, response_format=response_format)
            image_data_list = []
            if response.data:
                for image_obj in response.data:
                    if response_format == "url" and image_obj.url: image_data_list.append({"url": image_obj.url})
                    elif response_format == "b64_json" and image_obj.b64_json: image_data_list.append({"b64_json": image_obj.b64_json})
            if image_data_list:
                logger.info(f"DALL-E generated {len(image_data_list)} image(s).")
                return image_data_list
            else:
                logger.warning(f"DALL-E call successful but no image data returned.")
                return None
        except openai.APIError as e:
            logger.error(f"OpenAI DALL-E API Error: {e.message}", exc_info=False)
        except Exception as e:
            logger.error(f"Unexpected error during DALL-E image generation: {e}", exc_info=True)
        return None

    async def generate_image_with_dalle(
        self,
        prompt: str,
        n: int = 1,
        size: str = "1024x1024",
        quality: str = "standard",
        response_format: str = "url",
    ) -> Optional[List[Dict[str, str]]]:
        """Backward compatible DALL-E image generation wrapper.

        Historically the project exposed ``generate_image_with_dalle``.
        The new OpenAI client consolidates image generation under
        :meth:`generate_image_openai`.  Some parts of the codebase – and
        possibly third-party plugins – may still call the old name.  To
        prevent ``AttributeError`` exceptions we keep this thin wrapper
        that forwards the call to :meth:`generate_image_openai` and emits a
        warning to encourage migrating to the new API.
        """

        logger.warning(
            "generate_image_with_dalle is deprecated; use generate_image_openai instead."
        )
        return await self.generate_image_openai(
            prompt=prompt,
            n=n,
            size=size,
            quality=quality,
            response_format=response_format,
        )

    # --- NEW METHOD FOR TRANSCRIPTION ---
    async def transcribe_audio(self, audio_file_path: str) -> Optional[str]:
        """
        Transcribes an audio file using OpenAI's Whisper model.
        """
        if not self.is_provider_configured("openai"):
            logger.error("OpenAI client not configured. Cannot transcribe audio.")
            return None
        
        model_to_use = self.openai_whisper_model_id
        logger.info(f"Requesting Whisper transcription for file: {audio_file_path} using model {model_to_use}")

        try:
            with open(audio_file_path, "rb") as audio_file:
                transcription = await self.openai_async_client.audio.transcriptions.create(
                    model=model_to_use,
                    file=audio_file
                )
            
            transcribed_text = transcription.text
            if transcribed_text and transcribed_text.strip():
                logger.info(f"Whisper transcription successful. Text: '{transcribed_text[:100]}...'")
                return transcribed_text.strip()
            else:
                logger.warning("Whisper transcription returned empty text.")
                return None
        except openai.APIError as e:
            logger.error(f"OpenAI Whisper API Error: {e.message}", exc_info=False)
        except Exception as e:
            logger.error(f"Unexpected error during audio transcription: {e}", exc_info=True)
        return None

    # --- NEW METHOD FOR MODERATION ---
    async def moderate_text_openai(self, text: str) -> Optional[Dict[str, Any]]:
        """Calls OpenAI's moderation endpoint on the supplied text.

        Returns a dictionary with the moderation result or ``None`` if the
        moderation service is not available or an error occurred.
        """
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not configured. Cannot moderate text.")
            return None

        try:
            response = await self.openai_async_client.moderations.create(
                model="omni-moderation-latest",
                input=text,
            )
            if response.results:
                result = response.results[0]
                # Extract minimal useful information. ``result`` is an
                # OpenAI object; convert to a standard dict for downstream
                # processing. Include ``category_scores`` so callers can
                # compute risk levels without re-parsing the OpenAI object.
                return {
                    "flagged": bool(getattr(result, "flagged", False)),
                    "categories": getattr(result, "categories", {}),
                    "category_scores": getattr(result, "category_scores", {}),
                }
        except openai.APIError as e:
            logger.error(f"OpenAI Moderation API Error: {e.message}")
        except Exception as e:
            logger.error(f"Unexpected error during moderation call: {e}", exc_info=True)
        return None

    async def aclose(self) -> None:
        """Close underlying HTTP clients."""
        try:
            await self.http_client.aclose()
        finally:
            if self.openai_async_client:
                await self.openai_async_client.aclose()



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/telegram_handlers.py ---
======================================================================

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# enkibot/core/telegram_handlers.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
import asyncio
import os
import uuid
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, TYPE_CHECKING
from types import SimpleNamespace

from telegram import (
    Update,
    ReplyKeyboardRemove,
    ChatPermissions,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
    BotCommand,
    BotCommandScopeDefault,
    BotCommandScopeChat,
    BotCommandScopeChatAdministrators,
)
from telegram.ext import (
    Application,
    ContextTypes,
    ConversationHandler,
    CommandHandler,
    MessageHandler,
    filters,
    CallbackQueryHandler,
)
# Optional import for message reaction updates
try:  # pragma: no cover - optional dependency
    from telegram.ext import MessageReactionHandler
except Exception:  # pragma: no cover - fallback when not supported
    MessageReactionHandler = None
from telegram.constants import ChatAction
from telegram.helpers import mention_html
import re
import random
from datetime import datetime, timedelta

try:
    from nudenet import NudeClassifier
except Exception:
    NudeClassifier = None

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.utils.database import DatabaseManager
    from enkibot.core.llm_services import LLMServices
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.profile_manager import ProfileManager
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator
    from enkibot.modules.spam_detector import SpamDetector
    from enkibot.modules.stats_manager import StatsManager
    from enkibot.modules.karma_manager import KarmaManager
    from enkibot.modules.community_moderation import CommunityModerationService
    from .intent_handlers.weather_handler import WeatherIntentHandler
    from .intent_handlers.news_handler import NewsIntentHandler
    from .intent_handlers.general_handler import GeneralIntentHandler
    from .intent_handlers.image_generation_handler import ImageGenerationIntentHandler

from enkibot import config as bot_config
from .intent_handlers.weather_handler import WeatherIntentHandler 
from .intent_handlers.news_handler import NewsIntentHandler
from .intent_handlers.general_handler import GeneralIntentHandler
from .intent_handlers.image_generation_handler import ImageGenerationIntentHandler
from enkibot.modules.spam_detector import SpamDetector
from enkibot.utils.message_utils import is_forwarded_message
from enkibot.utils.trigger_extractor import extract_assistant_prompt

logger = logging.getLogger(__name__)

# Conversation states
ASK_CITY = 1
ASK_NEWS_TOPIC = 2 

class TelegramHandlerService:
    def __init__(self, 
                 application: Application, 
                 db_manager: 'DatabaseManager', 
                 llm_services: 'LLMServices',   
                 intent_recognizer: 'IntentRecognizer',
                 profile_manager: 'ProfileManager',
                 api_router: 'ApiRouter',
                 response_generator: 'ResponseGenerator',
                 language_service: 'LanguageService',
                 spam_detector: 'SpamDetector',
                 stats_manager: 'StatsManager',
                 karma_manager: 'KarmaManager',
                 community_moderation: 'CommunityModerationService',
                 allowed_group_ids: set,
                 bot_nicknames: list
                ):
        logger.info("TelegramHandlerService __init__ STARTING")
        self.application = application
        self.db_manager = db_manager
        self.llm_services = llm_services 
        self.intent_recognizer = intent_recognizer
        self.profile_manager = profile_manager
        self.api_router = api_router
        self.response_generator = response_generator
        self.language_service = language_service
        self.spam_detector = spam_detector
        self.stats_manager = stats_manager
        self.karma_manager = karma_manager
        self.community_moderation = community_moderation
        
        self.allowed_group_ids = allowed_group_ids 
        self.bot_nicknames = bot_nicknames

        self.pending_action_data: Dict[int, Dict[str, Any]] = {}
        self.pending_captchas: Dict[int, Dict[str, Any]] = {}
        self.nsfw_classifier: Optional['NudeClassifier'] = None
        # Feature flags per chat for dynamic command hints
        self.features_db: Dict[int, Dict[str, bool]] = {}
        # Manual language overrides per chat
        self.chat_languages: Dict[int, str] = {}
        # Registry of default commands for help text and Telegram registration
        self.default_commands: Dict[str, Dict[str, str]] = {}

        # Map reaction emoji to handler coroutines
        self.reaction_handlers: Dict[str, Any] = {
            "🔄": self._handle_regenerate_reaction,
            "➕": self._handle_expand_reaction,
            "📝": self._handle_summary_reaction,
        }

        # Track recent reactions to bot messages for follow-up triggers
        self.recent_reactors: Dict[int, datetime] = {}

        # Map inline refinement actions to handlers
        self.refinement_handlers: Dict[str, Any] = {
            "regenerate": self._handle_regenerate_reaction,
            "expand": self._handle_expand_reaction,
            "summary": self._handle_summary_reaction,
        }

        # Instantiate specialized handlers
        self.weather_handler = WeatherIntentHandler(
            language_service=self.language_service,
            intent_recognizer=self.intent_recognizer,
            api_router=self.api_router,
            response_generator=self.response_generator,
            pending_action_data_ref=self.pending_action_data
        )
        self.news_handler = NewsIntentHandler(
            language_service=self.language_service,
            intent_recognizer=self.intent_recognizer,
            api_router=self.api_router,
            response_generator=self.response_generator,
            pending_action_data_ref=self.pending_action_data
        )
        self.general_handler = GeneralIntentHandler(
            language_service=self.language_service,
            response_generator=self.response_generator
        )
        self.image_generation_handler = ImageGenerationIntentHandler(
            language_service=self.language_service,
            intent_recognizer=self.intent_recognizer,
            llm_services=self.llm_services,
            db_manager=self.db_manager
        )
        logger.info("TelegramHandlerService __init__ COMPLETED")

    async def log_message_and_profile_tasks(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not update.message or not update.message.text or not update.effective_user:
            return
        chat_id = update.effective_chat.id
        user = update.effective_user
        message = update.message
        if self.allowed_group_ids and chat_id not in self.allowed_group_ids: 
            return 
        current_lang_for_log = self.language_service.current_lang 
        action_taken = await self.db_manager.log_chat_message_and_upsert_user(
            chat_id=chat_id, user_id=user.id, username=user.username,
            first_name=user.first_name, last_name=user.last_name,
            message_id=message.message_id, message_text=message.text,
            preferred_language=current_lang_for_log )
        await self.stats_manager.log_message(chat_id, user.id, message.text, user.username)
        logger.info(f"Message from user {user.id} logged. Profile action: {action_taken}.")
        # Ensure chat activity is visible in the terminal even if logging is redirected
        print(
            f"CHAT[{chat_id}] {user.username or user.id}: {message.text}"
            f" | profile action: {action_taken}"
        )
        name_var_prompts = self.language_service.get_llm_prompt_set("name_variation_generator")
        if action_taken and action_taken.lower() == "insert" and name_var_prompts and "system" in name_var_prompts:
            asyncio.create_task(self.profile_manager.populate_name_variations_with_llm(
                user_id=user.id, first_name=user.first_name, last_name=user.last_name, username=user.username,
                system_prompt=name_var_prompts["system"], user_prompt_template=name_var_prompts.get("user","Generate for: {name_info}")))
        profile_create_prompts = self.language_service.get_llm_prompt_set("profile_creator")
        profile_update_prompts = self.language_service.get_llm_prompt_set("profile_updater")
        if message.text and len(message.text.strip()) > 10:
            if profile_create_prompts and "system" in profile_create_prompts and \
               profile_update_prompts and "system" in profile_update_prompts:
                asyncio.create_task(self.profile_manager.analyze_and_update_user_profile(
                    user_id=user.id, message_text=message.text,
                    create_system_prompt=profile_create_prompts["system"], create_user_prompt_template=profile_create_prompts.get("user","Analyze: {message_text}"),
                    update_system_prompt=profile_update_prompts["system"], update_user_prompt_template=profile_update_prompts.get("user","Update based on: {message_text} with existing: {current_profile_notes}")))
            else: logger.warning(f"Profile prompts missing for lang '{current_lang_for_log}'. Skipping profile analysis for user {user.id}.")

    async def _is_triggered(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt_lower: str) -> bool:
        if not update.message or not context.bot:
            return False

        current_chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ["group", "supergroup"]
        if not is_group:
            return True

        if self.allowed_group_ids and current_chat_id not in self.allowed_group_ids:
            return False

        if is_forwarded_message(update.message):
            logger.info(f"_is_triggered: True (Forwarded message in chat {current_chat_id})")
            return True

        bot_username_lower = getattr(context.bot, "username", "").lower() if getattr(context.bot, "username", None) else ""
        is_at_mentioned = bool(bot_username_lower and f"@{bot_username_lower}" in user_msg_txt_lower)
        tokens = re.findall(r"\w+", user_msg_txt_lower, flags=re.UNICODE)
        is_nickname_mentioned = any(nick.lower() in tokens for nick in self.bot_nicknames)
        is_bot_mentioned = is_at_mentioned or is_nickname_mentioned
        is_reply_to_bot = (
            update.message.reply_to_message
            and update.message.reply_to_message.from_user
            and context.bot
            and update.message.reply_to_message.from_user.id == context.bot.id
        )
        final_trigger_decision = is_bot_mentioned or is_reply_to_bot
        if is_group and final_trigger_decision:
            logger.info(
                f"_is_triggered: True (Group: {current_chat_id}): @M={is_at_mentioned}, NickM={is_nickname_mentioned}, Reply={is_reply_to_bot}"
            )
        return final_trigger_decision

    async def _is_user_admin(self, chat_id: int, user_id: int, context: ContextTypes.DEFAULT_TYPE) -> bool:
        try:
            member = await context.bot.get_chat_member(chat_id, user_id)
            return member.status in ("administrator", "creator")
        except Exception:
            return False

    def _generate_math_captcha(self) -> tuple[str, list[int], int]:
        a, b = random.randint(1, 20), random.randint(1, 20)
        question = f"{a} + {b}"
        correct = a + b
        options = {correct}
        while len(options) < 4:
            options.add(random.randint(correct - 10, correct + 10))
        option_list = list(options)
        random.shuffle(option_list)
        return question, option_list, correct

    async def _start_captcha(self, user, chat_id: int, context: ContextTypes.DEFAULT_TYPE):
        method = random.choice(["button", "math"])
        mention = user.mention_html()
        if method == "button":
            text = (
                f"Welcome, {mention}! Please prove you are human by clicking the button below within {bot_config.CAPTCHA_TIMEOUT_SECONDS} seconds."
            )
            keyboard = InlineKeyboardMarkup(
                [[InlineKeyboardButton("✅ I'm a human!", callback_data=f"captcha_button:{user.id}")]]
            )
            msg = await context.bot.send_message(chat_id, text, reply_markup=keyboard, parse_mode="HTML")
            self.pending_captchas[user.id] = {
                "chat_id": chat_id,
                "message_id": msg.message_id,
                "type": "button",
                "mention": mention,
            }
        else:
            question, options, correct = self._generate_math_captcha()
            text = (
                f"Welcome, {mention}! Solve: {question} = ?"
            )
            keyboard = InlineKeyboardMarkup(
                [[InlineKeyboardButton(str(opt), callback_data=f"captcha_math:{user.id}:{opt}") for opt in options]]
            )
            msg = await context.bot.send_message(chat_id, text, reply_markup=keyboard, parse_mode="HTML")
            self.pending_captchas[user.id] = {
                "chat_id": chat_id,
                "message_id": msg.message_id,
                "type": "math",
                "correct": correct,
                "attempts": bot_config.CAPTCHA_MAX_ATTEMPTS,
                "mention": mention,
            }

        task = asyncio.create_task(self._captcha_timeout(user.id))
        self.pending_captchas[user.id]["task"] = task

    async def start_captcha(self, user, chat_id: int, context: ContextTypes.DEFAULT_TYPE):
        """Public wrapper to initiate captcha verification for a user.

        This exposes the existing captcha mechanism to other modules
        (e.g., the spam detector) without requiring them to know about the
        private ``_start_captcha`` implementation details.
        """
        await self._start_captcha(user, chat_id, context)

    async def _captcha_timeout(self, user_id: int):
        await asyncio.sleep(bot_config.CAPTCHA_TIMEOUT_SECONDS)
        info = self.pending_captchas.get(user_id)
        if not info:
            return
        chat_id = info["chat_id"]
        try:
            await self.application.bot.ban_chat_member(chat_id, user_id)
            await self.application.bot.send_message(chat_id, f"❌ {info['mention']} failed verification and was removed.", parse_mode="HTML")
            try:
                await self.application.bot.delete_message(chat_id, info["message_id"])
            except Exception:
                pass
        finally:
            self.pending_captchas.pop(user_id, None)

    async def _verify_user(self, user_id: int, context: ContextTypes.DEFAULT_TYPE):
        info = self.pending_captchas.get(user_id)
        if not info:
            return
        task = info.get("task")
        if task:
            task.cancel()
        chat_id = info["chat_id"]
        await context.bot.restrict_chat_member(
            chat_id,
            user_id,
            permissions=ChatPermissions(
                can_send_messages=True,
                can_send_audios=True,
                can_send_documents=True,
                can_send_photos=True,
                can_send_videos=True,
                can_send_video_notes=True,
                can_send_voice_notes=True,
                can_send_polls=True,
                can_send_other_messages=True,
                can_add_web_page_previews=True,
                can_invite_users=True,
            ),
        )
        try:
            await context.bot.delete_message(chat_id, info["message_id"])
        except Exception:
            pass
        await self.db_manager.add_verified_user(user_id)
        await context.bot.send_message(chat_id, f"✅ {info['mention']} verified as human.", parse_mode="HTML")
        self.pending_captchas.pop(user_id, None)

    async def _fail_verification(self, user_id: int, context: ContextTypes.DEFAULT_TYPE):
        info = self.pending_captchas.get(user_id)
        if not info:
            return
        task = info.get("task")
        if task:
            task.cancel()
        chat_id = info["chat_id"]
        await context.bot.ban_chat_member(chat_id, user_id)
        try:
            await context.bot.delete_message(chat_id, info["message_id"])
        except Exception:
            pass
        await context.bot.send_message(chat_id, f"❌ {info['mention']} failed verification and was removed.", parse_mode="HTML")
        self.pending_captchas.pop(user_id, None)

    async def handle_new_chat_members(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not update.message or not update.message.new_chat_members:
            return
        chat_id = update.effective_chat.id
        if self.allowed_group_ids and chat_id not in self.allowed_group_ids:
            return
        for member in update.message.new_chat_members:
            if member.is_bot:
                continue
            await self.stats_manager.log_member_join(chat_id, member.id, member.username)
            if await self.db_manager.is_user_verified(member.id):
                continue
            # Print to console for visibility of join events
            print(
                f"JOIN[{chat_id}] {member.username or member.id} joined the chat"
            )
            try:
                await context.bot.restrict_chat_member(
                    chat_id,
                    member.id,
                    permissions=ChatPermissions(
                        can_send_messages=False,
                        can_send_audios=False,
                        can_send_documents=False,
                        can_send_photos=False,
                        can_send_videos=False,
                        can_send_video_notes=False,
                        can_send_voice_notes=False,
                        can_send_polls=False,
                        can_send_other_messages=False,
                        can_add_web_page_previews=False,
                        can_invite_users=False,
                        can_pin_messages=False,
                        can_change_info=False,
                    ),
                )
            except Exception as e:
                logger.error(f"Failed to restrict user {member.id}: {e}")
            await self._start_captcha(member, chat_id, context)

    async def handle_left_chat_member(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not update.message or not update.message.left_chat_member:
            return
        chat_id = update.effective_chat.id
        if self.allowed_group_ids and chat_id not in self.allowed_group_ids:
            return
        member = update.message.left_chat_member
        if member and not member.is_bot:
            await self.stats_manager.log_member_leave(chat_id, member.id)
            # Print to console for visibility of leave events
            print(
                f"LEAVE[{chat_id}] {member.username or member.id} left the chat"
            )

    async def captcha_button_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        query = update.callback_query
        if not query or not query.data:
            return
        parts = query.data.split(":")
        if len(parts) != 2:
            return
        _, target_id_str = parts
        target_id = int(target_id_str)
        user_id = query.from_user.id
        if user_id != target_id:
            await query.answer("This is not your captcha.", show_alert=True)
            return
        await query.answer()
        await self._verify_user(user_id, context)

    async def captcha_math_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        query = update.callback_query
        if not query or not query.data:
            return
        parts = query.data.split(":")
        if len(parts) != 3:
            return
        _, target_id_str, answer_str = parts
        target_id = int(target_id_str)
        answer = int(answer_str)
        user_id = query.from_user.id
        if user_id != target_id:
            await query.answer("This is not your captcha.", show_alert=True)
            return
        info = self.pending_captchas.get(user_id)
        if not info:
            await query.answer()
            return
        if answer == info.get("correct"):
            await query.answer("Correct!")
            await self._verify_user(user_id, context)
        else:
            info["attempts"] -= 1
            if info["attempts"] <= 0:
                await query.answer("Wrong answer.")
                await self._fail_verification(user_id, context)
            else:
                await query.answer("Wrong, try again.")
                question, options, correct = self._generate_math_captcha()
                info["correct"] = correct
                keyboard = InlineKeyboardMarkup(
                    [[InlineKeyboardButton(str(opt), callback_data=f"captcha_math:{user_id}:{opt}") for opt in options]]
                )
                text = f"Solve: {question} = ?\nAttempts left: {info['attempts']}"
                try:
                    await query.edit_message_text(text, reply_markup=keyboard)
                except Exception:
                    pass

    async def refinement_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        query = update.callback_query
        if not query or not query.data:
            return
        parts = query.data.split(":")
        if len(parts) != 2:
            await query.answer()
            return
        _, action = parts
        handler = self.refinement_handlers.get(action)
        await query.answer()
        if handler:
            await handler(update, context)

    async def handle_voice_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.voice:
            return
        
        # --- ADDED THIS CHECK ---
        # Ensure bot only processes voice in allowed groups or private chats
        chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ['group', 'supergroup']
        if is_group and self.allowed_group_ids and chat_id not in self.allowed_group_ids:
            logger.debug(f"handle_voice_message: Skipping voice message from unallowed group {chat_id}.")
            return
        # --- END CHECK ---

        # Set language context to Russian for the response, or detect from user's profile if available
        # For this feature, we will default to Russian for the bot's replies.
        self.language_service._set_current_language_internals('ru')
        
        await update.message.reply_text(self.language_service.get_response_string("voice_message_received"))
        
        voice_file = await update.message.voice.get_file()
        
        temp_dir = "temp_audio"
        os.makedirs(temp_dir, exist_ok=True)
        temp_file_path = os.path.join(temp_dir, f"{uuid.uuid4()}.oga")
        
        try:
            await voice_file.download_to_drive(temp_file_path)
            
            transcribed_text = await self.llm_services.transcribe_audio(temp_file_path)
            
            if not transcribed_text:
                await update.message.reply_text(self.language_service.get_response_string("voice_transcription_failed"))
                return
            
            transcription_header = self.language_service.get_response_string("voice_transcription_header")
            await update.message.reply_text(f"*{transcription_header}*\n\n`{transcribed_text}`", parse_mode='MarkdownV2')
            
            is_russian = bool(re.search('[а-яА-Я]', transcribed_text))
            
            if not is_russian:
                logger.info(f"Transcribed text is not Russian. Proceeding with translation.")
                
                translation_prompts = self.language_service.get_llm_prompt_set("text_translator")
                
                if translation_prompts and "system" in translation_prompts and "user_template" in translation_prompts:
                    translated_text = await self.response_generator.translate_text(
                        text_to_translate=transcribed_text,
                        target_language="Russian",
                        system_prompt=translation_prompts["system"],
                        user_prompt_template=translation_prompts["user_template"]
                    )
                    
                    if translated_text:
                        translation_header = self.language_service.get_response_string("voice_translation_header")
                        await update.message.reply_text(f"*{translation_header}*\n\n`{translated_text}`", parse_mode='MarkdownV2')
                    else:
                        logger.error("Translation failed for transcribed text.")
                else:
                    logger.error("Could not find 'text_translator' prompts in language pack.")

        except Exception as e:
            logger.error(f"Error processing voice message: {e}", exc_info=True)
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
        finally:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                logger.info(f"Cleaned up temporary audio file: {temp_file_path}")

    async def handle_video_note_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.video_note:
            return

        chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ['group', 'supergroup']
        if is_group and self.allowed_group_ids and chat_id not in self.allowed_group_ids:
            logger.debug(f"handle_video_note_message: Skipping video note from unallowed group {chat_id}.")
            return

        self.language_service._set_current_language_internals('ru')
        await update.message.reply_text(self.language_service.get_response_string("video_message_received"))

        video_file = await update.message.video_note.get_file()

        temp_dir = "temp_audio"
        os.makedirs(temp_dir, exist_ok=True)
        temp_file_path = os.path.join(temp_dir, f"{uuid.uuid4()}.mp4")

        try:
            await video_file.download_to_drive(temp_file_path)

            transcribed_text = await self.llm_services.transcribe_audio(temp_file_path)

            if not transcribed_text:
                await update.message.reply_text(self.language_service.get_response_string("video_transcription_failed"))
                return

            transcription_header = self.language_service.get_response_string("video_transcription_header")
            await update.message.reply_text(f"*{transcription_header}*\n\n`{transcribed_text}`", parse_mode='MarkdownV2')

            is_russian = bool(re.search('[а-яА-Я]', transcribed_text))

            if not is_russian:
                logger.info("Transcribed text is not Russian. Proceeding with translation.")

                translation_prompts = self.language_service.get_llm_prompt_set("text_translator")

                if translation_prompts and "system" in translation_prompts and "user_template" in translation_prompts:
                    translated_text = await self.response_generator.translate_text(
                        text_to_translate=transcribed_text,
                        target_language="Russian",
                        system_prompt=translation_prompts["system"],
                        user_prompt_template=translation_prompts["user_template"]
                    )

                    if translated_text:
                        translation_header = self.language_service.get_response_string("video_translation_header")
                        await update.message.reply_text(f"*{translation_header}*\n\n`{translated_text}`", parse_mode='MarkdownV2')
                    else:
                        logger.error("Translation failed for transcribed video note.")
                else:
                    logger.error("Could not find 'text_translator' prompts in language pack.")

        except Exception as e:
            logger.error(f"Error processing video note: {e}", exc_info=True)
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
        finally:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                logger.info(f"Cleaned up temporary audio file: {temp_file_path}")

    async def handle_photo_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message:
            return

        chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ['group', 'supergroup']
        if is_group and self.allowed_group_ids and chat_id not in self.allowed_group_ids:
            logger.debug(f"handle_photo_message: Skipping photo from unallowed group {chat_id}.")
            return

        if not await self.db_manager.get_nsfw_filter_enabled(chat_id):
            return

        file_obj = None
        ext = 'jpg'
        if update.message.photo:
            photo = update.message.photo[-1]
            file_obj = await photo.get_file()
        elif update.message.document and update.message.document.mime_type and update.message.document.mime_type.startswith('image/'):
            file_obj = await update.message.document.get_file()
            if update.message.document.file_name and '.' in update.message.document.file_name:
                ext = update.message.document.file_name.rsplit('.', 1)[-1]
        else:
            return

        temp_dir = 'temp_images'
        os.makedirs(temp_dir, exist_ok=True)
        temp_file_path = os.path.join(temp_dir, f"{uuid.uuid4()}.{ext}")
        await file_obj.download_to_drive(temp_file_path)

        try:
            if self.nsfw_classifier is None:
                if NudeClassifier is None:
                    logger.error("NSFW classifier library not installed. Install nudenet to enable filtering.")
                    return
                try:
                    self.nsfw_classifier = NudeClassifier()
                except Exception as e:
                    logger.error(f"Failed to initialize NSFW classifier: {e}", exc_info=True)
                    return

            result = self.nsfw_classifier.classify(temp_file_path)
            nsfw_score = result.get(temp_file_path, {}).get('unsafe', 0)
            threshold = await self.db_manager.get_nsfw_threshold(chat_id)
            if nsfw_score >= threshold:
                try:
                    await update.message.delete()
                    if update.effective_user:
                        text = self.language_service.get_response_string(
                            "nsfw_image_removed_user",
                            user=update.effective_user.mention_html(),
                        )
                        await context.bot.send_message(
                            chat_id,
                            text,
                            parse_mode='HTML',
                        )
                    else:
                        text = self.language_service.get_response_string(
                            "nsfw_image_removed_chat"
                        )
                        await context.bot.send_message(chat_id, text)
                except Exception as e:
                    logger.error(f"Error deleting NSFW image: {e}", exc_info=True)
        finally:
            try:
                os.remove(temp_file_path)
            except Exception:
                pass

    async def _handle_karma_vote(self, update: Update) -> bool:
        """Process simple text-based karma votes."""
        message = update.message
        if not message or not message.reply_to_message or not message.text:
            return False
        giver = update.effective_user
        receiver = message.reply_to_message.from_user if message.reply_to_message else None
        if not giver or not receiver:
            return False
        vote_result = await self.karma_manager.handle_text_vote(
            giver.id, receiver.id, message.chat_id, message.text
        )
        if vote_result is None:
            return False
        result, delta = vote_result
        receiver_display = f"@{receiver.username}" if receiver.username else receiver.first_name
        if result == "self_karma_error":
            await message.reply_text(
                self.language_service.get_response_string(
                    "karma_self_vote_error", "🤷 You can't vote on yourself."
                )
            )
        elif result == "cooldown_error":
            await message.reply_text(
                self.language_service.get_response_string(
                    "karma_vote_cooldown",
                    "⏱ You can vote for this user again later.",
                )
            )
        elif result == "karma_changed_success":
            stats = await self.karma_manager.get_user_stats(receiver.id)
            total = stats["received"] if stats else 0
            sign = "+" if delta > 0 else ""
            await message.reply_text(
                self.language_service.get_response_string(
                    "karma_vote_success",
                    f"{sign}{delta} to {receiver_display} (total {total:+})",
                    sign=sign,
                    delta=delta,
                    receiver=receiver_display,
                    total=total,
                )
            )
        return True

    async def _handle_regenerate_reaction(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """Handle regeneration request via emoji reaction."""
        reactor = getattr(getattr(update, "message_reaction", None), "user", None)
        lang_code = getattr(reactor, "language_code", None)
        if lang_code and lang_code in self.language_service.language_packs:
            self.language_service._set_current_language_internals(lang_code)
        else:
            self.language_service._set_current_language_internals(self.language_service.default_language)
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text=self.language_service.get_response_string(
                "reaction_regenerating", "Regenerating response..."
            ),
        )

    async def _handle_expand_reaction(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """Handle expansion request via emoji reaction."""
        reactor = getattr(getattr(update, "message_reaction", None), "user", None)
        lang_code = getattr(reactor, "language_code", None)
        if lang_code and lang_code in self.language_service.language_packs:
            self.language_service._set_current_language_internals(lang_code)
        else:
            self.language_service._set_current_language_internals(self.language_service.default_language)
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text=self.language_service.get_response_string(
                "reaction_expanding", "Expanding on previous response..."
            ),
        )

    async def _handle_summary_reaction(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """Handle summarization request via emoji reaction."""
        reactor = getattr(getattr(update, "message_reaction", None), "user", None)
        lang_code = getattr(reactor, "language_code", None)
        if lang_code and lang_code in self.language_service.language_packs:
            self.language_service._set_current_language_internals(lang_code)
        else:
            self.language_service._set_current_language_internals(self.language_service.default_language)
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text=self.language_service.get_response_string(
                "reaction_summary", "Generating summary..."
            ),
        )

    async def reaction_router(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """Route emoji reactions to configured actions."""
        reaction_update = getattr(update, "message_reaction", None)
        if not reaction_update or not reaction_update.new_reaction:
            return
        emoji = reaction_update.new_reaction[0].emoji
        try:
            msg = getattr(reaction_update, "message", None)
            rater = getattr(reaction_update, "user", None)
            if msg and msg.from_user and rater:
                if context.bot and msg.from_user.id == context.bot.id:
                    self.recent_reactors[rater.id] = datetime.utcnow()
                await self.karma_manager.record_reaction_event(
                    chat_id=update.effective_chat.id,
                    msg_id=msg.message_id,
                    target_user_id=msg.from_user.id,
                    rater_user_id=rater.id,
                    emoji=emoji,
                )
        except Exception as exc:
            logger.error(f"Karma reaction logging failed: {exc}")
        handler = self.reaction_handlers.get(emoji)
        if handler:
            await handler(update, context)

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        if not update.message or not update.message.text or not update.effective_chat or not update.effective_user:
            return None

        chat_id = update.effective_chat.id
        if chat_id in self.chat_languages:
            self.language_service._set_current_language_internals(self.chat_languages[chat_id])
        else:
            await self.language_service.determine_language_context(
                update.message.text,
                chat_id=chat_id,
                update_context=update
            )

        await self.log_message_and_profile_tasks(update, context)

        if await self._handle_karma_vote(update):
            return None

        text = update.message.text
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        user_id = update.effective_user.id
        is_reply_to_bot = (
            update.message.reply_to_message
            and update.message.reply_to_message.from_user
            and context.bot
            and update.message.reply_to_message.from_user.id == context.bot.id
        )
        reason = ""
        alias = ""
        now = datetime.utcnow()
        recent_react = self.recent_reactors.get(user_id)
        if recent_react and now - recent_react < timedelta(seconds=30):
            triggered = True
            user_msg_txt = text.strip()
            reason = "reaction"
            self.recent_reactors.pop(user_id, None)
        elif is_reply_to_bot:
            triggered = True
            user_msg_txt = text.strip()
            reason = "reply"
        else:
            triggered, user_msg_txt, alias = extract_assistant_prompt(text, self.bot_nicknames, bot_username_lower)
            reason = "name" if triggered else ""
            if not triggered:
                triggered = await self._is_triggered(update, context, text.lower())
                if triggered:
                    reason = "forward" if is_forwarded_message(update.message) else "mention"
                    user_msg_txt = text.strip()

        if not triggered:
            return None

        if not user_msg_txt:
            nudge = self.language_service.get_response_string("assistant_prompt_nudge", "Я здесь. О чём рассказать? 🙂")
            await update.message.reply_text(nudge)
            await self.db_manager.log_assistant_invocation(chat_id, user_id, update.message.message_id, True, alias, user_msg_txt, reason, self.language_service.current_lang, False, False, None)
            return None

        user_msg_txt_lower = user_msg_txt.lower()

        current_conv_state = context.user_data.get('conversation_state') if context.user_data else None
        pending_action_details = self.pending_action_data.get(chat_id)

        if current_conv_state == ASK_CITY and pending_action_details and pending_action_details.get("action_type") == "ask_city_weather":
            self.pending_action_data.pop(chat_id, None)
            context.user_data.pop('conversation_state', None)
            original_msg_id = pending_action_details.get("original_message_id")
            return await self.weather_handler.handle_city_response(update, context, original_msg_id)
        elif current_conv_state == ASK_NEWS_TOPIC and pending_action_details and pending_action_details.get("action_type") == "ask_news_topic":
            self.pending_action_data.pop(chat_id, None)
            context.user_data.pop('conversation_state', None)
            original_msg_id = pending_action_details.get("original_message_id")
            return await self.news_handler.handle_topic_response(update, context, original_msg_id)

        draw_request = bool(re.match(r'^\s*draw\b', user_msg_txt_lower))
        if await self.spam_detector.inspect_message(update, context):
            return ConversationHandler.END

        master_intent_prompts = self.language_service.get_llm_prompt_set("master_intent_classifier")
        master_intent = "UNKNOWN_INTENT"
        if master_intent_prompts and "system" in master_intent_prompts:
            user_template_for_master = master_intent_prompts.get("user_template","{text_to_classify}")
            master_intent = await self.intent_recognizer.classify_master_intent(
                text=user_msg_txt, lang_code=self.language_service.current_lang,
                system_prompt=master_intent_prompts["system"], user_prompt_template=user_template_for_master )
        else: 
            logger.error(f"Master intent classification prompt set missing/malformed for lang '{self.language_service.current_lang}'.")
        
        if draw_request:
            master_intent = "IMAGE_GENERATION_QUERY"

        logger.info(f"Master Intent for '{user_msg_txt[:50]}...' (lang: {self.language_service.current_lang}) classified as: {master_intent}")
        # Mirror key intent decisions to the console
        print(
            f"ACTION[{chat_id}] intent={master_intent} reason={reason}"
        )

        next_state: Optional[int] = None
        if master_intent == "WEATHER_QUERY":
            context.user_data['conversation_state'] = ASK_CITY
            next_state = await self.weather_handler.handle_intent(update, context, user_msg_txt)
        elif master_intent == "NEWS_QUERY": 
            context.user_data['conversation_state'] = ASK_NEWS_TOPIC
            next_state = await self.news_handler.handle_intent(update, context, user_msg_txt)
        elif master_intent == "IMAGE_GENERATION_QUERY": 
            await self.image_generation_handler.handle_intent(update, context, user_msg_txt)
            next_state = ConversationHandler.END 
        elif master_intent == "MESSAGE_ANALYSIS_QUERY": 
            await self._handle_message_analysis_query(update, context, user_msg_txt) 
        elif master_intent in ["USER_PROFILE_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]:
            await self.general_handler.handle_request(update, context, user_msg_txt, master_intent)
        else:
            logger.warning(f"Unhandled master intent type: {master_intent}. Falling back to general handler.")
            await self.general_handler.handle_request(update, context, user_msg_txt, "UNKNOWN_INTENT")

        await self.db_manager.log_assistant_invocation(
            chat_id,
            user_id,
            update.message.message_id,
            True,
            alias,
            user_msg_txt,
            reason,
            self.language_service.current_lang,
            True,
            True,
            None,
        )

        if next_state is None or next_state == ConversationHandler.END:
            if context.user_data and 'conversation_state' in context.user_data:
                context.user_data.pop('conversation_state')
            if chat_id in self.pending_action_data:
                 self.pending_action_data.pop(chat_id, None)
        return next_state

    async def _handle_message_analysis_query(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        is_valid_reply_scenario = (
            update.message and update.message.reply_to_message and
            (update.message.reply_to_message.text or update.message.reply_to_message.caption) and
            update.message.reply_to_message.from_user and context.bot and
            update.message.reply_to_message.from_user.id != context.bot.id )
        if not is_valid_reply_scenario:
            logger.info("MESSAGE_ANALYSIS_QUERY classified, but not valid reply. Delegating to GeneralIntentHandler.")
            await self.general_handler.handle_request(update, context, user_msg_txt, "GENERAL_CHAT")
            return
        
        logger.info("TelegramHandlers: Processing MESSAGE_ANALYSIS_QUERY directly.")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        original_msg = update.message.reply_to_message
        original_text = original_msg.text or original_msg.caption or ""
        question_for_analysis = user_msg_txt
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        cleaned_question = user_msg_txt.lower()
        for nick in self.bot_nicknames + ([f"@{bot_username_lower}"] if bot_username_lower else []): 
            cleaned_question = cleaned_question.replace(nick.lower(), "").strip()
        if len(cleaned_question) < 5: 
            question_for_analysis = self.language_service.get_response_string("replied_message_default_question")
        
        if is_forwarded_message(original_msg):
            analyzer_prompts = self.language_service.get_llm_prompt_set("forwarded_news_fact_checker")
            if not (analyzer_prompts and "system" in analyzer_prompts):
                logger.error("Prompt set for forwarded news fact-check is missing or malformed.")
                await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
                return
            # Detect language of the forwarded text so the deep research uses that language
            await self.language_service.determine_language_context(
                original_text, update.effective_chat.id
            )
            analysis_result = await self.response_generator.fact_check_forwarded_message(
                forwarded_text=original_text,
                user_question=question_for_analysis,
                system_prompt=analyzer_prompts["system"],
                user_prompt_template=analyzer_prompts.get("user_template"),
                fallback_text=self.language_service.get_response_string(
                    "analysis_unavailable", "Analysis unavailable."
                ),
            )
        else:
            analyzer_prompts = self.language_service.get_llm_prompt_set("replied_message_analyzer")
            if not (analyzer_prompts and "system" in analyzer_prompts):
                logger.error("Prompt set for replied message analysis is missing or malformed.")
                await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
                return
            analysis_result = await self.response_generator.analyze_replied_message(
                original_text=original_text,
                user_question=question_for_analysis,
                system_prompt=analyzer_prompts["system"],
                user_prompt_template=analyzer_prompts.get("user_template")
            )
        await update.message.reply_text(analysis_result)
        
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.effective_user or not update.message: return
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE) 
        await update.message.reply_html(self.language_service.get_response_string("start", user_mention=update.effective_user.mention_html()))

    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """Send help text to the user regardless of message type."""
        chat_id = update.effective_chat.id if update.effective_chat else None
        if chat_id is None:
            return
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE)
        intro = self.language_service.get_response_string("help")
        commands_text = "\n".join(
            f"/{name} - {info['long']}" for name, info in self.default_commands.items()
        )
        text = f"{intro}\n\n**Commands:**\n{commands_text}"
        if update.message:
            await update.message.reply_text(text)
        else:
            await context.bot.send_message(chat_id=chat_id, text=text)

    async def chat_stats_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        chat_id = update.effective_chat.id if update.effective_chat else None
        if chat_id is None:
            return
        stats = await self.stats_manager.get_chat_stats(chat_id)
        if not stats:
            if update.message:
                await update.message.reply_text("No statistics available yet.")
            else:
                await context.bot.send_message(chat_id=chat_id, text="No statistics available yet.")
            return
        lines = [
            "Chat Statistics:",
            f"Total messages: {stats['total_messages']}",
            f"Joins: {stats['joins']} | Leaves: {stats['leaves']}"
        ]
        if stats['top_users']:
            lines.append("Top users:")
            for u in stats['top_users']:
                name = f"@{u['username']}" if u.get('username') else f"@{u['user_id']}"
                lines.append(f"- {mention_html(u['user_id'], name)}: {u['count']}")
        if stats['top_links']:
            lines.append("Top links:")
            for l in stats['top_links']:
                lines.append(f"- {l['domain']}: {l['count']}")
        output = "\n".join(lines)
        if update.message:
            await update.message.reply_html(output)
        else:
            await context.bot.send_message(chat_id=chat_id, text=output, parse_mode='HTML')

    async def my_stats_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.effective_chat or not update.effective_user:
            return
        stats = await self.stats_manager.get_user_stats(update.effective_chat.id, update.effective_user.id)
        if not stats:
            await update.message.reply_text("No statistics available for you yet.")
            return
        total = stats['total_messages'] or 1
        percent = (stats['messages'] / total) * 100
        first_seen = stats['first_seen'] or datetime.utcnow()
        days = max((datetime.utcnow() - first_seen).days + 1, 1)
        avg_per_day = stats['messages'] / days
        lines = [
            f"You have sent {stats['messages']} messages ({percent:.1f}% of total).",
            f"First seen: {first_seen:%Y-%m-%d}",
            f"Average per day: {avg_per_day:.1f}",
            f"Rank: {stats['rank']} of {stats['total_users']}"
        ]
        await update.message.reply_text("\n".join(lines))

    async def user_stats_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.effective_chat:
            return
        target_user = None
        if update.message.reply_to_message and update.message.reply_to_message.from_user:
            target_user = update.message.reply_to_message.from_user
        elif context.args:
            arg = context.args[0]
            if arg.startswith('@'):
                username = arg[1:].lower()
                stats = self.stats_manager.memory_stats.get(update.effective_chat.id, {})
                for uid, info in stats.get('users', {}).items():
                    if info.get('username') and info['username'].lower() == username:
                        target_user = SimpleNamespace(id=uid, username=info['username'], full_name=info['username'])
                        break
            elif arg.isdigit():
                try:
                    member = await context.bot.get_chat_member(update.effective_chat.id, int(arg))
                    target_user = member.user
                except Exception:
                    target_user = SimpleNamespace(id=int(arg), username=None, full_name=arg)
        if not target_user:
            await update.message.reply_text("Reply to a user's message or provide a user ID/username.")
            return
        stats = await self.stats_manager.get_user_stats(update.effective_chat.id, target_user.id)
        if not stats:
            await update.message.reply_text("No statistics available for this user yet.")
            return
        total = stats['total_messages'] or 1
        percent = (stats['messages'] / total) * 100
        first_seen = stats['first_seen'] or datetime.utcnow()
        days = max((datetime.utcnow() - first_seen).days + 1, 1)
        avg_per_day = stats['messages'] / days
        name = target_user.full_name if getattr(target_user, 'full_name', None) else (target_user.username or str(target_user.id))
        lines = [
            f"Stats for {name}:",
            f"Messages: {stats['messages']} ({percent:.1f}% of total)",
            f"First seen: {first_seen:%Y-%m-%d}",
            f"Average per day: {avg_per_day:.1f}",
            f"Rank: {stats['rank']} of {stats['total_users']}"
        ]
        await update.message.reply_text("\n".join(lines))

    async def karma_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        chat_id = update.effective_chat.id if update.effective_chat else None
        if chat_id is None:
            return
        top_users = await self.karma_manager.get_top_users(chat_id)
        if not top_users:
            if update.message:
                await update.message.reply_text("No karma data available.")
            else:
                await context.bot.send_message(chat_id=chat_id, text="No karma data available.")
            return
        lines = ["Karma leaderboard:"]
        for idx, u in enumerate(top_users, start=1):
            lines.append(f"{idx}. {u['name']}: {u['score']}")
        output = "\n".join(lines)
        if update.message:
            await update.message.reply_text(output)
        else:
            await context.bot.send_message(chat_id=chat_id, text=output)

    async def language_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message:
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        if not context.args:
            await update.message.reply_text("Usage: /language <code>")
            return
        lang_code = context.args[0].lower()
        if lang_code not in self.language_service.language_packs:
            await update.message.reply_text("Unsupported language code.")
            return
        self.chat_languages[chat_id] = lang_code
        self.language_service._set_current_language_internals(lang_code)
        await update.message.reply_text(f"Language for this chat set to {lang_code}.")

    async def reload_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message:
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        self.chat_languages.pop(chat_id, None)
        self.stats_manager.memory_stats.pop(chat_id, None)
        self.features_db.pop(chat_id, None)
        await self.refresh_chat_commands(chat_id)
        await update.message.reply_text("Chat data reloaded.")

    async def news_command_entry(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        if not update.message or not update.effective_user: return ConversationHandler.END
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE)
        context.user_data['conversation_state'] = ASK_NEWS_TOPIC
        return await self.news_handler.handle_command_entry(update, context)

    async def report_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        await self.community_moderation.cmd_report(update, context)

    async def spam_vote_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        await self.community_moderation.cmd_vote(update, context, reason="spam")

    async def ban_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to ban them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        try:
            await context.bot.ban_chat_member(chat_id, target.id)
            await update.message.reply_html(
                f"User {target.mention_html()} has been banned by {update.effective_user.mention_html()}.")
        except Exception as e:
            logger.error(f"Failed to ban user {target.id}: {e}")
            await update.message.reply_text("Failed to ban user.")

    async def qban_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to ban them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        try:
            await context.bot.ban_chat_member(chat_id, target.id)
        except Exception as e:
            logger.error(f"Failed to ban user {target.id}: {e}")
            await update.message.reply_text("Failed to ban user.")
            return
        try:
            await update.message.reply_to_message.delete()
        except Exception:
            pass
        try:
            await update.message.delete()
        except Exception:
            pass

    async def unban_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to unban them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        try:
            await context.bot.unban_chat_member(chat_id, target.id)
            await update.message.reply_html(
                f"User {target.mention_html()} has been unbanned by {update.effective_user.mention_html()}.")
        except Exception as e:
            logger.error(f"Failed to unban user {target.id}: {e}")
            await update.message.reply_text("Failed to unban user.")

    async def baninfo_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to get ban info.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        try:
            member = await context.bot.get_chat_member(chat_id, target.id)
            if member.status == "kicked":
                if member.until_date:
                    await update.message.reply_html(
                        f"User {target.mention_html()} is banned until {member.until_date}.")
                else:
                    await update.message.reply_html(
                        f"User {target.mention_html()} is banned permanently.")
            else:
                await update.message.reply_text("User is not banned.")
        except Exception as e:
            logger.error(f"Failed to get ban info for user {target.id}: {e}")
            await update.message.reply_text("Failed to get ban info.")

    async def kick_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to kick them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        try:
            await context.bot.ban_chat_member(chat_id, target.id)
            await context.bot.unban_chat_member(chat_id, target.id)
            await update.message.reply_html(
                f"User {target.mention_html()} has been kicked by {update.effective_user.mention_html()}.")
        except Exception as e:
            logger.error(f"Failed to kick user {target.id}: {e}")
            await update.message.reply_text("Failed to kick user.")

    async def mute_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to mute them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        duration = 10
        if context.args and context.args[0].isdigit():
            duration = int(context.args[0])
        until_date = datetime.utcnow() + timedelta(minutes=duration)
        perms = ChatPermissions(
            can_send_messages=False,
            can_send_audios=False,
            can_send_documents=False,
            can_send_photos=False,
            can_send_videos=False,
            can_send_video_notes=False,
            can_send_voice_notes=False,
            can_send_polls=False,
            can_send_other_messages=False,
            can_add_web_page_previews=False,
        )
        try:
            await context.bot.restrict_chat_member(chat_id, target.id, permissions=perms, until_date=until_date)
            await update.message.reply_html(
                f"User {target.mention_html()} has been muted for {duration} minutes.")
        except Exception as e:
            logger.error(f"Failed to mute user {target.id}: {e}")
            await update.message.reply_text("Failed to mute user.")

    async def unmute_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to unmute them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        perms = ChatPermissions(
            can_send_messages=True,
            can_send_audios=True,
            can_send_documents=True,
            can_send_photos=True,
            can_send_videos=True,
            can_send_video_notes=True,
            can_send_voice_notes=True,
            can_send_polls=True,
            can_send_other_messages=True,
            can_add_web_page_previews=True,
        )
        try:
            await context.bot.restrict_chat_member(chat_id, target.id, permissions=perms)
            await update.message.reply_html(
                f"User {target.mention_html()} has been unmuted.")
        except Exception as e:
            logger.error(f"Failed to unmute user {target.id}: {e}")
            await update.message.reply_text("Failed to unmute user.")

    async def muteinfo_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to get mute info.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        try:
            member = await context.bot.get_chat_member(chat_id, target.id)
            can_send = getattr(member, 'can_send_messages', True)
            if member.status == "restricted" or not can_send:
                if member.until_date:
                    await update.message.reply_html(
                        f"User {target.mention_html()} is muted until {member.until_date}.")
                else:
                    await update.message.reply_html(
                        f"User {target.mention_html()} is muted indefinitely.")
            else:
                await update.message.reply_text("User is not muted.")
        except Exception as e:
            logger.error(f"Failed to get mute info for user {target.id}: {e}")
            await update.message.reply_text("Failed to get mute info.")

    async def warn_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to warn them.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        reason = " ".join(context.args) if context.args else None
        count = await self.db_manager.add_warning(chat_id, target.id, reason)
        await update.message.reply_html(
            f"\u26A0\uFE0F {target.mention_html()} has been warned (Warn #{count}).")
        if count >= 3:
            try:
                await context.bot.ban_chat_member(chat_id, target.id)
                await update.message.reply_html(
                    f"User {target.mention_html()} was banned after reaching 3 warnings.")
            except Exception as e:
                logger.error(f"Auto-ban after warnings failed for user {target.id}: {e}")

    async def warns_list_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message:
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        if update.message.reply_to_message:
            target = update.message.reply_to_message.from_user
            count = await self.db_manager.get_warning_count(chat_id, target.id)
            await update.message.reply_text(f"User {target.id} has {count} warning(s).")
            return
        rows = await self.db_manager.list_warnings(chat_id)
        if not rows:
            await update.message.reply_text("No warnings recorded.")
            return
        lines = [f"{user_id}: {warns}" for user_id, warns in rows]
        await update.message.reply_text("Warnings:\n" + "\n".join(lines))

    async def remove_warn_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.message.reply_text("Reply to a user's message to clear their warnings.")
            return
        chat_id = update.effective_chat.id
        invoker_id = update.effective_user.id if update.effective_user else 0
        if not await self._is_user_admin(chat_id, invoker_id, context):
            return
        target = update.message.reply_to_message.from_user
        await self.db_manager.clear_warnings(chat_id, target.id)
        await update.message.reply_text(f"Warnings cleared for user {target.id}.")

    async def toggle_nsfw_filter_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.effective_user:
            return
        chat_id = update.effective_chat.id
        user_id = update.effective_user.id
        if not await self._is_user_admin(chat_id, user_id, context):
            return
        if not context.args or context.args[0].lower() not in ("on", "off"):
            await update.message.reply_text(
                self.language_service.get_response_string("toggle_nsfw_usage")
            )
            return
        enabled = context.args[0].lower() == "on"
        await self.db_manager.set_nsfw_filter_enabled(chat_id, enabled)
        status_key = "enabled" if enabled else "disabled"
        status_txt = self.language_service.get_response_string(f"status_{status_key}")
        await update.message.reply_text(
            self.language_service.get_response_string(
                "nsfw_filter_status", status=status_txt
            )
        )

    async def set_nsfw_threshold_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.effective_user:
            return
        chat_id = update.effective_chat.id
        user_id = update.effective_user.id
        if not await self._is_user_admin(chat_id, user_id, context):
            return
        if not context.args:
            await update.message.reply_text(
                self.language_service.get_response_string("nsfw_threshold_usage")
            )
            return
        try:
            threshold = float(context.args[0])
        except ValueError:
            await update.message.reply_text(
                self.language_service.get_response_string("nsfw_threshold_must_number")
            )
            return
        threshold = max(0.0, min(1.0, threshold))
        await self.db_manager.set_nsfw_threshold(chat_id, threshold)
        await update.message.reply_text(
            self.language_service.get_response_string(
                "nsfw_threshold_set", threshold=threshold
            )
        )

    async def set_spam_threshold_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.effective_user:
            return
        chat_id = update.effective_chat.id
        user_id = update.effective_user.id
        try:
            member = await context.bot.get_chat_member(chat_id, user_id)
            if member.status not in ("administrator", "creator"):
                return
        except Exception:
            return
        if not context.args or not context.args[0].isdigit():
            await update.message.reply_text(
                self.language_service.get_response_string("set_spam_threshold_usage")
            )
            return
        threshold = int(context.args[0])
        await self.db_manager.set_spam_vote_threshold(chat_id, threshold)
        await update.message.reply_text(
            self.language_service.get_response_string(
                "spam_threshold_set", threshold=threshold
            )
        )

    async def push_default_commands(self) -> None:
        """Registers global default slash commands."""
        commands = [
            BotCommand(name, info["short"]) for name, info in self.default_commands.items()
        ]
        await self.application.bot.set_my_commands(commands, scope=BotCommandScopeDefault())

    async def refresh_chat_commands(self, chat_id: int) -> None:
        """Refreshes command list for a specific chat based on feature flags."""
        cfg = self.features_db.get(chat_id, {})
        cmds = [
            BotCommand("stat", "Show chat statistics"),
            BotCommand("report", "Report a message"),
            BotCommand("karma", "Show karma leaderboard"),
        ]
        if cfg.get("ai"):
            cmds.append(BotCommand("ask", "Ask AI a question"))
            cmds.append(BotCommand("draw", "Generate an image with AI"))
        if cfg.get("captcha"):
            cmds.append(BotCommand("captcha", "Show captcha settings"))
        await self.application.bot.set_my_commands(cmds, scope=BotCommandScopeChat(chat_id))
        admin_cmds = [
            BotCommand("ban", "Ban the replied user"),
            BotCommand("qban", "Quick ban and delete"),
            BotCommand("unban", "Unban the replied user"),
            BotCommand("baninfo", "Ban info of the replied user"),
            BotCommand("kick", "Kick the replied user"),
            BotCommand("mute", "Mute the replied user"),
            BotCommand("unmute", "Unmute the replied user"),
            BotCommand("muteinfo", "Mute info of the replied user"),
            BotCommand("warn", "Warn the replied user"),
            BotCommand("rm_warn", "Remove user warnings"),
            BotCommand("warns_list", "List user warnings"),
            BotCommand("language", "Set chat language"),
            BotCommand("reload", "Reload chat data"),
        ]
        await self.application.bot.set_my_commands(admin_cmds, scope=BotCommandScopeChatAdministrators(chat_id))

    async def toggle_ai_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        """Admin command to toggle AI features and refresh commands."""
        if not update.message or not update.effective_user:
            return
        chat_id = update.effective_chat.id
        user_id = update.effective_user.id
        if not await self._is_user_admin(chat_id, user_id, context):
            return
        self.features_db.setdefault(chat_id, {})
        self.features_db[chat_id]["ai"] = not self.features_db[chat_id].get("ai", False)
        await self.refresh_chat_commands(chat_id)
        status = "ON" if self.features_db[chat_id]["ai"] else "OFF"
        await update.message.reply_text(f"AI feature is now {status}.")

    async def error_handler(self, update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
        logger.error(f'Update "{update}" caused error "{context.error}"', exc_info=True)
        if isinstance(update, Update) and update.effective_chat:
            try:
                error_msg = self.language_service.get_response_string("generic_error_message", "Oops! Something went very wrong on my end.")
                await context.bot.send_message(chat_id=update.effective_chat.id, text=error_msg)
            except Exception as e: 
                logger.error(f"CRITICAL: Error sending error message to user: {e}", exc_info=True)
    
    async def cancel_conversation(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
        chat_id = update.effective_chat.id
        if chat_id in self.pending_action_data:
            del self.pending_action_data[chat_id]
        if context.user_data and 'conversation_state' in context.user_data:
            context.user_data.pop('conversation_state')

        logger.info(f"User {update.effective_user.id if update.effective_user else ''} cancelled conversation.")
        if update.message:
            await update.message.reply_text(
                self.language_service.get_response_string("conversation_cancelled", "Okay, current operation cancelled."),
                reply_markup=ReplyKeyboardRemove()
            )
        return ConversationHandler.END

    def _register_command(self, names, handler, short_desc: str, long_desc: str) -> None:
        """Register command handler and store its descriptions."""
        primary = names[0] if isinstance(names, (list, tuple, set)) else names
        if handler is not None:
            self.application.add_handler(CommandHandler(names, handler))
        self.default_commands[primary] = {"short": short_desc, "long": long_desc}

    def register_all_handlers(self):
        self._register_command("start", self.start_command, "Start the bot", "Start interaction")
        self._register_command("help", self.help_command, "How to use the bot", "This help message")
        self._register_command("report", self.report_command, "Report a message", "Report a message")
        self._register_command(["spam", "voteban"], self.spam_vote_command, "Vote to ban a spammer", "Vote to ban a spammer")
        self.application.add_handler(CommandHandler("ban", self.ban_command))
        self.application.add_handler(CommandHandler("qban", self.qban_command))
        self.application.add_handler(CommandHandler(["unban", "pardon"], self.unban_command))
        self.application.add_handler(CommandHandler("baninfo", self.baninfo_command))
        self.application.add_handler(CommandHandler("kick", self.kick_command))
        self.application.add_handler(CommandHandler("mute", self.mute_command))
        self.application.add_handler(CommandHandler("unmute", self.unmute_command))
        self.application.add_handler(CommandHandler("muteinfo", self.muteinfo_command))
        self._register_command(["stat", "stats"], self.chat_stats_command, "Chat statistics", "Show chat statistics")
        self._register_command("mystat", self.my_stats_command, "Your statistics", "Show your statistics")
        self.application.add_handler(CommandHandler("userstats", self.user_stats_command))
        self._register_command("karma", self.karma_command, "Show karma leaderboard", "Show karma leaderboard")
        self.application.add_handler(CommandHandler("language", self.language_command))
        self.application.add_handler(CommandHandler("reload", self.reload_command))
        self.application.add_handler(CommandHandler("warn", self.warn_command))
        self.application.add_handler(CommandHandler("warns_list", self.warns_list_command))
        self.application.add_handler(CommandHandler(["rm_warn", "clear_warn"], self.remove_warn_command))
        self.application.add_handler(CommandHandler("toggle_nsfw", self.toggle_nsfw_filter_command))
        self.application.add_handler(CommandHandler("nsfw_threshold", self.set_nsfw_threshold_command))
        self.application.add_handler(CommandHandler("setspamthreshold", self.set_spam_threshold_command))
        self.application.add_handler(CommandHandler("toggle_ai", self.toggle_ai_command))
        self.application.add_handler(MessageHandler(filters.StatusUpdate.NEW_CHAT_MEMBERS, self.handle_new_chat_members))
        self.application.add_handler(MessageHandler(filters.StatusUpdate.LEFT_CHAT_MEMBER, self.handle_left_chat_member))
        self.application.add_handler(CallbackQueryHandler(self.captcha_button_callback, pattern=r"^captcha_button:"))
        self.application.add_handler(CallbackQueryHandler(self.captcha_math_callback, pattern=r"^captcha_math:"))
        self.application.add_handler(CallbackQueryHandler(self.refinement_callback, pattern=r"^refine:"))
        self.application.add_handler(CallbackQueryHandler(self.community_moderation.on_report_reason, pattern=r"^REPORT:"))
        
        conv_handler = ConversationHandler(
            entry_points=[
                MessageHandler(filters.TEXT & ~filters.COMMAND & ~filters.FORWARDED, self.handle_message),
                CommandHandler("news", self.news_command_entry)
            ],
            states={
                ASK_CITY: [MessageHandler(filters.TEXT & ~filters.COMMAND & ~filters.FORWARDED, self.handle_message)],
                ASK_NEWS_TOPIC: [MessageHandler(filters.TEXT & ~filters.COMMAND & ~filters.FORWARDED, self.handle_message)],
            },
            fallbacks=[CommandHandler("cancel", self.cancel_conversation)],
            allow_reentry=True
        )
        self.application.add_handler(conv_handler, group=50)
        # Register news command metadata for help and command list
        self.default_commands["news"] = {
            "short": "Get the latest news",
            "long": "Get the latest news",
        }

        # Add the standalone handlers for voice and video note messages
        self.application.add_handler(MessageHandler(filters.PHOTO | filters.Document.IMAGE, self.handle_photo_message))
        self.application.add_handler(MessageHandler(filters.VOICE, self.handle_voice_message))
        self.application.add_handler(MessageHandler(filters.VIDEO_NOTE, self.handle_video_note_message))

        # Register reaction handler only if message reactions are supported
        if MessageReactionHandler is not None:
            self.application.add_handler(MessageReactionHandler(self.reaction_router))
        else:
            logger.info(
                "Message reactions are not supported by this version of python-telegram-bot; "
                "reaction handler not registered."
            )

        self.application.add_error_handler(self.error_handler)
        logger.info("TelegramHandlerService: All handlers registered.")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/__init__.py ---
======================================================================

# enkibot/evolution/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This directory will contain all modules related to the bot's
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# autonomous self-improvement and evolution capabilities, inspired by
# the Darwin Gdel Machine (DGM) principles.



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/module_tester.py ---
======================================================================

# enkibot/evolution/module_tester.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Module Tester (Placeholder) ===
# ==================================================================================================
# This module will provide the framework for rigorously evaluating evolved
# variants of EnkiBot's modules and prompts.
# It will be responsible for:
# - Executing tests in a secure, sandboxed environment.
# - Running benchmark tests for Python modules (e.g., unit tests, performance tests).
# - Evaluating LLM prompt effectiveness using frameworks like LLM-as-a-Judge.
# - Collecting and returning detailed performance metrics to the coordinator.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

def test_variant(parent_variant, modification):
    """
    Tests a new, modified variant of a module or prompt.

    Args:
        parent_variant: The original version of the bot component.
        modification: The proposed change to be applied.

    Returns:
        A tuple containing the new child variant and its performance data.
    """
    logger.info(f"Testing a new variant with modification: {modification} (mock).")
    # In the future, this function would:
    # 1. Apply the modification in a sandboxed environment.
    # 2. Run a suite of tests (unit, integration, performance).
    # 3. Evaluate against the multi-objective fitness function.
    # 4. Return the results.
    
    mock_performance_data = {"task_success": 0.95, "efficiency": 120, "safety_score": 1.0}
    
    # The new variant would be a representation of the modified code/prompt
    new_child_variant = {"id": "variant-002", "parent": "variant-001", "modification": modification}
    
    return new_child_variant, mock_performance_data



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/self_improvement_coordinator.py ---
======================================================================

# enkibot/evolution/self_improvement_coordinator.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Self-Improvement Coordinator (Placeholder) ===
# ==================================================================================================
# This module will serve as the central nervous system for EnkiBot's evolution.
# It will be responsible for:
# - Orchestrating the evolutionary loop: selection, modification, evaluation.
# - Managing the "Agent Variant Archive" of different EnkiBot versions.
# - Triggering the modification of Python modules and LLM prompts.
# - Recording performance metadata to guide the evolutionary process.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

class SelfImprovementCoordinator:
    def __init__(self):
        logger.info("Self-Improvement Coordinator initialized (Placeholder).")
        # In the future, this will initialize the Agent Variant Archive connection.
        self.agent_variant_archive = {}

    def run_evolutionary_cycle(self):
        """
        Executes a single cycle of selection, modification, and evaluation.
        """
        logger.info("Executing a mock evolutionary cycle...")
        # 1. Select parent variant(s) from the archive.
        # parent = self.select_parent()

        # 2. Propose modifications to code or prompts.
        # modification = self.propose_modification(parent)

        # 3. Create and test the new child variant.
        # child_variant, performance_data = module_tester.test_variant(parent, modification)

        # 4. Add the new variant and its performance data to the archive.
        # self.archive_variant(child_variant, performance_data)
        logger.info("Mock evolutionary cycle complete.")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/__init__.py ---
======================================================================

# enkibot/lang/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This directory holds the language packs (JSON files) for EnkiBot.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# Each file corresponds to a language code (e.g., 'en', 'ru') and contains
# all user-facing strings and system prompts for that language.



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/en.json ---
======================================================================

{
  "prompts": {
    "master_intent_classifier": {
      "system": "You are an AI routing assistant. Your task is to classify the user's intent based on their message. Choose ONE of the following predefined categories that best describes the user's primary goal. Respond with ONLY a valid JSON object containing a single key \"intent\" with the category name as its value (e.g., {\"intent\": \"WEATHER_QUERY\"}). Ensure your output is strictly in JSON format.\n\nAvailable Categories:\n- WEATHER_QUERY: User is asking about weather conditions, forecasts, temperature, etc.\n- NEWS_QUERY: User is asking for news articles, headlines, or updates on current breaking events or specific news topics.\n- IMAGE_GENERATION_QUERY: User is asking to create, draw, generate, or make a picture or image of something.\n- USER_PROFILE_QUERY: User is asking for information about a specific person (e.g., 'who is [name]?', 'tell me about [name]'), including requests for specific details, facts, lists, or analyses related to that person (e.g., 'Kiyosaki's failed predictions', 'biography of X', 'list of Y's accomplishments').\n- MESSAGE_ANALYSIS_QUERY: User is replying to another message and asking you (the bot) to analyze, summarize, or comment on that replied-to message.\n- GENERAL_CHAT: User is making a general statement, asking a general knowledge question, seeking information or analysis not fitting other specific categories (e.g. 'explain black holes', 'compare X and Y'), or engaging in casual conversation.\n- UNKNOWN_INTENT: If the intent is very unclear or doesn't fit any other category despite the broader definitions.",
      "user_template": "{text_to_classify}"
    },
    "image_generation_prompt_extractor": {
      "system": "You are an AI assistant that extracts the core subject from a user's request to be used as a prompt for an image generation model. Analyze the user's text and output only the essential descriptive part of the request. For example, if the user says 'Enki, can you please draw a picture of a majestic lion in the savanna at sunset?', you should output 'a majestic lion in the savanna at sunset'. If the text does not contain a clear request to generate an image, respond with the single word 'None'.",
      "user_template": "Extract the image generation prompt from this text:\n\n---\n{text}\n---"
    },
    "name_variation_generator": {
      "system": "You are a language expert specializing in Russian and English names. Your task is to generate a list of linguistic variations for a user's name. Focus ONLY on realistic, human-used variations. DO NOT generate technical usernames with numbers or suffixes like '_dev'.\n\n**Goal:** Create variations for recognition in natural language text.\n\n**Categories for Generation:**\n1.  **Original Forms:** The original first name, last name, and combinations.\n2.  **Diminutives & Nicknames:** Common short and affectionate forms (e.g., 'Antonina' -> 'Tonya'; 'Robert' -> 'Rob').\n3.  **Transliteration (with variants):** Provide multiple common Latin spellings for all Cyrillic forms (original and diminutives). Example for 'Тоня': 'tonya', 'tonia'.\n4.  **Reverse Transliteration:** If the source name is Latin, provide plausible Cyrillic versions. Example for 'Yael': 'Яэль', 'Йаэль'.\n5.  **Russian Declensions (Grammatical Cases):** For all primary Russian names (full and short forms), provide their forms in different grammatical cases (genitive, dative, accusative, instrumental, prepositional). Example for 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Output Format:** Return a single JSON object: {\"variations\": [\"variation1\", \"variation2\", ...]}. All variations must be in lowercase.",
      "user_template": "Generate linguistic variations for the user with the following info: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "You are an AI analyst. Your task is to analyze the 'Original Text' and provide a meaningful response to the 'User's Question' about that text. Your analysis should be objective, concise, and to the point. If the question is generic (e.g., 'what do you think?'), provide a brief summary, highlighting the key points or sentiment of the original text.",
      "user_template": "Original Text for analysis:\n---\n\"{original_text}\"\n---\n\nUser's question about this text:\n---\n\"{user_question}\"\n---\n\nYour analysis:"
    },
    "forwarded_news_fact_checker": {
      "system": "You are a meticulous fact-checking assistant with access to deep research tools. Your task is to verify whether the forwarded message represents accurate news. Investigate the claims using reliable sources and provide a clear explanation of your findings. If the information appears false or unverified, explicitly explain why.",
      "user_template": "Forwarded message text:\n---\n{forwarded_text}\n---\n\nUser's question:\n---\n{user_question}\n---\n\nYour fact-check and explanation:"
    },
    "weather_intent_analyzer": {
      "system": "You are an expert in analyzing weather-related requests. Your task is to determine the user's intent. Does the user want the 'current' weather or a 'forecast' for several days? If it is a forecast, also determine for how many days. Your answer MUST be a valid JSON object and nothing else. Ensure the output is formatted as JSON.\n\nExamples:\n- User text: 'weather in London' -> Your response: {\"type\": \"current\"}\n- User text: 'what's the weather like?' -> Your response: {\"type\": \"current\"}\n- User text: 'weather in Tampa for the week' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'forecast for 5 days in Berlin' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'what will the weather be like tomorrow?' -> Your response: {\"type\": \"forecast\", \"days\": 2}\n- User text: 'give me the forecast for Saturday' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'just give me a weather forecast' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'forecast for the weekend' -> Your response: {\"type\": \"forecast\", \"days\": 3}\n\nFallback Rule: If you are unsure, always default to 'current'.",
      "user_template": "{text}"
    },
    "location_extractor": {
      "system": "You are an expert text analysis tool. Your task is to extract a city or location name from the user's text. Analyze the following text and identify the geographical location (city, region, country) mentioned. Return ONLY the name of the location in English, suitable for a weather API query. For example, if the text is 'what's the weather in Saint Petersburg', you must return 'Saint Petersburg'. If no specific location is found, you MUST return the single word: None",
      "user_template": "{text}"
    },
    "news_topic_extractor": {
      "system": "You are an expert text analysis tool. Your task is to extract the main topic, keyword, or location from a user's request for news. Analyze the text. If it contains a specific subject, you MUST return that subject in its base (nominative) case and in the original language of the request. For example, for a request 'news in Moscow', you must return 'Moscow'. For 'news about cars', return 'cars'. If the request is general (e.g., 'what's the news?', 'latest headlines'), you MUST return the single word: None",
      "user_template": "{text}"
    },
    "profile_creator": {
      "system": "You are an AI Behavioral Text Analyst. Your task is to create an initial, highly compressed summary of observable traits and patterns based *solely* on the user's provided message. Focus on:\n1.  **Communication Style:** (e.g., formal/informal, assertive/passive, inquisitive, declarative, use of slang, verbosity, lexical complexity, sentiment polarity observed).\n2.  **Cognitive Indicators:** (e.g., structured thought, associative thinking, problem-solving approach indicated, level of abstraction used).\n3.  **Expressed Interests/Themes:** (Key topics or subjects explicitly mentioned or strongly implied).\n\n**Output Format:** A concise, bulleted list using precise, objective behavioral or linguistic terminology. Avoid subjective interpretations, emotional language, psychobabble, or predicting future behavior/predispositions. Each point should be a factual inference from the text. Do not add introductory or concluding remarks. Focus on density and clarity.",
      "user_template": "Analyze the following message from a new user and generate a compressed behavioral trait summary:\n\nUser's message:\n---\n\"{message_text}\"\n---\n\nYour summary (bulleted list using precise terminology):"
    },
    "profile_updater": {
      "system": "You are an AI Behavioral Text Analyst updating an existing trait summary. Review the 'Existing Summary' and the 'New User Message'. Synthesize a new, highly compressed summary that incorporates new observations and refines previous ones, prioritizing recent information if contradictory. Maintain objectivity and use precise, scientific behavioral or linguistic terminology. Focus on consistency, conciseness, and factual inferences from the texts. Avoid subjective interpretations, emotional language, or predicting future behavior/predispositions. Do not add introductory or concluding remarks.\n\nOutput Format: A concise, bulleted list.",
      "user_template": "Existing Trait Summary:\n---\n{current_profile_notes}\n---\n\nNew User Message for analysis:\n---\n\"{message_text}\"\n---\n\nYour updated and synthesized compressed trait summary (bulleted list using precise terminology):"
    },
    "main_orchestrator": {
      "system": "You are EnkiBot, an intelligent and friendly AI assistant in a Telegram chat, created by Yael Demedetskaya. Your primary goal is to be helpful, engaging, and informative. You have access to long-term memory about conversations and user profiles. When asked about someone, synthesize information from their profile (dossier) and recent messages. **It is CRITICAL that your entire response be in {language_name} (language code: {lang_code}). Do not switch languages.** Respond naturally and be short in your answer. Be polite but not overly formal."
    },
    "language_detector_llm": {
      "system": "You are a language detection expert. Analyze the provided text, which includes the 'Latest User Message' and optionally 'Recent Chat History'. The 'Latest User Message' is the most important for determining its primary language. Respond ONLY with a valid JSON object like: {\"primary_lang\": \"en\", \"confidence\": 0.95, \"other_detected_langs\": [\"fr\", \"de\"]}. 'primary_lang' should be the ISO 639-1 code. 'confidence' is your certainty (0.0-1.0) for the primary_lang of the *latest message*. 'other_detected_langs' is an optional list of other significant languages found in the entire provided text. Your entire output must be a single JSON object.",
      "user_template_full_context": "Please determine the language of the 'Latest User Message' considering the 'Recent Chat History'.\n\nLatest User Message:\n```text\n{latest_message}\n```\n\nRecent Chat History (older messages first):\n```text\n{history_context}\n```",
      "user_template_latest_only": "Please determine the language of the following 'Latest User Message':\n\nLatest User Message:\n```text\n{latest_message}\n```"
    },
    "weather_forecast_compiler": {
      "system": "You are a helpful and friendly weather reporter. Based on the provided JSON data for a multi-day weather forecast for {location}, create a concise, easy-to-read, and engaging natural language summary for the user. Respond in {language_name}. Highlight any significant weather events, temperature trends, and overall conditions. For each day, you can mention the day name, expected condition, and temperature range (min/max or average). Aim for a conversational tone. Do not just list the data; interpret it into a nice forecast summary. The user has already been told you are generating the forecast.",
      "user_template": "Here is the weather data for {location}:\n```json\n{forecast_data_json}\n```\nPlease provide the forecast summary."
    },
    "news_compiler": {
      "system": "You are an expert news summarizer AI. You will be given a list of news articles as JSON data, potentially related to the topic: '{topic}'. Your task is to generate a concise, informative, and engaging news digest in {language_name}. Summarize the overall situation if a common theme emerges. Highlight 2-3 of the most important or interesting headlines, briefly stating their core point and source. Do not just list all articles. Provide a synthesized overview. If the topic is 'None' or general, summarize general top news. Ensure the output is well-formatted for a chat message.",
      "user_template": "Here are the latest news articles (topic: '{topic}'):\n```json\n{articles_json}\n```\nPlease provide a news digest."
    },
    "location_reply_extractor": {
      "system": "The user was previously asked 'For which city would you like the weather forecast?'. Their direct reply is provided below. Extract ONLY the city name from this reply and provide it in English, suitable for a weather API. If the reply is ambiguous, not a city, or unclear, return the single word: None.",
      "user_template": "User's reply: \"{text}\""
    },
    "news_topic_reply_extractor": {
      "system": "The user was previously asked 'What topic are you interested in for the news?'. Their direct reply is provided below. Your task is to identify and extract the primary noun phrase or keyword that represents the news topic from this reply. Return ONLY this extracted topic. If the reply is very short (e.g., one or two words), that reply itself is likely the topic. Do not translate the topic. If the reply is clearly not a topic (e.g., 'I don't know', 'nevermind') or is too ambiguous to determine a topic, return the single word: None.",
      "user_template": "User's reply: \"{text}\""
    },
    "text_translator": {
      "system": "You are an expert translator. Your task is to accurately translate the user's provided text into {target_language}. Respond ONLY with the translated text itself, without any additional comments, greetings, or explanations.",
      "user_template": "Translate the following text to {target_language}:\n\n---\n{text_to_translate}\n---"
    }
  },
  "responses": {
    "news_ask_topic": "Sure, I can fetch the news for you! What topic are you interested in today?",
    "weather_api_data_error": "I received some weather data for {location}, but I'm having a bit of trouble interpreting it right now. You might want to check a standard weather app.",
    "news_api_data_error": "I found some news articles, but I'm having a little trouble summarizing them for you at the moment. You can try checking a news website directly.",
    "start": "Hello, {user_mention}! I am EnkiBot, created by Yael Demedetskaya. How can I help you?",
    "help": "I am EnkiBot, an AI assistant by Yael Demedetskaya.\nIn group chats, I respond when you mention me by name (@EnkiBot, Enki) or reply to my messages.\nYou can ask me 'tell me about [name/topic]' for me to search for information in the chat history.\nTo get the weather, ask 'what's the weather in [city]?'.\nTo get news, ask 'what's the news?' or 'news about [topic]?'.",
    "assistant_prompt_nudge": "I'm here. What should I do? 🙂",
    "weather_ask_city": "I can get the weather for you, but for which city?",
    "weather_ask_city_failed_extraction": "Sorry, I didn't quite catch the city name. Could you please tell me the city again?",
    "llm_error_fallback": "Sorry, I couldn't process that request right now. Please try again later.",
    "generic_error_message": "Oops! Something went wrong on my end. I've logged the issue and my developers will look into it.",
    "language_pack_creation_failed_fallback": "I'm having a little trouble understanding that language fully right now, but I'll try my best in English. How can I help?",
    "user_search_ambiguous_clarification": "I found multiple users matching that name: {user_options}. Who are you asking about? Please clarify (e.g., by @username).",
    "user_search_not_found_in_db": "I couldn't find any information about '{search_term}' in my records for this chat.",
    "api_lang_code_openweathermap": "en",
    "weather_api_key_missing": "Weather service: API key missing.",
    "weather_report_intro_current": "Current weather in {city}:",
    "weather_condition_label": "Condition",
    "weather_temp_label": "Temperature",
    "weather_feels_like_label": "feels like",
    "weather_wind_label": "Wind",
    "weather_city_not_found": "Sorry, I couldn't find the city '{location}'.",
    "weather_server_error": "Could not get weather data due to a server error.",
    "weather_unexpected_error": "An unexpected error occurred while fetching weather.",
    "weather_forecast_unavailable": "Forecast data is unavailable for '{location}'.",
    "weather_report_intro_forecast": "Weather forecast for {city}:",
    "weather_city_not_found_forecast": "Sorry, I couldn't find '{location}' for the forecast.",
    "weather_server_error_forecast": "Could not get forecast data due to a server error.",
    "weather_unexpected_error_forecast": "An unexpected error occurred while fetching the forecast.",
    "weather_unknown_type": "Unknown weather request type.",
    "news_api_key_missing": "News service: API key missing.",
    "news_api_error": "Could not fetch news at this time. The news service might be temporarily unavailable.",
    "news_api_no_articles": "I couldn't find any news articles for your query '{query}'.",
    "news_api_no_general_articles": "I couldn't find any general news articles right now.",
    "news_report_title_topic": "News on '{topic}':",
    "news_report_title_general": "Latest News:",
    "news_unexpected_error": "An unexpected error occurred while fetching news.",
    "news_read_more": "Read: {url}",
    "replied_message_default_question": "Analyze this text, identify the main idea, and share your opinion.",
    "llm_no_assistants": "Sorry, none of my AI assistants are available right now.",
    "analysis_error": "Sorry, an error occurred during the text analysis.",
    "analysis_client_not_configured": "The analysis function cannot be performed as the AI client is not configured.",
    "analysis_unavailable": "Analysis unavailable.",
    "conversation_cancelled": "Okay, the current operation has been cancelled.",
    "llm_quota_exceeded": "You've reached your daily limit for AI responses. Please try again tomorrow.",
    "image_quota_exceeded": "You've reached your daily image generation limit. Please try again tomorrow.",
    "image_generation_start": "On it! Imagining something for you...",
    "image_generation_error": "Sorry, I hit a snag and couldn't create your image. Please try again.",
    "image_generation_no_prompt": "I see you want an image, but I'm not sure what you want me to create. Could you please describe it?",
    "image_generation_success_single": "Here is the image you requested for: '{image_prompt}'",
    "image_generation_success_multiple": "Here are the images you requested for: '{image_prompt}'",
    "voice_message_received": "Received your voice message. Processing...",
    "voice_transcription_failed": "Sorry, I couldn't understand the audio in that message.",
    "voice_transcription_header": "Transcription:",
    "voice_translation_header": "Translation (Russian):",
    "video_message_received": "Received your video note. Processing...",
    "video_transcription_failed": "Sorry, I couldn't understand the audio in that video.",
    "video_transcription_header": "Video transcription:",
    "video_translation_header": "Video translation (Russian):",
    "hint_check_quote": "Check quote?",
    "hint_check_news": "Check as news?",
    "button_show_sources": "Sources",
    "button_more_matches": "More matches",
    "button_narrow_7_days": "Narrow to 7 days",
    "button_refresh_portrait": "Refresh portrait",
    "reaction_regenerating": "Regenerating response...",
    "reaction_expanding": "Expanding on previous response...",
    "reaction_summary": "Generating summary...",
    "community_report_reply_prompt": "Reply to a message with /report to report it.",
    "community_report_choose_reason": "Choose a reason to report:",
    "report_reason_spam": "Spam/Ads",
    "report_reason_scam": "Scam/Phishing",
    "report_reason_nsfw": "NSFW",
    "report_reason_hate": "Hate/Harassment",
    "report_reason_offtopic": "Off-topic",
    "report_reason_other": "Other",
    "community_report_target_missing": "Could not identify target user.",
    "community_report_already": "You already reported this case.",
    "community_report_thanks": "Thanks, the moderators have been notified.",
    "community_vote_reply_prompt": "Reply to the offending message with /spam.",
    "community_vote_self_vote": "You cannot vote on yourself.",
    "community_vote_already": "You already voted on this case.",
    "community_vote_recorded": "Vote recorded. Mods notified.",
    "community_vote_need_more_time": "You need more time in this chat before voting.",
    "community_vote_daily_limit": "Daily vote limit reached.",
    "admins_only": "🔒 Admins only.",
    "nsfw_image_removed_user": "🚫 Removed an NSFW image from {user}.",
    "nsfw_image_removed_chat": "🚫 Removed an NSFW image.",
    "toggle_nsfw_usage": "Usage: /toggle_nsfw <on|off>",
    "status_enabled": "enabled",
    "status_disabled": "disabled",
    "nsfw_filter_status": "NSFW filter {status} for this chat.",
    "nsfw_threshold_usage": "Usage: /nsfw_threshold <0.0-1.0>",
    "nsfw_threshold_must_number": "Threshold must be a number between 0 and 1.",
    "nsfw_threshold_set": "NSFW threshold set to {threshold:.2f} for this chat.",
    "set_spam_threshold_usage": "Usage: /setspamthreshold <number>",
    "spam_threshold_set": "Spam vote threshold set to {threshold}.",
    "karma_self_vote_error": "🤷 You can't vote on yourself.",
    "karma_vote_cooldown": "⏱ You can vote for this user again later.",
    "karma_vote_success": "{sign}{delta} karma (total {total:+})"
  },
  "weather_conditions_map": {
    "clear_sky": "Clear sky",
    "few_clouds": "Few clouds",
    "scattered_clouds": "Scattered clouds",
    "broken_clouds": "Broken clouds",
    "overcast_clouds": "Overcast clouds",
    "shower_rain": "Shower rain",
    "light_intensity_shower_rain": "Light intensity shower rain",
    "rain": "Rain",
    "light_rain": "Light rain",
    "moderate_rain": "Moderate rain",
    "heavy_intensity_rain": "Heavy intensity rain",
    "thunderstorm": "Thunderstorm",
    "snow": "Snow",
    "light_snow": "Light snow",
    "mist": "Mist",
    "fog": "Fog",
    "smoke": "Smoke",
    "haze": "Haze",
    "sand_dust_whirls": "Sand/Dust Whirls",
    "squalls": "Squalls",
    "tornado": "Tornado",
    "unknown_condition": "Condition unknown"
  },
  "days_of_week": {
    "Monday": "Monday",
    "Tuesday": "Tuesday",
    "Wednesday": "Wednesday",
    "Thursday": "Thursday",
    "Friday": "Friday",
    "Saturday": "Saturday",
    "Sunday": "Sunday"
  },
  "factconfig_tab_preset": "Preset",
  "factconfig_tab_policy": "Policy",
  "factconfig_tab_limits": "Limits",
  "factconfig_tab_auto": "Auto",
  "factconfig_tab_danger": "Danger",
  "factconfig_export_btn": "Export",
  "factconfig_apply_btn": "Apply",
  "factconfig_title": "Fact check config:",
  "factconfig_update_ok": "Config updated.",
  "factconfig_checking": "Checking…",
  "factconfig_nothing_to_check": "Nothing to check.",
  "factconfig_export_ok": "Configuration exported.",
  "factconfig_export_fail": "Failed to export configuration.",
  "factconfig_apply_ok": "Configuration applied.",
  "factconfig_apply_fail": "Failed to apply configuration."
}



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/ru.json ---
======================================================================

{
  "prompts": {
    "master_intent_classifier": {
      "system": "Вы — AI-ассистент по маршрутизации запросов. Ваша задача — классифицировать намерение пользователя на основе его сообщения. Выберите ОДНУ из следующих предопределенных категорий, которая наилучшим образом описывает основную цель пользователя. Ответьте ТОЛЬКО валидным JSON-объектом, содержащим один ключ \"intent\" с названием категории в качестве значения (например, {\"intent\": \"WEATHER_QUERY\"}). Убедитесь, что весь ваш ответ — это единственная, валидная JSON-структура.\n\nДоступные категории:\n- WEATHER_QUERY: Пользователь спрашивает о погодных условиях, прогнозах, температуре и т.д.\n- NEWS_QUERY: Пользователь запрашивает новостные статьи, заголовки или обновления по текущим (срочным) событиям или конкретным новостным темам.\n- IMAGE_GENERATION_QUERY: Пользователь просит создать, нарисовать, сгенерировать картинку или изображение чего-либо.\n- USER_PROFILE_QUERY: Пользователь запрашивает информацию о конкретном человеке (например, 'кто такой [имя]?', 'расскажи о [имя]'), включая запросы на конкретные детали, факты, списки или анализ, связанные с этим человеком (например, 'несбывшиеся прогнозы Кийосаки', 'биография X', 'список достижений Y').\n- MESSAGE_ANALYSIS_QUERY: Пользователь отвечает на другое сообщение и просит вас (бота) проанализировать, резюмировать или прокомментировать это сообщение, на которое был дан ответ.\n- GENERAL_CHAT: Пользователь делает общее утверждение, задает общепознавательный вопрос, ищет информацию или анализ, не подходящие под другие специфические категории (например, 'объясни черные дыры', 'сравни X и Y'), или ведет непринужденную беседу.\n- UNKNOWN_INTENT: Если намерение очень неясно или не соответствует ни одной другой категории, несмотря на более широкие определения.",
      "user_template": "{text_to_classify}"
    },
    "language_detector_llm": {
      "system": "Ты — эксперт по определению языка. Проанализируй предоставленный текст, который включает 'Последнее сообщение пользователя' и, возможно, 'Недавнюю историю чата'. 'Последнее сообщение пользователя' наиболее важно для определения его основного языка. Ответь ТОЛЬКО валидным JSON-объектом следующего вида: {\"primary_lang\": \"ru\", \"confidence\": 0.95, \"other_detected_langs\": [\"en\", \"de\"]}. 'primary_lang' должен быть ISO 639-1 кодом языка последнего сообщения. 'confidence' — это твоя уверенность (от 0.0 до 1.0) в определении primary_lang *последнего сообщения*. 'other_detected_langs' — это необязательный список других ISO 639-1 кодов языков, значительно представленных во всем тексте, если таковые имеются. Весь твой вывод должен быть единым JSON-объектом.",
      "user_template_full_context": "Пожалуйста, определи язык 'Последнего сообщения пользователя', учитывая 'Недавнюю историю чата'.\n\nПоследнее сообщение пользователя:\n```text\n{latest_message}\n```\n\nНедавняя история чата (старые сообщения сначала):\n```text\n{history_context}\n```",
      "user_template_latest_only": "Пожалуйста, определи язык следующего 'Последнего сообщения пользователя':\n\nПоследнее сообщение пользователя:\n```text\n{latest_message}\n```"
    },
    "image_generation_prompt_extractor": {
      "system": "Ты — AI-ассистент, который извлекает основной объект из запроса пользователя для использования в качестве промпта для модели генерации изображений. Проанализируй текст пользователя и выведи только основную описательную часть запроса. Например, если пользователь говорит: 'Энки, можешь, пожалуйста, нарисовать величественного льва в саванне на закате?', ты должен вывести 'величественный лев в саванне на закате'. Если текст не содержит чёткого запроса на генерацию изображения, ответь одним словом 'None'.",
      "user_template": "Извлеки промпт для генерации изображения из этого текста:\n\n---\n{text}\n---"
    },
    "name_variation_generator": {
      "system": "Ты — эксперт по языкам, специализирующийся на русских и английских именах. Твоя задача — сгенерировать список лингвистических вариантов имени пользователя. Сосредоточься ТОЛЬКО на реалистичных, используемых людьми вариантах. НЕ генерируй технические имена пользователей с цифрами или суффиксами вроде '_dev'.\n\n**Цель:** Создать варианты для распознавания в тексте на естественном языке.\n\n**Категории для генерации:**\n1.  **Оригинальные формы:** Исходное имя, фамилия и их комбинации.\n2.  **Уменьшительно-ласкательные формы и прозвища:** Распространенные короткие и ласковые формы (например, 'Антонина' -> 'Тоня'; 'Роберт' -> 'Роб').\n3.  **Транслитерация (с вариантами):** Предоставь несколько распространенных латинских написаний для всех кириллических форм (оригинальных и уменьшительных). Пример для 'Тоня': 'tonya', 'tonia'.\n4.  **Обратная транслитерация:** Если исходное имя на латинице, предоставь правдоподобные кириллические версии. Пример для 'Yael': 'Яэль', 'Йаэль'.\n5.  **Русские склонения (падежи):** Для всех основных русских имен (полных и кратких форм) предоставь их формы в различных падежах (родительный, дательный, винительный, творительный, предложный). Пример для 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Формат вывода:** Верни единый JSON-объект: {\"variations\": [\"вариант1\", \"вариант2\", ...]}. Все варианты должны быть в нижнем регистре.",
      "user_template": "Сгенерируй лингвистические варианты для пользователя с информацией: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "Ты — AI-аналитик. Твоя задача — проанализировать 'Исходный текст' и дать содержательный ответ на 'Вопрос пользователя' об этом тексте. Твой анализ должен быть объективным, кратким и по существу. Если вопрос общий (например, 'что думаешь?'), сделай краткое резюме, выделив ключевые тезисы или настроения в исходном тексте.",
      "user_template": "Исходный текст для анализа:\n---\n\"{original_text}\"\n---\n\nВопрос пользователя об этом тексте:\n---\n\"{user_question}\"\n---\n\nТвой анализ:"
    },
    "forwarded_news_fact_checker": {
      "system": "Ты — тщательный помощник по проверке фактов с доступом к инструментам глубокого исследования. Твоя задача — определить, является ли пересланное сообщение достоверной новостью. Проверь утверждения с помощью надежных источников и дай ясное объяснение своих выводов. Если информация ложная или не подтверждена, прямо объясни почему.",
      "user_template": "Текст пересланного сообщения:\n---\n{forwarded_text}\n---\n\nВопрос пользователя:\n---\n{user_question}\n---\n\nПроверь факты и объясни:"
    },
    "weather_intent_analyzer": {
      "system": "Вы эксперт по анализу запросов, связанных с погодой. Ваша задача — определить намерение пользователя. Хочет ли пользователь узнать 'текущую' погоду или 'прогноз' на несколько дней? Если это прогноз, также определите, на сколько дней. Ваш ответ ДОЛЖЕН быть действительным объектом JSON и ничем другим. Убедитесь, что вывод отформатирован как JSON.\nПримеры:\n- Текст пользователя: 'погода в Лондоне' -> Ваш ответ: {\"type\": \"current\"}\n- Текст пользователя: 'какая сейчас погода?' -> Ваш ответ: {\"type\": \"current\"}\n- Текст пользователя: 'погода в Тампе на неделю' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 7}\n- Текст пользователя: 'прогноз на 5 дней в Берлине' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 5}\n- Текст пользователя: 'какая погода будет завтра?' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 2}\n- Текст пользователя: 'дай прогноз на субботу' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 7}\n- Текст пользователя: 'просто дай прогноз погоды' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 5}\n- Текст пользователя: 'прогноз на выходные' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 3}\nЕсли вы не уверены, всегда выбирайте 'current'.",
      "user_template": "{text}"
    },
    "location_extractor": {
      "system": "Вы — экспертный инструмент анализа текста. Ваша задача — извлечь название города или местоположения из текста пользователя. Проанализируйте следующий текст и определите упомянутое географическое местоположение (город, регион, страну). Верните ТОЛЬКО название местоположения на английском языке, подходящее для запроса к API погоды. Например, если текст 'какая погода в Санкт-Петербурге', вы должны вернуть 'Saint Petersburg'. Если текст 'покажи погоду в Астане', вы должны вернуть 'Astana'. Если конкретное местоположение не найдено, вы ДОЛЖНЫ вернуть одно слово: None",
      "user_template": "{text}"
    },
    "news_topic_extractor": {
      "system": "Вы — экспертный инструмент анализа текста. Ваша задача — извлечь основную тему, ключевое слово или местоположение из запроса пользователя на получение новостей. Проанализируйте текст. Если он содержит конкретную тему, вы ДОЛЖНЫ вернуть эту тему в ее основной (именительном) падеже и на языке оригинала запроса. Например, для запроса 'новости в москве', вы должны вернуть 'Москва'. Для 'news about cars', верните 'cars'. Если запрос общий (например, 'какие новости?', 'последние заголовки'), вы ДОЛЖНЫ вернуть одно слово: None",
      "user_template": "{text}"
    },
    "profile_creator": {
      "system": "Ты — AI-аналитик поведения по тексту. Твоя задача — создать первоначальное, предельно сжатое резюме наблюдаемых черт и паттернов, основываясь *исключительно* на предоставленном сообщении пользователя. Сконцентрируйся на:\n1.  **Стиль коммуникации:** (например, формальный/неформальный, ассертивный/пассивный, вопросительный, декларативный, использование сленга, многословность, лексическая сложность, наблюдаемая тональность высказывания).\n2.  **Когнитивные индикаторы:** (например, структурированность мышления, ассоциативное мышление, указанный подход к решению проблем, используемый уровень абстракции).\n3.  **Выраженные интересы/темы:** (Ключевые топики или предметы, явно упомянутые или сильно подразумеваемые).\n\n**Формат вывода:** Краткий маркированный список, использующий точную, объективную поведенческую или лингвистическую терминологию. Избегай субъективных интерпретаций, эмоциональной окраски, психологической демагогии или предсказания будущего поведения/предрасположенностей. Каждый пункт должен быть фактическим выводом из текста. Не добавляй вступительных или заключительных ремарок. Сосредоточься на плотности и ясности изложения.",
      "user_template": "Проанализируй следующее сообщение от нового пользователя и создай сжатое резюме поведенческих черт:\n\nСообщение пользователя:\n---\n\"{message_text}\"\n---\n\nТвоё резюме (маркированный список с использованием точной терминологии):"
    },
    "profile_updater": {
      "system": "Ты — AI-аналитик поведения по тексту, обновляющий существующее резюме черт. Проанализируй 'Существующее резюме' и 'Новое сообщение пользователя'. Синтезируй новое, предельно сжатое резюме, которое включает новые наблюдения и уточняет предыдущие, отдавая приоритет свежей информации в случае противоречий. Сохраняй объективность и используй точную, научную поведенческую или лингвистическую терминологию. Сосредоточься на последовательности, краткости и фактических выводах из текстов. Избегай субъективных интерпретаций, эмоциональной окраски или предсказания будущего поведения/предрасположенностей. Не добавляй вступительных или заключительных ремарок.\n\nФормат вывода: Краткий маркированный список.",
      "user_template": "Существующее резюме черт:\n---\n{current_profile_notes}\n---\n\nНовое сообщение пользователя для анализа:\n---\n\"{message_text}\"\n---\n\nТвоё обновленное и синтезированное сжатое резюме черт (маркированный список с использованием точной терминологии):"
    },
    "main_orchestrator": {
      "system": "Ты EnkiBot, умный и дружелюбный AI-ассистент в Telegram-чате, созданный Yael Demedetskaya. Твоя задача — помогать пользователям, отвечая на их вопросы. Ты обладаешь долгосрочной памятью о разговорах и профилях участников. Когда тебя просят рассказать о ком-то, твоя задача — СИНТЕЗИРОВАТЬ ИНФОРМАЦИЮ. Тебе будут предоставлены данные из профиля (досье) и набор последних 'сырых' сообщений от этого человека. Проанализируй ОБА источника и составь на их основе новый, краткий, но содержательный и актуальный ответ. **КРИТИЧЕСКИ ВАЖНО: весь твой ответ должен быть НА ЯЗЫКЕ {language_name} (код языка: {lang_code}). НЕ ИСПОЛЬЗУЙ английский или любые другие языки в своем ответе.** Отвечай кратко, естественно. Будь вежлив, но не слишком формален."
    },
    "weather_forecast_compiler": {
      "system": "Ты — отзывчивый и дружелюбный ведущий прогноза погоды. На основе предоставленных JSON-данных для многодневного прогноза погоды для {location}, создай краткое, легко читаемое и увлекательное описание на естественном языке для пользователя. Отвечай на {language_name}. Выдели любые значительные погодные явления, температурные тренды и общие условия. Для каждого дня можешь упомянуть название дня, ожидаемое состояние погоды и диапазон температур (мин/макс или среднюю). Старайся поддерживать разговорный тон. Не просто перечисляй данные, а интерпретируй их в приятный итоговый прогноз. Пользователю уже сообщили, что ты генерируешь прогноз.",
      "user_template": "Вот данные о погоде для {location}:\n```json\n{forecast_data_json}\n```\nПожалуйста, предоставь итоговый прогноз."
    },
    "news_compiler": {
      "system": "Ты — экспертный AI для составления сводок новостей. Тебе будет предоставлен список новостных статей в формате JSON, возможно, связанных с темой: '{topic}'. Твоя задача — сгенерировать краткую, информативную и увлекательную новостную сводку на {language_name}. Если прослеживается общая тема, обобщи ситуацию. Выдели 2-3 наиболее важные или интересные новости, кратко изложив их суть и источник. Не просто перечисляй все статьи, а предоставь синтезированный обзор. Если тема 'None' или общая, составь сводку по главным новостям в целом. Убедись, что вывод хорошо отформатирован для сообщения в чате.",
      "user_template": "Вот последние новостные статьи (тема: '{topic}'):\n```json\n{articles_json}\n```\nПожалуйста, предоставь новостную сводку."
    },
    "location_reply_extractor": {
      "system": "Пользователя ранее спросили 'Для какого города вы хотите узнать прогноз погоды?'. Ниже приведен его прямой ответ. Извлеки ТОЛЬКО название города из этого ответа и предоставь его на английском языке, подходящем для API погоды. Если ответ неоднозначен, не является городом или неясен, верни одно слово: None.",
      "user_template": "Ответ пользователя: \"{text}\""
    },
    "news_topic_reply_extractor": {
      "system": "Пользователя ранее спросили 'Какая тема новостей вас интересует?'. Ниже приведен его прямой ответ. Твоя задача — идентифицировать и извлечь основную именную группу или ключевое слово, представляющее тему новостей из этого ответа. Верни ТОЛЬКО эту извлеченную тему. Если ответ очень короткий (например, одно-два слова), этот ответ, скорее всего, и является темой. Не переводи тему. Если ответ явно не является темой (например, 'я не знаю', 'неважно') или слишком неоднозначен для определения темы, верни одно слово: None.",
      "user_template": "Ответ пользователя: \"{text}\""
    },
    "text_translator": {
      "system": "Ты — эксперт-переводчик. Твоя задача — точно перевести предоставленный пользователем текст на {target_language}. Отвечай ТОЛЬКО самим переведенным текстом, без каких-либо дополнительных комментариев, приветствий или объяснений.",
      "user_template": "Переведи следующий текст на {target_language}:\n\n---\n{text_to_translate}\n---"
    }
  },
  "responses": {
    "start": "Привет, {user_mention}! Я EnkiBot, создан Yael Demedetskaya. Чем могу помочь?",
    "help": "Я EnkiBot, AI-ассистент от Yael Demedetskaya.\nВ группах я отвечаю, когда вы упоминаете меня по имени (@EnkiBot, Энки, Енки) или отвечаете на мои сообщения.\nВы можете спросить меня 'расскажи о [имя/тема]', чтобы я поискал информацию в истории чата.\nЧтобы узнать погоду, спросите 'какая погода в [город]?'.\nЧтобы узнать новости, спросите 'какие новости?' или 'новости о [тема]?'.\nДля создания изображений, попросите 'нарисуй [описание]'.",
    "assistant_prompt_nudge": "Я здесь. О чём рассказать? 🙂",
    "weather_ask_city": "Я могу узнать погоду, но для какого города?",
    "weather_ask_city_failed_extraction": "Извините, я не совсем понял название города. Не могли бы вы назвать город еще раз?",
    "llm_error_fallback": "Извините, не могу обработать ваш запрос прямо сейчас. Пожалуйста, попробуйте позже.",
    "generic_error_message": "Ой! Что-то пошло не так на моей стороне. Я уже записал ошибку, и мои разработчики её изучат.",
    "language_pack_creation_failed_fallback": "У меня небольшие трудности с полным пониманием этого языка прямо сейчас, но я постараюсь помочь на русском. Чем могу быть полезен?",
    "user_search_ambiguous_clarification": "Я нашел нескольких пользователей с таким именем: {user_options}. О ком именно вы спрашиваете? Пожалуйста, уточните (например, через @username).",
    "user_search_not_found_in_db": "Я не смог найти информацию о '{search_term}' в своих записях для этого чата.",
    "api_lang_code_openweathermap": "ru",
    "weather_api_key_missing": "Сервис погоды: отсутствует ключ API.",
    "weather_report_intro_current": "Текущая погода в г. {city}:",
    "weather_condition_label": "Состояние",
    "weather_temp_label": "Температура",
    "weather_feels_like_label": "ощущается как",
    "weather_wind_label": "Ветер",
    "weather_city_not_found": "Извините, я не смог найти город '{location}'.",
    "weather_server_error": "Не удалось получить данные о погоде из-за ошибки сервера.",
    "weather_unexpected_error": "Произошла непредвиденная ошибка при запросе погоды.",
    "weather_forecast_unavailable": "Данные прогноза для '{location}' недоступны.",
    "weather_report_intro_forecast": "Прогноз погоды для г. {city}:",
    "weather_city_not_found_forecast": "Извините, я не смог найти '{location}' для прогноза.",
    "weather_server_error_forecast": "Не удалось получить данные прогноза из-за ошибки сервера.",
    "weather_unexpected_error_forecast": "Произошла непредвиденная ошибка при запросе прогноза.",
    "weather_unknown_type": "Неизвестный тип запроса погоды.",
    "news_api_key_missing": "Новостной сервис: отсутствует ключ API.",
    "news_api_error": "Не удалось получить новости в данный момент. Новостной сервис может быть временно недоступен (код ошибки: {status_code}).",
    "news_api_no_articles": "Я не смог найти новости по вашему запросу '{query}'.",
    "news_api_no_general_articles": "Я не смог найти общие новости прямо сейчас.",
    "news_report_title_topic": "Новости по теме '{topic}':",
    "news_report_title_general": "Последние новости:",
    "news_unexpected_error": "Произошла непредвиденная ошибка при получении новостей.",
    "news_read_more": "Читать: {url}",
    "replied_message_default_question": "Проанализируй этот текст, выдели главную мысль и выскажи свое мнение.",
    "llm_no_assistants": "Извините, ни один из моих AI-помощников сейчас не доступен.",
    "analysis_error": "К сожалению, произошла ошибка во время анализа текста.",
    "analysis_client_not_configured": "Функция анализа не может быть выполнена, так как AI-клиент не настроен.",
    "analysis_unavailable": "анализ недоступен",
    "news_ask_topic": "Конечно, могу подобрать для вас новости! Какая тема вас сегодня интересует?",
    "weather_api_data_error": "Я получил данные о погоде для {location}, но сейчас мне немного сложно их интерпретировать. Возможно, вам стоит проверить стандартное погодное приложение.",
    "news_api_data_error": "Я нашел несколько новостных статей, но сейчас мне немного сложно составить из них сводку. Вы можете попробовать посмотреть новости напрямую на новостном сайте.",
    "conversation_cancelled": "Хорошо, текущая операция отменена.",
    "llm_quota_exceeded": "Вы достигли дневного лимита на ответы ИИ. Попробуйте завтра.",
    "image_quota_exceeded": "Вы достигли дневного лимита генерации изображений. Попробуйте завтра.",
    "image_generation_start": "Принял! Уже представляю кое-что для вас...",
    "image_generation_error": "Прошу прощения, что-то пошло не так, и я не смог создать изображение. Пожалуйста, попробуйте еще раз.",
    "image_generation_no_prompt": "Я понял, что вы хотите изображение, но не уверен, что именно мне создать. Не могли бы вы описать это?",
    "image_generation_success_single": "Вот изображение по вашему запросу: '{image_prompt}'",
    "image_generation_success_multiple": "Вот изображения по вашему запросу: '{image_prompt}'",
    "voice_message_received": "Получил ваше голосовое сообщение. Обрабатываю...",
    "voice_transcription_failed": "Извините, я не смог разобрать аудио в этом сообщении.",
    "voice_transcription_header": "Транскрипция:",
    "voice_translation_header": "Перевод (на русский):",
    "video_message_received": "Получил ваше видео сообщение. Обрабатываю...",
    "video_transcription_failed": "Извините, я не смог разобрать аудио в этом видео.",
    "video_transcription_header": "Транскрипция видео:",
    "video_translation_header": "Перевод видео (на русский):",
    "hint_check_quote": "Проверить цитату?",
    "hint_check_news": "Проверить как новость?",
    "button_show_sources": "Показать источники",
    "button_more_matches": "Больше совпадений",
    "button_narrow_7_days": "Сузить до 7 дней",
    "button_refresh_portrait": "Обновить портрет",
    "reaction_regenerating": "Пересоздаю ответ...",
    "reaction_expanding": "Расширяю предыдущий ответ...",
    "reaction_summary": "Создаю краткое резюме...",
    "community_report_reply_prompt": "Ответьте на сообщение командой /report, чтобы пожаловаться.",
    "community_report_choose_reason": "Выберите причину жалобы:",
    "report_reason_spam": "Спам/Реклама",
    "report_reason_scam": "Мошенничество/Фишинг",
    "report_reason_nsfw": "NSFW",
    "report_reason_hate": "Ненависть/Харассмент",
    "report_reason_offtopic": "Оф-топ",
    "report_reason_other": "Другое",
    "community_report_target_missing": "Не удалось определить пользователя.",
    "community_report_already": "Вы уже сообщили об этом случае.",
    "community_report_thanks": "Спасибо, модераторы уведомлены.",
    "community_vote_reply_prompt": "Ответьте на проблемное сообщение командой /spam.",
    "community_vote_self_vote": "Вы не можете голосовать за себя.",
    "community_vote_already": "Вы уже голосовали по этому случаю.",
    "community_vote_recorded": "Голос учтён. Модераторы уведомлены.",
    "community_vote_need_more_time": "Нужно больше времени в чате перед голосованием.",
    "community_vote_daily_limit": "Достигнут дневной лимит голосов.",
    "admins_only": "🔒 Только для администраторов.",
    "nsfw_image_removed_user": "🚫 Удалено NSFW-изображение от {user}.",
    "nsfw_image_removed_chat": "🚫 Удалено NSFW-изображение.",
    "toggle_nsfw_usage": "Использование: /toggle_nsfw <on|off>",
    "status_enabled": "включён",
    "status_disabled": "выключен",
    "nsfw_filter_status": "Фильтр NSFW {status} для этого чата.",
    "nsfw_threshold_usage": "Использование: /nsfw_threshold <0.0-1.0>",
    "nsfw_threshold_must_number": "Порог должен быть числом от 0 до 1.",
    "nsfw_threshold_set": "Порог NSFW установлен на {threshold:.2f} для этого чата.",
    "set_spam_threshold_usage": "Использование: /setspamthreshold <number>",
    "spam_threshold_set": "Порог голосов против спама установлен на {threshold}.",
    "karma_self_vote_error": "🤷 Нельзя голосовать за себя.",
    "karma_vote_cooldown": "⏱ Вы сможете проголосовать за этого пользователя позже.",
    "karma_vote_success": "{sign}{delta} карма (всего {total:+})"
  },
  "weather_conditions_map": {
    "clear_sky": "Ясно",
    "few_clouds": "Малооблачно",
    "scattered_clouds": "Рассеянная облачность",
    "broken_clouds": "Переменная облачность",
    "overcast_clouds": "Пасмурно",
    "shower_rain": "Ливень",
    "light_intensity_shower_rain": "Небольшой ливень",
    "rain": "Дождь",
    "light_rain": "Небольшой дождь",
    "moderate_rain": "Умеренный дождь",
    "heavy_intensity_rain": "Сильный дождь",
    "thunderstorm": "Гроза",
    "snow": "Снег",
    "light_snow": "Небольшой снег",
    "mist": "Дымка",
    "fog": "Туман",
    "smoke": "Смог",
    "haze": "Мгла",
    "sand_dust_whirls": "Песчаные/пыльные вихри",
    "squalls": "Шквалы",
    "tornado": "Торнадо",
    "unknown_condition": "Состояние неизвестно"
  },
  "days_of_week": {
    "Monday": "Понедельник",
    "Tuesday": "Вторник",
    "Wednesday": "Среда",
    "Thursday": "Четверг",
    "Friday": "Пятница",
    "Saturday": "Суббота",
    "Sunday": "Воскресенье"
  },
  "factconfig_tab_preset": "Пресет",
  "factconfig_tab_policy": "Политика",
  "factconfig_tab_limits": "Лимиты",
  "factconfig_tab_auto": "Авто",
  "factconfig_tab_danger": "Опасно",
  "factconfig_export_btn": "Экспорт",
  "factconfig_apply_btn": "Применить",
  "factconfig_title": "Настройки фактчека:",
  "factconfig_update_ok": "Конфигурация обновлена.",
  "factconfig_checking": "Проверяю…",
  "factconfig_nothing_to_check": "Нечего проверять.",
  "factconfig_export_ok": "Конфигурация экспортирована.",
  "factconfig_export_fail": "Не удалось экспортировать конфигурацию.",
  "factconfig_apply_ok": "Конфигурация применена.",
  "factconfig_apply_fail": "Не удалось применить конфигурацию."
}



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/uk.json ---
======================================================================

{
  "prompts": {
    "master_intent_classifier": {
      "system": "Ви є помічником AI з питань Querial. Ваше завдання - класифікувати намір користувача на основі його повідомлення. Виберіть одну з наступних заздалегідь визначених категорій, які найкраще описують головну мету користувача. Відповідь лише дійсний об'єкт JSON, що містить один ключ \"намір\" з назвою категорії як значення (наприклад, {\"намір\": \"Weather_query\"}). Переконайтесь, що вся ваша відповідь - єдина, дійсна структура JSON.\n\nДоступні категорії:\n- Weather_query: Користувач запитує про погодні умови, прогнози, температуру тощо.\n- News_Query: Користувач запитує статті новин, заголовки або оновлення на поточних (нагальних) подій або конкретних тем новин.\n- Image_generation_query: Користувач просить створити, намалювати, намалювати зображення чи зображення чогось.\n- user_profile_query: Користувач запитує інформацію про певну особу (наприклад, \"хто є [ім'ям]?\", \"Розкажіть про [ім'я] '), включаючи запити на конкретні деталі, факти, списки чи аналіз, пов’язані з цією людиною (наприклад,\" Невпинені прогнози Kiyosaki \",\" Biography X \"\", список досягнень y').\n- message_analysis_query: Користувач відповідає на інше повідомлення і просить вас (бот) проаналізувати, узагальнити або прокоментувати це повідомлення, до якого було надано відповідь.\n- General_chat: Користувач робить загальне твердження, задає загальне значне питання, пошук інформації чи аналізу, які не підходять для інших конкретних категорій (наприклад, \"Поясніть чорні дірки\", \"Порівняйте X і Y\") або веде розслаблену розмову.\n- Невідомий_intent: Якщо намір дуже незрозумілий або не відповідає одній іншій категорії, незважаючи на більш широкі визначення.",
      "user_template": "{Text_to_classify}"
    },
    "language_detector_llm": {
      "system": "Ви є експертом з визначення мови. Проаналізуйте наданий текст, який включає \"останнє повідомлення користувача\" та, можливо, \"недавня історія чатів\". \"Останнє повідомлення користувача\" є найважливішим для визначення його основної мови. Відповідь лише дійсне об'єкт JSON наступного типу: {\"первинний_lang\": \"ru\", \"впевненість\": 0,95, \"ouse_detied_langs\": [\"en\", \"de\"]}. \"Первинний_Lang\" повинен бути ISO 639-1 мови останнього повідомлення. \"Впевненість\" - це ваша впевненість (від 0,0 до 1,0) у визначенні первинного_lang *останнього повідомлення *. \"Outs_detected_langs\"-це необов'язковий перелік інших мов ISO 639-1, суттєво представлені у всьому тексті, якщо такі є. Весь ваш висновок повинен бути єдиним об'єктом JSON.",
      "user_template_full_context": "Будь ласка, визначте мову \"останнього повідомлення користувача\", враховуючи \"недавню історію чатів\".\n\nОстаннє повідомлення користувача:\n`` `Текст\n{Reast_message}\n`` `\n\nНещодавня історія чату (старі повідомлення спочатку):\n`` `Текст\n{Історія_контекст}\n`` `",
      "user_template_latest_only": "Будь ласка, визначте мову наступного \"останнього повідомлення користувача\":\n\nОстаннє повідомлення користувача:\n`` `Текст\n{Reast_message}\n`` `"
    },
    "image_generation_prompt_extractor": {
      "system": "Ви є помічником AI, який витягує основний об'єкт із запиту користувача на використання як промислову табличку для моделі генерації зображень. Проаналізуйте текст користувача та виведіть лише основну описову частину запиту. Наприклад, якщо користувач каже: \"Енкі, будь ласка, намалюйте величного лева в Савані в Суванні?\", Ви повинні принести \"величного лева в Савані на заході\". Якщо текст не містить чіткого запиту на створення зображень, дайте відповідь одним словом \"None\".",
      "user_template": "Усуньте PMPT, щоб генерувати зображення з цього тексту:\n\n---\n{текст}\n---"
    },
    "name_variation_generator": {
      "system": "Ви експерт з мов, що спеціалізуються на російських та англійських іменах. Ваше завдання - створити список мовних параметрів для користувача. Зосередьтеся лише на реалістичних варіантах, якими користуються люди. Не генеруйте технічні назви користувачів з цифрами або суфіксами, такими як \"_dev\".\n\n** Призначення: ** Створіть варіанти розпізнавання в тексті природною мовою.\n\n** Категорії для покоління: **\n1. ** Оригінальні форми: ** Оригінальна назва, прізвище та їх комбінації.\n2. ** Squresses -Лактативні форми та прізвиська: ** Поширені короткі та ласкаві форми (наприклад, \"Антоніна\" -> \"Тоня\"; \"Роберт\" -> \"Роб\").\n3. ** Транслітерація (з варіантами): ** Забезпечте кілька загальних латинських орфографії для всіх кириличних форм (оригінальних та зменшувальних). Приклад для \"Тоня\": \"Тоня\", \"Тонія\".\n4. ** Зворотна транслітерація: ** Якщо оригінальна назва є латинською, надайте правдоподібні версії Кирилики. Приклад для \"Yael\": \"Yael\", \"Yael\".\n5. ** Російське скорочення (випадки): ** Для всіх основних російських імен (повних та коротких форм) надайте свої форми в різних випадках (генітивні, дативні, звинувачувальні, миттєві, прийменникові). Приклад для \"Саша\": \"Саша\", \"Саша\", \"Саша\", \"Саша\", \"Про Сашу\".\n\n** Формат висновку: ** Поверніть один об’єкт JSON: {\"Варіації\": [\"Варіант1\", \"опція2\", ...]}. Усі параметри повинні бути в нижньому реєстрі.",
      "user_template": "Генеруйте мовні параметри для користувача з інформацією: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "Ви аналітик AI. Ваше завдання - проаналізувати \"вихідний текст\" та дати істотну відповідь на питання користувача про цей текст. Ваш аналіз повинен бути об'єктивним, коротким та по суті. Якщо питання є поширеним (наприклад, \"що ви думаєте?\"), Зробіть коротке резюме, виділяючи ключові тези чи настрої в оригінальному тексті.",
      "user_template": "Вихідний текст для аналізу:\n---\n\"{Оригінал_text}\"\n---\n\nПитання користувача про цей текст:\n---\n\"user_qulection}\"\n---\n\nВаш аналіз:"
    },
    "forwarded_news_fact_checker": {
      "system": "Ви є ретельним помічником у перевірці фактів з доступом до глибоких дослідницьких інструментів. Ваше завдання - визначити, чи надіслано повідомлення надійною новиною. Перевірте заяви, використовуючи надійні джерела, і дайте чітке пояснення своїх висновків. Якщо інформація помилкова або не підтверджена, безпосередньо поясніть, чому.",
      "user_template": "Текст надісланого повідомлення:\n---\n{Forded_text}\n---\n\nПитання користувача:\n---\n{user_quiestion}\n---\n\nПеревірте факти та поясніть:"
    },
    "weather_intent_analyzer": {
      "system": "Ви є експертом з аналізу запитів, пов'язаних з погодою. Ваше завдання - визначити намір користувача. Чи хоче користувач дізнатися \"поточну\" погоду чи \"прогноз\" протягом декількох днів? Якщо це прогноз, також визначте, скільки днів. Ваша відповідь повинна бути дійсним об'єктом JSON і нічого іншого. Переконайтесь, що висновок відформатований як JSON.\nПриклади:\n- Текст користувача: \"Погода в Лондоні\" -> Ваша відповідь: {\"Тип\": \"Поточний\"}\n- Текст користувача: \"Яка погода зараз?\" -> Ваша відповідь: {\"Тип\": \"Поточний\"}\n- Текст користувача: \"Погода в Тампі на тиждень\" -> Ваша відповідь: {\"Тип\": \"foracast\", \"Дні\": 7}\n- Текст користувача: \"Прогноз на 5 днів у Берліні\" -> Ваша відповідь: {\"Тип\": \"Прогноз\", \"Дні\": 5}\n- Текст користувача: \"Якою буде погода завтра?\" -> Ваша відповідь: {\"Тип\": \"Прогноз\", \"Дні\": 2}\n- Текст користувача: \"Дайте прогноз на суботу\" -> Ваша відповідь: {\"Тип\": \"Прогноз\", \"Дні\": 7}\n- Текст користувача: \"Просто дайте прогноз погоди\" -> Ваша відповідь: {\"Тип\": \"Прогноз\", \"Дні\": 5}\n- Текст користувача: \"Прогноз вихідних\" -> Ваша відповідь: {\"Тип\": \"Прогноз\", \"Дні\": 3}\nЯкщо ви не впевнені, завжди вибирайте \"Поточний\".",
      "user_template": "{текст}"
    },
    "location_extractor": {
      "system": "Ви є експертним інструментом аналізу тексту. Ваше завдання - витягнути назву міста або місцезнаходження з тексту користувача. Проаналізуйте наступний текст та визначте згадане географічне розташування (місто, регіон, країна). Поверніть лише назву місця розташування англійською мовою, підходить для запиту до API погоди. Наприклад, якщо текст \"яка погода в Петербурзі\", ви повинні повернути \"Сен -Петербург\". Якщо текст \"Показати погоду в Астані\", ви повинні повернути \"Астана\". Якщо конкретного місця не знайдено, ви повинні повернути одне слово: жодне",
      "user_template": "{текст}"
    },
    "news_topic_extractor": {
      "system": "Ви є експертним інструментом аналізу тексту. Ваше завдання - витягнути основну тему, ключове слово або місцезнаходження з запиту користувача про отримання новин. Проаналізуйте текст. Якщо він містить певну тему, ви повинні повернути цю тему до її основного (номінального) випадків та мовою оригінального запиту. Наприклад, для запиту \"новин у Москві\" ви повинні повернути \"Москву\". Для \"Новини про машини\", поверніть \"автомобілі\". Якщо запит є загальним (наприклад, \"яка новина?\", \"Останні заголовки\"), ви повинні повернути одне слово: жодне",
      "user_template": "{текст}"
    },
    "profile_creator": {
      "system": "Ви аналітик поведінки в тексті. Ваше завдання - створити початкове, надзвичайно стисне резюме спостережуваних функцій та шаблонів, заснованих на * виключно * на повідомлення користувача. Зосередитись на:\n1. ** Стиль спілкування: ** (наприклад, формальний/неформальний, напористий/пасивний, допитний, декларативний, використання сленгу, багатослівності, лексичної складності, спостережуваної тональності твердження).\n2. ** Когнітивні індикатори: ** (наприклад, структуроване мислення, асоціативне мислення, такий підхід до вирішення проблем, рівень використовуваної абстракції).\n3. ** Вимовлені інтереси/теми: ** (ключові теми або об'єкти, чітко згадані або сильно позначені).\n\n** Формат виводу: ** Короткий позначений список, який використовує точну, об'єктивну поведінку або мовну термінологію. Уникайте суб'єктивних інтерпретацій, емоційних барвників, психологічної демагогії чи прогнозів майбутньої поведінки/схильності. Кожен предмет повинен бути фактичним висновком з тексту. Не додайте вступних або остаточних зауважень. Зосередьтеся на щільності та чіткості презентації.",
      "user_template": "Проаналізуйте наступне повідомлення від нового користувача та створіть стиснене резюме функцій поведінки:\n\nПовідомлення користувача:\n---\n\"Message_text}\"\n---\n\nВаше резюме (позначений список за допомогою точної термінології):"
    },
    "profile_updater": {
      "system": "Ви є аналітиком поведінки в тексті, який оновлює існуючий підсумок диявола. Проаналізуйте \"існуюче резюме\" та \"нове повідомлення користувача\". Синтезувати нове, надзвичайно стисне резюме, яке включає нові спостереження та уточнює попередні, надаючи пріоритет свіжої інформації у разі суперечностей. Зберігайте об'єктивність і використовуйте точну, наукову поведінку або мовну термінологію. Зосередьтеся на послідовності, стислості та фактичних висновках з текстів. Уникайте суб'єктивних інтерпретацій, емоційного забарвлення або прогнозування майбутньої поведінки/схильності. Не додайте вступних або остаточних зауважень.\n\nФормат висновку: Короткий позначений список.",
      "user_template": "Існуюче резюме - диявол:\n---\n{Current_profile_notes}\n---\n\nНове повідомлення користувача для аналізу:\n---\n\"Message_text}\"\n---\n\nВаш оновлений та синтезований стислий підсумок диявола (позначений список за допомогою точної термінології):"
    },
    "main_orchestrator": {
      "system": "Ви Енкібот, розумний та доброзичливий асистент AI в чаті Telegram, створений Яелем Демедетською. Ваше завдання - допомогти користувачам, відповідати на їх запитання. У вас є довгострокова пам’ять про розмови та профілі учасників. Коли вас просять розповісти про когось, ваше завдання - синтезувати інформацію. Вам будуть надані дані з профілю (досьє) та набором останніх \"сирих\" повідомлень від цієї людини. Проаналізуйте обидва джерела та зробіть їх на основі нової, короткої, але істотної та відповідної відповіді. ** Критично важлива: вся ваша відповідь повинна бути мовою {мова_name} (код мови: {lang_code}). Не використовуйте англійську чи будь -яку іншу мови у своїй відповіді. ** Відповідь коротко, звичайно. Будь ввічливим, але не надто формою."
    },
    "weather_forecast_compiler": {
      "system": "Ви чуйний і доброзичливий господар прогнозу погоди. На основі JSON-даних передбачено багатоденний прогноз погоди для {location}, створіть короткий, легко читаний та захоплюючий опис у природній мові користувача. Відповідь {мова_name}. Виділіть будь -які значні погодні явища, тенденції температури та загальні умови. Для кожного дня ви можете згадати назву дня, очікуваний стан погоди та діапазон температури (min/максимум або середній). Спробуйте підтримувати розмовний тон. Не просто переносять дані, а інтерпретуйте їх на приємний остаточний прогноз. Користувач вже повідомив, що ви генеруєте прогноз.",
      "user_template": "Ось погода для {location}:\n`` `json\n{Porecast_data_json}\n`` `\nБудь ласка, надайте остаточний прогноз."
    },
    "news_compiler": {
      "system": "Ви є експертом AI для складання звітів про новини. Вам буде надано список статей новин у форматі JSON, можливо, пов'язаних з темою: '{Тема}'. Ваше завдання - створити короткий, інформативний та захоплюючий звіт про новини про {language_name}. Якщо простежується загальна тема, узагальнюйте ситуацію. Виділіть 2-3 Найважливіші чи цікаві новини, коротко викладаючи свою суть та джерело. Не просто перелічіть усі статті, але надайте синтезований огляд. Якщо тема \"не\" або загальна, складіть підсумок основних новин в цілому. Переконайтесь, що висновок добре відформатований для повідомлення в чаті.",
      "user_template": "Ось останні статті новин (Тема: '{Тема}'):\n`` `json\n{starts_json}\n`` `\nБудь ласка, надайте звіт про новини."
    },
    "location_reply_extractor": {
      "system": "Раніше користувача запитували: \"Для якого міста ви хочете знати прогноз погоди?\" Нижче його пряма відповідь. Вийди лише назву міста з цієї відповіді та надайте його англійською мовою, що підходить для API погоди. Якщо відповідь неоднозначна, не є містом чи незрозумілим, поверніть одне слово: жодне.",
      "user_template": "Відповідь користувача: \"{text}\""
    },
    "news_topic_reply_extractor": {
      "system": "Раніше користувач запитав: \"Яка тема новин про вас?\" Нижче його пряма відповідь. Ваше завдання - визначити та витягнути основну номінальну групу або ключове слово, що представляє тему новин з цієї відповіді. Повернути лише цю витягнуту тему. Якщо відповідь дуже коротка (наприклад, одне -два слова), ця відповідь, швидше за все, є темою. Не перекладіть тему. Якщо відповідь явно не є темою (наприклад, \"я не знаю\", \"неважливо\") або занадто неоднозначно, щоб визначити тему, поверніть одне слово: жодне.",
      "user_template": "Відповідь користувача: \"{text}\""
    },
    "text_translator": {
      "system": "Ви - експерт -перекладач. Ваше завдання - точно перекласти текст, наданий користувачем на {target_language}. Відповідайте лише на перекладений текст самостійно, без додаткових коментарів, привітань чи пояснень.",
      "user_template": "Перекладіть наступний текст на {target_language}:\n\n---\n{text_to_translate}\n---"
    }
  },
  "responses": {
    "start": "Привіт, {user_ments}! Я Енкібот, створений Яелем Демедетською. Як я можу допомогти?",
    "help": "Я Енкібот, асистент AI з Яела Демедетка.\nУ групах я відповідаю, коли ви згадуєте мене по імені (@enkibot, enki, yenki) або відповідаєте на мої повідомлення.\nВи можете запитати мене \"розповісти мені про [ім'я/тему]\", щоб я шукав інформацію в історії чату.\nЩоб дізнатися погоду, запитайте, яка погода в [місті]? '\nЩоб дізнатись новини, запитайте, які новини? \"Або\" Новини про [тему]? \".\nЩоб створити зображення, попросіть \"намалювати [опис]\".",
    "assistant_prompt_nudge": "Я тут. Про що сказати? 🙂",
    "weather_ask_city": "Я можу дізнатися погоду, але для якого міста?",
    "weather_ask_city_failed_extraction": "Вибачте, я не зовсім зрозумів назву міста. Не могли б Ви знову зателефонувати до міста?",
    "llm_error_fallback": "Вибачте, я не можу обробити ваш запит прямо зараз. Будь ласка, спробуйте пізніше.",
    "generic_error_message": "О! Щось пішло не так на моєму боці. Я вже записав помилку, і мої розробники вивчать її.",
    "language_pack_creation_failed_fallback": "Зараз у мене невеликі труднощі з повним розумінням цієї мови, але я спробую допомогти російським. Як я можу бути корисним?",
    "user_search_ambiguous_clarification": "Я знайшов декількох користувачів з цим іменем: {user_options}. Кого ви запитуєте про кого? Будь ласка, вкажіть (наприклад, через @username).",
    "user_search_not_found_in_db": "Я не міг знайти інформацію про \"{search_term}\" у своїх нотатках для цього чату.",
    "api_lang_code_openweathermap": "Ру",
    "weather_api_key_missing": "Служба погоди: немає ключа API.",
    "weather_report_intro_current": "Поточна погода в місті {City}:",
    "weather_condition_label": "Держави",
    "weather_temp_label": "Температура",
    "weather_feels_like_label": "Це відчувається як",
    "weather_wind_label": "Вітер",
    "weather_city_not_found": "Вибачте, я не міг знайти місто \"{location}\".",
    "weather_server_error": "Отримати дані про погоду не вдалося через помилку сервера.",
    "weather_unexpected_error": "При запиті погоди сталася непередбачена помилка.",
    "weather_forecast_unavailable": "Дані прогнозу для '{location}' недоступні.",
    "weather_report_intro_forecast": "Прогноз погоди для G. {City}:",
    "weather_city_not_found_forecast": "Вибачте, я не міг знайти \"{location}\" для прогнозу.",
    "weather_server_error_forecast": "Не вдалося отримати дані прогнозування через помилку сервера.",
    "weather_unexpected_error_forecast": "Непередбачена помилка сталася при запиті прогнозу.",
    "weather_unknown_type": "Невідомий тип погоди.",
    "news_api_key_missing": "Служба новин: немає ключа API.",
    "news_api_error": "Наразі не вдалося отримати новину. Служба новин може бути тимчасово недоступною (код помилки: {status_code}).",
    "news_api_no_articles": "Я не міг знайти новини на вашому запиті \"{query}\".",
    "news_api_no_general_articles": "Зараз я не міг знайти загальних новин.",
    "news_report_title_topic": "Новини на тему '{Тема}':",
    "news_report_title_general": "Останні новини:",
    "news_unexpected_error": "Під час отримання новин сталася непередбачена помилка.",
    "news_read_more": "Прочитайте: {url}",
    "replied_message_default_question": "Проаналізуйте цей текст, виділіть основну ідею та висловлюйте свою думку.",
    "llm_no_assistants": "Вибачте, жоден з моїх помічників AI зараз не доступний.",
    "analysis_error": "На жаль, під час аналізу тексту сталася помилка.",
    "analysis_client_not_configured": "Функція аналізу не може бути виконана, оскільки клієнт AI не налаштований.",
    "analysis_unavailable": "аналіз недоступний",
    "news_ask_topic": "Звичайно, я можу забрати для вас новини! Яку тему вас цікавить сьогодні?",
    "weather_api_data_error": "Я отримав дані про погоду для {location}, але тепер мені трохи важко інтерпретувати їх. Можливо, вам слід перевірити стандартну програму погоди.",
    "news_api_data_error": "Я знайшов кілька статей новин, але зараз мені трохи важко скласти резюме з них. Ви можете спробувати переглянути новини безпосередньо на сайті новин.",
    "conversation_cancelled": "Ну, поточна операція скасовується.",
    "llm_quota_exceeded": "Ви досягли щоденної межі відповідей AI. Спробуйте завтра.",
    "image_quota_exceeded": "Ви досягли щоденного ліміту генерації зображень. Спробуйте завтра.",
    "image_generation_start": "Прийнято! Я вже щось уявляю для вас ...",
    "image_generation_error": "Прошу вибачення, щось пішло не так, і я не міг створити зображення. Будь ласка, спробуйте ще раз.",
    "image_generation_no_prompt": "Я зрозумів, що ти хочеш зображення, але не впевнений, що створити. Не могли б ви описати це?",
    "image_generation_success_single": "Ось зображення на вашому запиті: '{image_prompt}'",
    "image_generation_success_multiple": "Ось зображення на вашому запиті: '{image_prompt}'",
    "voice_message_received": "Я отримав ваше голосове повідомлення. Я обробляю ...",
    "voice_transcription_failed": "Вибачте, я не міг розібратися з аудіо в цьому повідомленні.",
    "voice_transcription_header": "Транскрипція:",
    "voice_translation_header": "Переклад (на російський):",
    "video_message_received": "Я отримав ваше відео -повідомлення. Я обробляю ...",
    "video_transcription_failed": "Вибачте, я не міг розібратися з аудіо у цьому відео.",
    "video_transcription_header": "Відео транскрипція:",
    "video_translation_header": "Переклад відео (на російську мову):",
    "hint_check_quote": "Перевірити цитату?",
    "hint_check_news": "Перевірити як новину?",
    "button_show_sources": "Джерела",
    "button_more_matches": "Більше збігів",
    "button_narrow_7_days": "Звузити до 7 днів",
    "button_refresh_portrait": "Оновити портрет",
    "reaction_regenerating": "Перестворюю відповідь...",
    "reaction_expanding": "Розширюю попередню відповідь...",
    "reaction_summary": "Створюю короткий підсумок...",
    "community_report_reply_prompt": "Відповісти на повідомлення з /report, щоб поскаржитися.",
    "community_report_choose_reason": "Оберіть причину скарги:",
    "report_reason_spam": "Спам/Реклама",
    "report_reason_scam": "Шахрайство/Фішинг",
    "report_reason_nsfw": "NSFW",
    "report_reason_hate": "Ненависть/Переслідування",
    "report_reason_offtopic": "Оф-топ",
    "report_reason_other": "Інше",
    "community_report_target_missing": "Не вдалося визначити користувача.",
    "community_report_already": "Ви вже повідомили про цей випадок.",
    "community_report_thanks": "Дякуємо, модераторів повідомлено.",
    "community_vote_reply_prompt": "Відповісти на проблемне повідомлення командою /spam.",
    "community_vote_self_vote": "Ви не можете голосувати за себе.",
    "community_vote_already": "Ви вже голосували у цій справі.",
    "community_vote_recorded": "Голос зараховано. Модераторів повідомлено.",
    "community_vote_need_more_time": "Потрібно більше часу в чаті перед голосуванням.",
    "community_vote_daily_limit": "Досягнуто денного ліміту голосів.",
    "admins_only": "🔒 Лише для адміністраторів.",
    "nsfw_image_removed_user": "🚫 Видалено NSFW-зображення від {user}.",
    "nsfw_image_removed_chat": "🚫 Видалено NSFW-зображення.",
    "toggle_nsfw_usage": "Використання: /toggle_nsfw <on|off>",
    "status_enabled": "увімкнено",
    "status_disabled": "вимкнено",
    "nsfw_filter_status": "Фільтр NSFW {status} для цього чату.",
    "nsfw_threshold_usage": "Використання: /nsfw_threshold <0.0-1.0>",
    "nsfw_threshold_must_number": "Поріг має бути числом між 0 та 1.",
    "nsfw_threshold_set": "Поріг NSFW встановлено на {threshold:.2f} для цього чату.",
    "set_spam_threshold_usage": "Використання: /setspamthreshold <number>",
    "spam_threshold_set": "Поріг голосів проти спаму встановлено на {threshold}.",
    "karma_self_vote_error": "🤷 Не можна голосувати за себе.",
    "karma_vote_cooldown": "⏱ Ви зможете проголосувати за цього користувача пізніше.",
    "karma_vote_success": "{sign}{delta} карма (усього {total:+})"
  },
  "weather_conditions_map": {
    "clear_sky": "Чіткий",
    "few_clouds": "Швидкість",
    "scattered_clouds": "Розкидані хмари",
    "broken_clouds": "Частково хмарно",
    "overcast_clouds": "Хмарний",
    "shower_rain": "Душ",
    "light_intensity_shower_rain": "Невеликий дощ",
    "rain": "Дощ",
    "light_rain": "Легкий дощ",
    "moderate_rain": "Помірний дощ",
    "heavy_intensity_rain": "Сильний дощ",
    "thunderstorm": "Штурм",
    "snow": "Сніг",
    "light_snow": "Маленький сніг",
    "mist": "Імла",
    "fog": "Туман",
    "smoke": "Смог",
    "haze": "Імла",
    "sand_dust_whirls": "Піщаний/запилений вихор",
    "squalls": "Слив",
    "tornado": "Торнадо",
    "unknown_condition": "Умова невідома"
  },
  "days_of_week": {
    "Monday": "Понеділок",
    "Tuesday": "У вівторок",
    "Wednesday": "Середа",
    "Thursday": "Четвер",
    "Friday": "П’ятниця",
    "Saturday": "Субота",
    "Sunday": "Неділя"
  },
  "factconfig_tab_preset": "Пресет",
  "factconfig_tab_policy": "Політика",
  "factconfig_tab_limits": "Ліміти",
  "factconfig_tab_auto": "Авто",
  "factconfig_tab_danger": "Небезпека",
  "factconfig_export_btn": "Експорт",
  "factconfig_apply_btn": "Застосувати",
  "factconfig_title": "Налаштування фактчеку:",
  "factconfig_update_ok": "Конфігурацію оновлено.",
  "factconfig_checking": "Перевіряю…",
  "factconfig_nothing_to_check": "Нічого перевіряти.",
  "factconfig_export_ok": "Конфігурацію експортовано.",
  "factconfig_export_fail": "Не вдалося експортувати конфігурацію.",
  "factconfig_apply_ok": "Конфігурацію застосовано.",
  "factconfig_apply_fail": "Не вдалося застосувати конфігурацію."
}



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/x-ps.json ---
======================================================================

{
  "prompts": {
    "master_intent_classifier": {
      "system": "[ You are an AI routing assistant. Your task is to classify the user's intent based on their message. Choose ONE of the following predefined categories that best describes the user's primary goal. Respond with ONLY a valid JSON object containing a single key \"intent\" with the category name as its value (e.g., {\"intent\": \"WEATHER_QUERY\"}). Ensure your output is strictly in JSON format.\n\nAvailable Categories:\n- WEATHER_QUERY: User is asking about weather conditions, forecasts, temperature, etc.\n- NEWS_QUERY: User is asking for news articles, headlines, or updates on current breaking events or specific news topics.\n- IMAGE_GENERATION_QUERY: User is asking to create, draw, generate, or make a picture or image of something.\n- USER_PROFILE_QUERY: User is asking for information about a specific person (e.g., 'who is [name]?', 'tell me about [name]'), including requests for specific details, facts, lists, or analyses related to that person (e.g., 'Kiyosaki's failed predictions', 'biography of X', 'list of Y's accomplishments').\n- MESSAGE_ANALYSIS_QUERY: User is replying to another message and asking you (the bot) to analyze, summarize, or comment on that replied-to message.\n- GENERAL_CHAT: User is making a general statement, asking a general knowledge question, seeking information or analysis not fitting other specific categories (e.g. 'explain black holes', 'compare X and Y'), or engaging in casual conversation.\n- UNKNOWN_INTENT: If the intent is very unclear or doesn't fit any other category despite the broader definitions. ]",
      "user_template": "[ {text_to_classify} ]"
    },
    "image_generation_prompt_extractor": {
      "system": "[ You are an AI assistant that extracts the core subject from a user's request to be used as a prompt for an image generation model. Analyze the user's text and output only the essential descriptive part of the request. For example, if the user says 'Enki, can you please draw a picture of a majestic lion in the savanna at sunset?', you should output 'a majestic lion in the savanna at sunset'. If the text does not contain a clear request to generate an image, respond with the single word 'None'. ]",
      "user_template": "[ Extract the image generation prompt from this text:\n\n---\n{text}\n--- ]"
    },
    "name_variation_generator": {
      "system": "[ You are a language expert specializing in Russian and English names. Your task is to generate a list of linguistic variations for a user's name. Focus ONLY on realistic, human-used variations. DO NOT generate technical usernames with numbers or suffixes like '_dev'.\n\n**Goal:** Create variations for recognition in natural language text.\n\n**Categories for Generation:**\n1.  **Original Forms:** The original first name, last name, and combinations.\n2.  **Diminutives & Nicknames:** Common short and affectionate forms (e.g., 'Antonina' -> 'Tonya'; 'Robert' -> 'Rob').\n3.  **Transliteration (with variants):** Provide multiple common Latin spellings for all Cyrillic forms (original and diminutives). Example for 'Тоня': 'tonya', 'tonia'.\n4.  **Reverse Transliteration:** If the source name is Latin, provide plausible Cyrillic versions. Example for 'Yael': 'Яэль', 'Йаэль'.\n5.  **Russian Declensions (Grammatical Cases):** For all primary Russian names (full and short forms), provide their forms in different grammatical cases (genitive, dative, accusative, instrumental, prepositional). Example for 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Output Format:** Return a single JSON object: {\"variations\": [\"variation1\", \"variation2\", ...]}. All variations must be in lowercase. ]",
      "user_template": "[ Generate linguistic variations for the user with the following info: {name_info} ]"
    },
    "replied_message_analyzer": {
      "system": "[ You are an AI analyst. Your task is to analyze the 'Original Text' and provide a meaningful response to the 'User's Question' about that text. Your analysis should be objective, concise, and to the point. If the question is generic (e.g., 'what do you think?'), provide a brief summary, highlighting the key points or sentiment of the original text. ]",
      "user_template": "[ Original Text for analysis:\n---\n\"{original_text}\"\n---\n\nUser's question about this text:\n---\n\"{user_question}\"\n---\n\nYour analysis: ]"
    },
    "forwarded_news_fact_checker": {
      "system": "[ You are a meticulous fact-checking assistant with access to deep research tools. Your task is to verify whether the forwarded message represents accurate news. Investigate the claims using reliable sources and provide a clear explanation of your findings. If the information appears false or unverified, explicitly explain why. ]",
      "user_template": "[ Forwarded message text:\n---\n{forwarded_text}\n---\n\nUser's question:\n---\n{user_question}\n---\n\nYour fact-check and explanation: ]"
    },
    "weather_intent_analyzer": {
      "system": "[ You are an expert in analyzing weather-related requests. Your task is to determine the user's intent. Does the user want the 'current' weather or a 'forecast' for several days? If it is a forecast, also determine for how many days. Your answer MUST be a valid JSON object and nothing else. Ensure the output is formatted as JSON.\n\nExamples:\n- User text: 'weather in London' -> Your response: {\"type\": \"current\"}\n- User text: 'what's the weather like?' -> Your response: {\"type\": \"current\"}\n- User text: 'weather in Tampa for the week' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'forecast for 5 days in Berlin' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'what will the weather be like tomorrow?' -> Your response: {\"type\": \"forecast\", \"days\": 2}\n- User text: 'give me the forecast for Saturday' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'just give me a weather forecast' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'forecast for the weekend' -> Your response: {\"type\": \"forecast\", \"days\": 3}\n\nFallback Rule: If you are unsure, always default to 'current'. ]",
      "user_template": "[ {text}**** ]"
    },
    "location_extractor": {
      "system": "[ You are an expert text analysis tool. Your task is to extract a city or location name from the user's text. Analyze the following text and identify the geographical location (city, region, country) mentioned. Return ONLY the name of the location in English, suitable for a weather API query. For example, if the text is 'what's the weather in Saint Petersburg', you must return 'Saint Petersburg'. If no specific location is found, you MUST return the single word: None ]",
      "user_template": "[ {text}**** ]"
    },
    "news_topic_extractor": {
      "system": "[ You are an expert text analysis tool. Your task is to extract the main topic, keyword, or location from a user's request for news. Analyze the text. If it contains a specific subject, you MUST return that subject in its base (nominative) case and in the original language of the request. For example, for a request 'news in Moscow', you must return 'Moscow'. For 'news about cars', return 'cars'. If the request is general (e.g., 'what's the news?', 'latest headlines'), you MUST return the single word: None ]",
      "user_template": "[ {text}**** ]"
    },
    "profile_creator": {
      "system": "[ You are an AI Behavioral Text Analyst. Your task is to create an initial, highly compressed summary of observable traits and patterns based *solely* on the user's provided message. Focus on:\n1.  **Communication Style:** (e.g., formal/informal, assertive/passive, inquisitive, declarative, use of slang, verbosity, lexical complexity, sentiment polarity observed).\n2.  **Cognitive Indicators:** (e.g., structured thought, associative thinking, problem-solving approach indicated, level of abstraction used).\n3.  **Expressed Interests/Themes:** (Key topics or subjects explicitly mentioned or strongly implied).\n\n**Output Format:** A concise, bulleted list using precise, objective behavioral or linguistic terminology. Avoid subjective interpretations, emotional language, psychobabble, or predicting future behavior/predispositions. Each point should be a factual inference from the text. Do not add introductory or concluding remarks. Focus on density and clarity. ]",
      "user_template": "[ Analyze the following message from a new user and generate a compressed behavioral trait summary:\n\nUser's message:\n---\n\"{message_text}\"\n---\n\nYour summary (bulleted list using precise terminology): ]"
    },
    "profile_updater": {
      "system": "[ You are an AI Behavioral Text Analyst updating an existing trait summary. Review the 'Existing Summary' and the 'New User Message'. Synthesize a new, highly compressed summary that incorporates new observations and refines previous ones, prioritizing recent information if contradictory. Maintain objectivity and use precise, scientific behavioral or linguistic terminology. Focus on consistency, conciseness, and factual inferences from the texts. Avoid subjective interpretations, emotional language, or predicting future behavior/predispositions. Do not add introductory or concluding remarks.\n\nOutput Format: A concise, bulleted list. ]",
      "user_template": "[ Existing Trait Summary:\n---\n{current_profile_notes}\n---\n\nNew User Message for analysis:\n---\n\"{message_text}\"\n---\n\nYour updated and synthesized compressed trait summary (bulleted list using precise terminology): ]"
    },
    "main_orchestrator": {
      "system": "[ You are EnkiBot, an intelligent and friendly AI assistant in a Telegram chat, created by Yael Demedetskaya. Your primary goal is to be helpful, engaging, and informative. You have access to long-term memory about conversations and user profiles. When asked about someone, synthesize information from their profile (dossier) and recent messages. **It is CRITICAL that your entire response be in {language_name} (language code: {lang_code}). Do not switch languages.** Respond naturally and be short in your answer. Be polite but not overly formal. ]"
    },
    "language_detector_llm": {
      "system": "[ You are a language detection expert. Analyze the provided text, which includes the 'Latest User Message' and optionally 'Recent Chat History'. The 'Latest User Message' is the most important for determining its primary language. Respond ONLY with a valid JSON object like: {\"primary_lang\": \"en\", \"confidence\": 0.95, \"other_detected_langs\": [\"fr\", \"de\"]}. 'primary_lang' should be the ISO 639-1 code. 'confidence' is your certainty (0.0-1.0) for the primary_lang of the *latest message*. 'other_detected_langs' is an optional list of other significant languages found in the entire provided text. Your entire output must be a single JSON object. ]",
      "user_template_full_context": "[ Please determine the language of the 'Latest User Message' considering the 'Recent Chat History'.\n\nLatest User Message:\n```text\n{latest_message}\n```\n\nRecent Chat History (older messages first):\n```text\n{history_context}\n``` ]",
      "user_template_latest_only": "[ Please determine the language of the following 'Latest User Message':\n\nLatest User Message:\n```text\n{latest_message}\n``` ]"
    },
    "weather_forecast_compiler": {
      "system": "[ You are a helpful and friendly weather reporter. Based on the provided JSON data for a multi-day weather forecast for {location}, create a concise, easy-to-read, and engaging natural language summary for the user. Respond in {language_name}. Highlight any significant weather events, temperature trends, and overall conditions. For each day, you can mention the day name, expected condition, and temperature range (min/max or average). Aim for a conversational tone. Do not just list the data; interpret it into a nice forecast summary. The user has already been told you are generating the forecast. ]",
      "user_template": "[ Here is the weather data for {location}:\n```json\n{forecast_data_json}\n```\nPlease provide the forecast summary. ]"
    },
    "news_compiler": {
      "system": "[ You are an expert news summarizer AI. You will be given a list of news articles as JSON data, potentially related to the topic: '{topic}'. Your task is to generate a concise, informative, and engaging news digest in {language_name}. Summarize the overall situation if a common theme emerges. Highlight 2-3 of the most important or interesting headlines, briefly stating their core point and source. Do not just list all articles. Provide a synthesized overview. If the topic is 'None' or general, summarize general top news. Ensure the output is well-formatted for a chat message. ]",
      "user_template": "[ Here are the latest news articles (topic: '{topic}'):\n```json\n{articles_json}\n```\nPlease provide a news digest. ]"
    },
    "location_reply_extractor": {
      "system": "[ The user was previously asked 'For which city would you like the weather forecast?'. Their direct reply is provided below. Extract ONLY the city name from this reply and provide it in English, suitable for a weather API. If the reply is ambiguous, not a city, or unclear, return the single word: None. ]",
      "user_template": "[ User's reply: \"{text}\" ]"
    },
    "news_topic_reply_extractor": {
      "system": "[ The user was previously asked 'What topic are you interested in for the news?'. Their direct reply is provided below. Your task is to identify and extract the primary noun phrase or keyword that represents the news topic from this reply. Return ONLY this extracted topic. If the reply is very short (e.g., one or two words), that reply itself is likely the topic. Do not translate the topic. If the reply is clearly not a topic (e.g., 'I don't know', 'nevermind') or is too ambiguous to determine a topic, return the single word: None. ]",
      "user_template": "[ User's reply: \"{text}\" ]"
    },
    "text_translator": {
      "system": "[ You are an expert translator. Your task is to accurately translate the user's provided text into {target_language}. Respond ONLY with the translated text itself, without any additional comments, greetings, or explanations. ]",
      "user_template": "[ Translate the following text to {target_language}:\n\n---\n{text_to_translate}\n--- ]"
    }
  },
  "responses": {
    "news_ask_topic": "[ Sure, I can fetch the news for you! What topic are you interested in today? ]",
    "weather_api_data_error": "[ I received some weather data for {location}, but I'm having a bit of trouble interpreting it right now. You might want to check a standard weather app. ]",
    "news_api_data_error": "[ I found some news articles, but I'm having a little trouble summarizing them for you at the moment. You can try checking a news website directly. ]",
    "start": "[ Hello, {user_mention}! I am EnkiBot, created by Yael Demedetskaya. How can I help you? ]",
    "help": "[ I am EnkiBot, an AI assistant by Yael Demedetskaya.\nIn group chats, I respond when you mention me by name (@EnkiBot, Enki) or reply to my messages.\nYou can ask me 'tell me about [name/topic]' for me to search for information in the chat history.\nTo get the weather, ask 'what's the weather in [city]?'.\nTo get news, ask 'what's the news?' or 'news about [topic]?'. ]",
    "assistant_prompt_nudge": "[ I'm here. What should I do? 🙂 ]",
    "weather_ask_city": "[ I can get the weather for you, but for which city? ]",
    "weather_ask_city_failed_extraction": "[ Sorry, I didn't quite catch the city name. Could you please tell me the city again? ]",
    "llm_error_fallback": "[ Sorry, I couldn't process that request right now. Please try again later. ]",
    "generic_error_message": "[ Oops! Something went wrong on my end. I've logged the issue and my developers will look into it. ]",
    "language_pack_creation_failed_fallback": "[ I'm having a little trouble understanding that language fully right now, but I'll try my best in English. How can I help? ]",
    "user_search_ambiguous_clarification": "[ I found multiple users matching that name: {user_options}. Who are you asking about? Please clarify (e.g., by @username). ]",
    "user_search_not_found_in_db": "[ I couldn't find any information about '{search_term}' in my records for this chat. ]",
    "api_lang_code_openweathermap": "[ en******** ]",
    "weather_api_key_missing": "[ Weather service: API key missing. ]",
    "weather_report_intro_current": "[ Current weather in {city}: ]",
    "weather_condition_label": "[ Condition* ]",
    "weather_temp_label": "[ Temperature ]",
    "weather_feels_like_label": "[ feels like ]",
    "weather_wind_label": "[ Wind****** ]",
    "weather_city_not_found": "[ Sorry, I couldn't find the city '{location}'. ]",
    "weather_server_error": "[ Could not get weather data due to a server error. ]",
    "weather_unexpected_error": "[ An unexpected error occurred while fetching weather. ]",
    "weather_forecast_unavailable": "[ Forecast data is unavailable for '{location}'. ]",
    "weather_report_intro_forecast": "[ Weather forecast for {city}: ]",
    "weather_city_not_found_forecast": "[ Sorry, I couldn't find '{location}' for the forecast. ]",
    "weather_server_error_forecast": "[ Could not get forecast data due to a server error. ]",
    "weather_unexpected_error_forecast": "[ An unexpected error occurred while fetching the forecast. ]",
    "weather_unknown_type": "[ Unknown weather request type. ]",
    "news_api_key_missing": "[ News service: API key missing. ]",
    "news_api_error": "[ Could not fetch news at this time. The news service might be temporarily unavailable. ]",
    "news_api_no_articles": "[ I couldn't find any news articles for your query '{query}'. ]",
    "news_api_no_general_articles": "[ I couldn't find any general news articles right now. ]",
    "news_report_title_topic": "[ News on '{topic}': ]",
    "news_report_title_general": "[ Latest News: ]",
    "news_unexpected_error": "[ An unexpected error occurred while fetching news. ]",
    "news_read_more": "[ Read: {url} ]",
    "replied_message_default_question": "[ Analyze this text, identify the main idea, and share your opinion. ]",
    "llm_no_assistants": "[ Sorry, none of my AI assistants are available right now. ]",
    "analysis_error": "[ Sorry, an error occurred during the text analysis. ]",
    "analysis_client_not_configured": "[ The analysis function cannot be performed as the AI client is not configured. ]",
    "analysis_unavailable": "[ Analysis unavailable. ]",
    "conversation_cancelled": "[ Okay, the current operation has been cancelled. ]",
    "llm_quota_exceeded": "[ You've reached your daily limit for AI responses. Please try again tomorrow. ]",
    "image_quota_exceeded": "[ You've reached your daily image generation limit. Please try again tomorrow. ]",
    "image_generation_start": "[ On it! Imagining something for you... ]",
    "image_generation_error": "[ Sorry, I hit a snag and couldn't create your image. Please try again. ]",
    "image_generation_no_prompt": "[ I see you want an image, but I'm not sure what you want me to create. Could you please describe it? ]",
    "image_generation_success_single": "[ Here is the image you requested for: '{image_prompt}' ]",
    "image_generation_success_multiple": "[ Here are the images you requested for: '{image_prompt}' ]",
    "voice_message_received": "[ Received your voice message. Processing... ]",
    "voice_transcription_failed": "[ Sorry, I couldn't understand the audio in that message. ]",
    "voice_transcription_header": "[ Transcription: ]",
    "voice_translation_header": "[ Translation (Russian): ]",
    "video_message_received": "[ Received your video note. Processing... ]",
    "video_transcription_failed": "[ Sorry, I couldn't understand the audio in that video. ]",
    "video_transcription_header": "[ Video transcription: ]",
    "video_translation_header": "[ Video translation (Russian): ]",
    "hint_check_quote": "[ Check quote? ]",
    "hint_check_news": "[ Check as news? ]",
    "button_show_sources": "[ Sources*** ]",
    "button_more_matches": "[ More matches ]",
    "button_narrow_7_days": "[ Narrow to 7 days ]",
    "button_refresh_portrait": "[ Refresh portrait ]",
    "reaction_regenerating": "[ Regenerating response... ]",
    "reaction_expanding": "[ Expanding on previous response... ]",
    "reaction_summary": "[ Generating summary... ]",
    "community_report_reply_prompt": "[ Reply to a message with /report to report it. ]",
    "community_report_choose_reason": "[ Choose a reason to report: ]",
    "report_reason_spam": "[ Spam/Ads** ]",
    "report_reason_scam": "[ Scam/Phishing ]",
    "report_reason_nsfw": "[ NSFW****** ]",
    "report_reason_hate": "[ Hate/Harassment ]",
    "report_reason_offtopic": "[ Off-topic* ]",
    "report_reason_other": "[ Other***** ]",
    "community_report_target_missing": "[ Could not identify target user. ]",
    "community_report_already": "[ You already reported this case. ]",
    "community_report_thanks": "[ Thanks, the moderators have been notified. ]",
    "community_vote_reply_prompt": "[ Reply to the offending message with /spam. ]",
    "community_vote_self_vote": "[ You cannot vote on yourself. ]",
    "community_vote_already": "[ You already voted on this case. ]",
    "community_vote_recorded": "[ Vote recorded. Mods notified. ]",
    "community_vote_need_more_time": "[ You need more time in this chat before voting. ]",
    "community_vote_daily_limit": "[ Daily vote limit reached. ]",
    "admins_only": "[ 🔒 Admins only. ]",
    "nsfw_image_removed_user": "[ 🚫 Removed an NSFW image from {user}. ]",
    "nsfw_image_removed_chat": "[ 🚫 Removed an NSFW image. ]",
    "toggle_nsfw_usage": "[ Usage: /toggle_nsfw <on|off> ]",
    "status_enabled": "[ enabled*** ]",
    "status_disabled": "[ disabled** ]",
    "nsfw_filter_status": "[ NSFW filter {status} for this chat. ]",
    "nsfw_threshold_usage": "[ Usage: /nsfw_threshold <0.0-1.0> ]",
    "nsfw_threshold_must_number": "[ Threshold must be a number between 0 and 1. ]",
    "nsfw_threshold_set": "[ NSFW threshold set to {threshold:.2f} for this chat. ]",
    "set_spam_threshold_usage": "[ Usage: /setspamthreshold <number> ]",
    "spam_threshold_set": "[ Spam vote threshold set to {threshold}. ]",
    "karma_self_vote_error": "[ 🤷 You can't vote on yourself. ]",
    "karma_vote_cooldown": "[ ⏱ You can vote for this user again later. ]",
    "karma_vote_success": "[ {sign}{delta} karma (total {total:+}) ]"
  },
  "weather_conditions_map": {
    "clear_sky": "[ Clear sky* ]",
    "few_clouds": "[ Few clouds ]",
    "scattered_clouds": "[ Scattered clouds ]",
    "broken_clouds": "[ Broken clouds ]",
    "overcast_clouds": "[ Overcast clouds ]",
    "shower_rain": "[ Shower rain ]",
    "light_intensity_shower_rain": "[ Light intensity shower rain ]",
    "rain": "[ Rain****** ]",
    "light_rain": "[ Light rain ]",
    "moderate_rain": "[ Moderate rain ]",
    "heavy_intensity_rain": "[ Heavy intensity rain ]",
    "thunderstorm": "[ Thunderstorm ]",
    "snow": "[ Snow****** ]",
    "light_snow": "[ Light snow ]",
    "mist": "[ Mist****** ]",
    "fog": "[ Fog******* ]",
    "smoke": "[ Smoke***** ]",
    "haze": "[ Haze****** ]",
    "sand_dust_whirls": "[ Sand/Dust Whirls ]",
    "squalls": "[ Squalls*** ]",
    "tornado": "[ Tornado*** ]",
    "unknown_condition": "[ Condition unknown ]"
  },
  "days_of_week": {
    "Monday": "[ Monday**** ]",
    "Tuesday": "[ Tuesday*** ]",
    "Wednesday": "[ Wednesday* ]",
    "Thursday": "[ Thursday** ]",
    "Friday": "[ Friday**** ]",
    "Saturday": "[ Saturday** ]",
    "Sunday": "[ Sunday**** ]"
  },
  "factconfig_tab_preset": "[ Preset**** ]",
  "factconfig_tab_policy": "[ Policy**** ]",
  "factconfig_tab_limits": "[ Limits**** ]",
  "factconfig_tab_auto": "[ Auto****** ]",
  "factconfig_tab_danger": "[ Danger**** ]",
  "factconfig_export_btn": "[ Export**** ]",
  "factconfig_apply_btn": "[ Apply***** ]",
  "factconfig_title": "[ Fact check config: ]",
  "factconfig_update_ok": "[ Config updated. ]",
  "factconfig_checking": "[ Checking…* ]",
  "factconfig_nothing_to_check": "[ Nothing to check. ]",
  "factconfig_export_ok": "[ Configuration exported. ]",
  "factconfig_export_fail": "[ Failed to export configuration. ]",
  "factconfig_apply_ok": "[ Configuration applied. ]",
  "factconfig_apply_fail": "[ Failed to apply configuration. ]"
}


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/local_telegram_bot.py ---
======================================================================

# enkibot/local_telegram_bot.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Minimal Telegram wiring for the local two‑tier model setup.

This script bypasses the project's main application stack to present a compact
example of running EnkiBot purely with local models.  It exposes three
commands:

``/fast`` – use the 7–8B model directly.
``/deep`` – force the 70B/72B model.
``/web``  – perform a web search via the OpenAI responses API and
            summarise the top pages with citations using the fast model.

Any other message goes through :class:`~enkibot.modules.model_router.ModelRouter`
which escalates to the deep model when needed.
"""
from __future__ import annotations

import logging
import os

from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters

from .modules.local_model_manager import LocalModelManager, ModelConfig
from .modules.model_router import ModelRouter
from .modules.web_tool import web_research

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

FAST_MODEL_PATH = os.getenv("ENKIBOT_FAST_MODEL", "mistral-7b-instruct.Q5_K_M.gguf")
DEEP_MODEL_PATH = os.getenv("ENKIBOT_DEEP_MODEL", "llama-3-70b-instruct.Q4_K_M.gguf")

manager = LocalModelManager(
    ModelConfig(FAST_MODEL_PATH, n_ctx=4096, n_threads=16),
    ModelConfig(DEEP_MODEL_PATH, n_ctx=8192, n_threads=16),
)
router = ModelRouter(manager)


async def fast_cmd(update, context):  # /fast
    prompt = " ".join(context.args)
    await update.message.reply_text(manager.generate(prompt, model="fast"))


async def deep_cmd(update, context):  # /deep
    prompt = " ".join(context.args)
    await update.message.reply_text(manager.generate(prompt, model="deep"))


async def web_cmd(update, context):  # /web query
    query = " ".join(context.args)
    docs = web_research(query, k=3)
    context_block = "\n\n".join(
        f"[{i+1}] {d['title']}\n{d['text'][:500]}" for i, d in enumerate(docs)
    )
    prompt = (
        f"Use the following web results to answer the question. Cite sources "
        f"as [number](url).\n\n{context_block}\n\nQuestion: {query}"
    )
    answer = manager.generate(prompt, model="fast")
    await update.message.reply_text(answer)


async def default_handler(update, context):
    response = router.generate(update.message.text)
    await update.message.reply_text(response)


def main() -> None:
    token = os.environ["TELEGRAM_BOT_TOKEN"]
    app = ApplicationBuilder().token(token).build()
    app.add_handler(CommandHandler("fast", fast_cmd))
    app.add_handler(CommandHandler("deep", deep_cmd))
    app.add_handler(CommandHandler("web", web_cmd))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, default_handler))
    logger.info("Starting local EnkiBot")
    app.run_polling()


if __name__ == "__main__":
    main()



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/main.py ---
======================================================================

# enkibot/main.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import os
import logging
from typing import Optional

try:
    from telegram import Update
    from telegram.ext import Application
    from telegram.request import HTTPXRequest
except ImportError as exc:
    raise ImportError(
        "EnkiBot requires the 'python-telegram-bot' package. Install it with "
        "`pip install python-telegram-bot>=20.0` and make sure any conflicting "
        "`telegram` package is uninstalled."
    ) from exc

from enkibot import config
from enkibot.utils.logging_config import setup_logging
from enkibot.utils.database import initialize_database
# --- MODIFIED IMPORT ---
from enkibot.app import EnkiBotApplication 
# --- END MODIFICATION ---

logger: Optional[logging.Logger] = None 

def clear_terminal():
    os.system('cls' if os.name == 'nt' else 'clear')

# Commented out backfill for now, as it needs careful setup if run from here
# async def run_backfill_async(): ...

def main() -> None:
    global logger 
    clear_terminal()
    setup_logging() 
    logger = logging.getLogger(__name__) 

    logger.info("Initializing database schema...")
    initialize_database()

    if not config.TELEGRAM_BOT_TOKEN:
        logger.critical("FATAL: TELEGRAM_BOT_TOKEN missing. Bot cannot start.")
        return

    try:
        logger.info("Initializing Telegram PTB Application...")
        request = HTTPXRequest(
            connect_timeout=config.TELEGRAM_CONNECT_TIMEOUT,
            read_timeout=config.TELEGRAM_READ_TIMEOUT,
            write_timeout=config.TELEGRAM_WRITE_TIMEOUT,
            pool_timeout=config.TELEGRAM_POOL_TIMEOUT,
        )

        # Placeholder for the bot application instance so it can be referenced
        # inside the post-init callback.
        enkibot_app_instance: EnkiBotApplication | None = None

        async def post_init(application: Application) -> None:
            """Publish default commands once the application is running."""
            if enkibot_app_instance:
                await enkibot_app_instance.handler_service.push_default_commands()

        ptb_app = (
            Application.builder()
            .token(config.TELEGRAM_BOT_TOKEN)
            .request(request)
            .build()
        )
        # Register the post_init callback without calling it (avoids NoneType errors)
        ptb_app.post_init = post_init

        logger.info("Initializing EnkiBotApplication...")
        # --- MODIFIED BOT INSTANTIATION ---
        enkibot_app_instance = EnkiBotApplication(ptb_app)
        enkibot_app_instance.register_handlers()  # Call the method to register handlers
        # --- END MODIFICATION ---

        logger.info("Starting EnkiBot polling...")
        # The run method is now part of EnkiBotApplication, or keep polling here
        # For simplicity, keeping polling here:
        ptb_app.run_polling(allowed_updates=Update.ALL_TYPES)
        # Alternatively, if you add a run() method to EnkiBotApplication:
        # enkibot_app_instance.run()
        
        logger.info("EnkiBot has stopped.")
    except Exception as e:
        logger.critical(f"Unrecoverable error during bot setup or run: {e}", exc_info=True)

if __name__ == '__main__':
    main()



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/__init__.py ---
======================================================================

# enkibot/modules/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file intentionally left blank.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/admin_tools.py ---
======================================================================

# enkibot/modules/admin_tools.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
"""Administrative moderation helpers for Telegram.

This module provides a *best in class* reference implementation of
administrative tools for Telegram chats.  It combines progressive
discipline (warn → mute → temp-ban → permaban), extensive auditing and
reversible actions.  The implementation is intentionally self contained
so it can be plugged into :class:`~telegram.ext.Application` directly or
integrated into the :class:`~enkibot.app.EnkiBotApplication` pipeline.

Only a subset of the full system described in the project specification
is implemented.  The code is structured so additional features like
crowd‑moderation, trust scores and external policy engines can be added
without major refactoring.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Dict, Optional
import logging

from sqlalchemy import (
    BigInteger,
    Column,
    DateTime,
    Integer,
    String,
    JSON,
    create_engine,
)
from sqlalchemy.orm import declarative_base, sessionmaker
from sqlalchemy.exc import SQLAlchemyError

from telegram import (
    InlineKeyboardButton,
    InlineKeyboardMarkup,
    ChatPermissions,
    Update,
)
from telegram.constants import ParseMode
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
    CallbackQueryHandler,
)

from enkibot.core.language_service import LanguageService

# Configure module level logger
logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Database setup
# ---------------------------------------------------------------------------

Base = declarative_base()


class ModerationAction(Base):
    __tablename__ = "moderation_actions"
    action_id = Column(Integer, primary_key=True, autoincrement=True)
    chat_id = Column(BigInteger, index=True)
    target_user_id = Column(BigInteger, index=True)
    action = Column(String(32))
    reason = Column(String(64))
    params_json = Column(JSON, default=dict)
    until_ts = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)


class Warning(Base):
    __tablename__ = "warnings"
    warn_id = Column(Integer, primary_key=True, autoincrement=True)
    chat_id = Column(BigInteger, index=True)
    user_id = Column(BigInteger, index=True)
    reason = Column(String(64))
    created_at = Column(DateTime, default=datetime.utcnow)


class ModeratorNote(Base):
    __tablename__ = "moderator_notes"
    note_id = Column(Integer, primary_key=True, autoincrement=True)
    chat_id = Column(BigInteger, index=True)
    user_id = Column(BigInteger, index=True)
    note_text = Column(String(1024))
    created_at = Column(DateTime, default=datetime.utcnow)


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def parse_duration(duration: str) -> Optional[int]:
    """Parse a short duration spec like ``10m`` or ``1h``.

    Returns the number of seconds or ``None`` for invalid input.
    """
    if not duration:
        return None
    try:
        value = int(duration[:-1])
    except (ValueError, TypeError):
        return None
    unit = duration[-1].lower()
    if unit == "m":
        return value * 60
    if unit == "h":
        return value * 3600
    if unit == "d":
        return value * 86400
    return None


PERM_READ_ONLY = ChatPermissions(can_send_messages=False)
FULL_PERMS = ChatPermissions(
    can_send_messages=True,
    can_send_media_messages=True,
    can_send_polls=True,
    can_add_web_page_previews=True,
)

# ---------------------------------------------------------------------------
# Administrative service
# ---------------------------------------------------------------------------


@dataclass
class AdminTools:
    """Container for moderation logic.

    Parameters
    ----------
    app:
        PTB :class:`~telegram.ext.Application` to which handlers will be
        attached.
    engine_url:
        SQLAlchemy connection string.  A tiny SQLite database is used by
        default which is sufficient for tests and local runs.
    """

    app: Application
    language_service: LanguageService
    engine_url: str = "sqlite:///admin_tools.sqlite3"

    def __post_init__(self) -> None:
        try:
            self.engine = create_engine(self.engine_url, future=True)
            Base.metadata.create_all(self.engine)
        except SQLAlchemyError:
            logger.error(
                "Database initialization failed. Ensure required tables and columns exist.",
                exc_info=True,
            )
            raise
        self.Session = sessionmaker(self.engine, expire_on_commit=False)
        self._register_handlers()

    # ------------------------------------------------------------------
    # Handler registration
    # ------------------------------------------------------------------
    def _register_handlers(self) -> None:
        self.app.add_handler(CommandHandler("ban", self.cmd_ban))
        self.app.add_handler(CommandHandler("unban", self.cmd_unban))
        self.app.add_handler(CommandHandler("mute", self.cmd_mute))
        self.app.add_handler(CommandHandler("unmute", self.cmd_unmute))
        self.app.add_handler(CommandHandler("warn", self.cmd_warn))
        self.app.add_handler(CommandHandler("warns_list", self.cmd_warns))
        self.app.add_handler(CommandHandler("rm_warn", self.cmd_rm_warn))
        self.app.add_handler(CommandHandler("note", self.cmd_note))
        self.app.add_handler(CommandHandler("shadowdel", self.cmd_shadowdel))
        self.app.add_handler(CallbackQueryHandler(self.on_confirm, pattern=r"^mod:"))

    # ------------------------------------------------------------------
    # Helper methods
    # ------------------------------------------------------------------
    async def _is_admin(self, update: Update) -> bool:
        chat_id = update.effective_chat.id
        user_id = update.effective_user.id
        member = await self.app.bot.get_chat_member(chat_id, user_id)
        return member.status in {"creator", "administrator"}

    async def _ensure_admin(self, update: Update) -> bool:
        if await self._is_admin(update):
            return True
        await update.effective_message.reply_text(
            self.language_service.get_response_string("admins_only")
        )
        return False

    def _log_action(
        self,
        chat_id: int,
        target_id: int,
        action: str,
        reason: str | None = None,
        until: datetime | None = None,
    ) -> None:
        try:
            with self.Session() as s:
                s.add(
                    ModerationAction(
                        chat_id=chat_id,
                        target_user_id=target_id,
                        action=action,
                        reason=reason or "",
                        until_ts=until,
                    )
                )
                s.commit()
        except SQLAlchemyError:
            logger.error(
                "Failed to log moderation action '%s' for user %s in chat %s",
                action,
                target_id,
                chat_id,
                exc_info=True,
            )

    # ------------------------------------------------------------------
    # Command implementations
    # ------------------------------------------------------------------
    async def cmd_ban(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not await self._ensure_admin(update):
            return
        if not update.message or not update.message.reply_to_message:
            await update.effective_message.reply_text(
                "Reply to a user with /ban [reason]."
            )
            return
        reason = " ".join(context.args) if context.args else ""
        target = update.message.reply_to_message.from_user
        kb = InlineKeyboardMarkup(
            [
                [
                    InlineKeyboardButton(
                        "Confirm ban", callback_data=f"mod:ban:{target.id}:{reason}"
                    )
                ]
            ]
        )
        await update.effective_message.reply_text(
            "Confirm permanent ban?", reply_markup=kb
        )

    async def cmd_unban(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        target_id = None
        if update.message.reply_to_message:
            target_id = update.message.reply_to_message.from_user.id
        elif context.args:
            try:
                target_id = int(context.args[0])
            except ValueError:
                pass
        if not target_id:
            await update.effective_message.reply_text("Reply or pass user id to /unban")
            return
        await context.bot.unban_chat_member(update.effective_chat.id, target_id)
        self._log_action(update.effective_chat.id, target_id, "unban")
        await update.effective_message.reply_text("User unbanned.")

    async def cmd_mute(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        if (
            not update.message
            or not update.message.reply_to_message
            or not context.args
        ):
            await update.effective_message.reply_text("Usage: /mute <10m|1h> [reason]")
            return
        seconds = parse_duration(context.args[0])
        if not seconds:
            await update.effective_message.reply_text("Invalid duration.")
            return
        reason = " ".join(context.args[1:]) if len(context.args) > 1 else ""
        target = update.message.reply_to_message.from_user
        kb = InlineKeyboardMarkup(
            [
                [
                    InlineKeyboardButton(
                        "Confirm mute",
                        callback_data=f"mod:mute:{target.id}:{seconds}:{reason}",
                    )
                ]
            ]
        )
        await update.effective_message.reply_text("Confirm mute?", reply_markup=kb)

    async def cmd_unmute(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        if not update.message or not update.message.reply_to_message:
            await update.effective_message.reply_text("Reply to a user with /unmute.")
            return
        target = update.message.reply_to_message.from_user
        await context.bot.restrict_chat_member(
            update.effective_chat.id, target.id, permissions=FULL_PERMS
        )
        self._log_action(update.effective_chat.id, target.id, "unmute")
        await update.effective_message.reply_text("User unmuted.")

    async def cmd_warn(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        if not update.message or not update.message.reply_to_message:
            await update.effective_message.reply_text(
                "Reply to a user with /warn [reason]."
            )
            return
        target = update.message.reply_to_message.from_user
        reason = " ".join(context.args) if context.args else ""
        try:
            with self.Session() as s:
                s.add(
                    Warning(
                        chat_id=update.effective_chat.id,
                        user_id=target.id,
                        reason=reason,
                    )
                )
                s.commit()
                count = (
                    s.query(Warning)
                    .filter(
                        Warning.chat_id == update.effective_chat.id,
                        Warning.user_id == target.id,
                    )
                    .count()
                )
        except SQLAlchemyError:
            logger.error(
                "Failed to store warning for user %s in chat %s",
                target.id,
                update.effective_chat.id,
                exc_info=True,
            )
            await update.effective_message.reply_text(
                "Database error while storing warning."
            )
            return
        await update.effective_message.reply_html(
            f"⚠️ {target.mention_html()} warned (#{count}). Reason: {reason or '—'}"
        )

    async def cmd_warns(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        target = None
        if update.message.reply_to_message:
            target = update.message.reply_to_message.from_user
        elif context.args:
            try:
                target = await context.bot.get_chat_member(
                    update.effective_chat.id, int(context.args[0])
                )
                target = target.user
            except Exception:
                target = None
        if not target:
            await update.effective_message.reply_text(
                "Usage: /warns_list (reply or user_id)"
            )
            return
        try:
            with self.Session() as s:
                warns = (
                    s.query(Warning)
                    .filter(
                        Warning.chat_id == update.effective_chat.id,
                        Warning.user_id == target.id,
                    )
                    .order_by(Warning.created_at.desc())
                    .all()
                )
        except SQLAlchemyError:
            logger.error(
                "Failed to fetch warnings for user %s in chat %s",
                target.id,
                update.effective_chat.id,
                exc_info=True,
            )
            await update.effective_message.reply_text(
                "Database error while fetching warnings."
            )
            return
        if not warns:
            await update.effective_message.reply_text("No warnings.")
            return
        lines = [f"{w.created_at:%Y-%m-%d} • {w.reason or '-'}" for w in warns[:20]]
        await update.effective_message.reply_text("\n".join(lines))

    async def cmd_rm_warn(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        target = None
        if update.message.reply_to_message:
            target = update.message.reply_to_message.from_user
        elif context.args:
            try:
                target = await context.bot.get_chat_member(
                    update.effective_chat.id, int(context.args[0])
                )
                target = target.user
            except Exception:
                target = None
        if not target:
            await update.effective_message.reply_text(
                "Usage: /rm_warn (reply or user_id) [count|all]"
            )
            return
        count_arg = context.args[1] if len(context.args) > 1 else "1"
        try:
            with self.Session() as s:
                q = s.query(Warning).filter(
                    Warning.chat_id == update.effective_chat.id,
                    Warning.user_id == target.id,
                )
                if count_arg == "all":
                    removed = q.count()
                    q.delete()
                else:
                    try:
                        n = int(count_arg)
                    except ValueError:
                        n = 1
                    warns = q.order_by(Warning.created_at.desc()).limit(n).all()
                    removed = len(warns)
                    for w in warns:
                        s.delete(w)
                s.commit()
        except SQLAlchemyError:
            logger.error(
                "Failed to remove warnings for user %s in chat %s",
                target.id,
                update.effective_chat.id,
                exc_info=True,
            )
            await update.effective_message.reply_text(
                "Database error while removing warnings."
            )
            return
        await update.effective_message.reply_text(f"Removed {removed} warning(s).")

    async def cmd_note(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        if (
            not update.message
            or not update.message.reply_to_message
            or not context.args
        ):
            await update.effective_message.reply_text(
                "Reply to a user with /note <text>."
            )
            return
        target = update.message.reply_to_message.from_user
        text = " ".join(context.args)
        try:
            with self.Session() as s:
                s.add(
                    ModeratorNote(
                        chat_id=update.effective_chat.id,
                        user_id=target.id,
                        note_text=text,
                    )
                )
                s.commit()
        except SQLAlchemyError:
            logger.error(
                "Failed to store note for user %s in chat %s",
                target.id,
                update.effective_chat.id,
                exc_info=True,
            )
            await update.effective_message.reply_text(
                "Database error while storing note."
            )
            return
        await update.effective_message.reply_text("Note stored (private).")

    async def cmd_shadowdel(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        if not await self._ensure_admin(update):
            return
        if not update.message or not update.message.reply_to_message:
            await update.effective_message.reply_text(
                "Reply to a message with /shadowdel."
            )
            return
        try:
            await update.message.reply_to_message.delete()
        except Exception:
            pass
        self._log_action(
            update.effective_chat.id,
            update.message.reply_to_message.from_user.id,
            "delete",
        )

    # ------------------------------------------------------------------
    # Confirmation callbacks
    # ------------------------------------------------------------------
    async def on_confirm(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        q = update.callback_query
        await q.answer()
        try:
            _, action, user_id, *rest = q.data.split(":", 3)
        except ValueError:
            await q.edit_message_text("Malformed callback.")
            return
        chat_id = update.effective_chat.id
        user_id_int = int(user_id)
        if action == "ban":
            reason = rest[0] if rest else ""
            await context.bot.ban_chat_member(chat_id, user_id_int)
            self._log_action(chat_id, user_id_int, "ban", reason)
            await q.edit_message_text("User banned.")
        elif action == "mute":
            seconds = int(rest[0]) if rest else 0
            reason = rest[1] if len(rest) > 1 else ""
            until = datetime.now(timezone.utc) + timedelta(seconds=seconds)
            await context.bot.restrict_chat_member(
                chat_id,
                user_id_int,
                permissions=PERM_READ_ONLY,
                until_date=int(until.timestamp()),
            )
            self._log_action(chat_id, user_id_int, "mute", reason, until)
            await q.edit_message_text("User muted.")
        else:
            await q.edit_message_text("Unknown action.")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/api_router.py ---
======================================================================

﻿# enkibot/modules/api_router.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot API Router ===
# ==================================================================================================

# (GPLv3 Header as in your files)
import logging
import httpx
import time
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List

from enkibot import config # For API Keys
from enkibot.utils.database import DatabaseManager
from enkibot.utils.provider_metrics import ProviderMetrics

logger = logging.getLogger(__name__)

class ApiRouter:
    def __init__(
        self,
        weather_api_key: str | None,
        news_api_key: str | None,
        llm_services: Any = None,
        db_manager: Optional[DatabaseManager] = None,
    ):
        self.weather_api_key = weather_api_key
        self.news_api_key = news_api_key
        self.db_manager = db_manager
        # self.llm_services = llm_services # Not used directly in this version of ApiRouter

        # Persistent HTTP client and metrics containers
        self.http_client = httpx.AsyncClient()
        self.metrics: Dict[str, ProviderMetrics] = {
            "Weather": ProviderMetrics(),
            "News": ProviderMetrics(),
        }

        self.lang_to_country_map = {
            "en": "us", "ru": "ru", "de": "de", "fr": "fr", "es": "es",
            "it": "it", "ja": "jp", "ko": "kr", "zh": "cn", "bg": "bg",
            "ua": "ua", "pl": "pl", "tr": "tr", "pt": "pt",
        }
        self.default_news_country = "us"

    def _record_metrics(self, provider: str, latency: float) -> None:
        self.metrics.setdefault(provider, ProviderMetrics()).record(latency)

    def _get_localized_response_string_from_pack(self, lang_pack_full: Optional[Dict[str, Any]], key: str, default_value: str, **kwargs) -> str:
        """ Helper to get response strings directly from a full language pack. """
        if lang_pack_full and "responses" in lang_pack_full:
            raw_string = lang_pack_full["responses"].get(key, default_value)
        else: # Fallback if pack or responses section is missing
            raw_string = default_value
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError: return default_value


    async def get_weather_data_structured(self, location: str, lang_pack_full: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """
        Fetches 5-day weather forecast data and returns it in a structured format
        suitable for LLM processing. Includes city name and a list of daily forecasts.
        """
        if not self.weather_api_key:
            logger.warning("Weather API key is not configured.")
            return None

        api_lang = "en" # Default to English for weather data city name, LLM can localize description
        if lang_pack_full and "responses" in lang_pack_full:
            api_lang = lang_pack_full["responses"].get("api_lang_code_openweathermap", "en")

        # OpenWeatherMap 5 day / 3 hour forecast endpoint
        url = "https://api.openweathermap.org/data/2.5/forecast"
        # Requesting enough data points for 5 days (8 records per day * 5 days = 40)
        params = {"q": location, "appid": self.weather_api_key, "units": "metric", "lang": api_lang, "cnt": 40}

        try:
            start = time.perf_counter()
            response = await self.http_client.get(url, params=params)
            latency = time.perf_counter() - start
            response.raise_for_status()
            self._record_metrics("Weather", latency)
            if self.db_manager:
                await self.db_manager.log_web_request(url, "GET", response.status_code, int(latency * 1000), None)
            data = response.json()

            city_name = data.get("city", {}).get("name", location)
            forecast_items = data.get("list", [])

            if not forecast_items:
                logger.warning(f"No forecast items received for {location}")
                return None

            daily_forecasts_processed = []
            temp_daily_data = {} # date_str -> {'temps': [], 'conditions': set(), 'icons': set()}

            for item in forecast_items:
                dt_object = datetime.fromtimestamp(item["dt"])
                date_str = dt_object.strftime("%Y-%m-%d")
                
                if date_str not in temp_daily_data:
                    temp_daily_data[date_str] = {'temps': [], 'conditions': set(), 'icons': set()}
                
                temp_daily_data[date_str]['temps'].append(item["main"]["temp"])
                if item.get("weather") and item["weather"][0]:
                    temp_daily_data[date_str]['conditions'].add(item["weather"][0].get("description", "N/A"))
                    temp_daily_data[date_str]['icons'].add(item["weather"][0].get("icon", "N/A"))

            for date_str, daily_data in sorted(temp_daily_data.items()):
                if not daily_data['temps']: continue
                
                # For simplicity, take the condition that appears most or first unique for the day.
                # LLM can make sense of multiple conditions if provided as a list.
                day_condition = list(daily_data['conditions'])[0] if daily_data['conditions'] else "N/A"
                
                daily_forecasts_processed.append({
                    "date": date_str,
                    "day_name": datetime.strptime(date_str, "%Y-%m-%d").strftime('%A'), # English day name
                    "temp_min": min(daily_data['temps']),
                    "temp_max": max(daily_data['temps']),
                    "avg_temp": sum(daily_data['temps']) / len(daily_data['temps']),
                    "condition_descriptions": list(daily_data['conditions']), # Provide all conditions
                    "primary_condition": day_condition # LLM can choose or summarize
                })
            
            if not daily_forecasts_processed:
                 logger.warning(f"Could not process daily forecast data for {location}")
                 return None

            return {
                "location": city_name,
                "forecast_days": daily_forecasts_processed[:7] # Return up to 7 days of processed data
            }

        except httpx.HTTPStatusError as e:
            latency = time.perf_counter() - start
            if self.db_manager:
                await self.db_manager.log_web_request(url, "GET", e.response.status_code, int(latency * 1000), str(e))
            logger.error(f"HTTP error fetching weather forecast for {location}: {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e:
            latency = time.perf_counter() - start
            if self.db_manager:
                await self.db_manager.log_web_request(url, "GET", None, int(latency * 1000), str(e))
            logger.error(f"Unexpected error fetching structured weather data for {location}: {e}", exc_info=True)
        return None

    async def get_latest_news_structured(self, query: Optional[str] = None, lang_code: str = "en", num: int = 5) -> Optional[List[Dict[str, Any]]]:
        """
        Fetches latest news and returns a list of article dictionaries.
        """
        if not self.news_api_key:
            logger.warning("News API key is not configured.")
            return None

        params: Dict[str, Any] = {"apiKey": self.news_api_key, "pageSize": num}
        base_url = "https://newsapi.org/v2/"
        endpoint: str

        if query:
            logger.info(f"Fetching news for query: '{query}', language: {lang_code}")
            endpoint = "everything"
            params.update({"q": query, "language": lang_code, "sortBy": "relevancy"})
        else:
            country = self.lang_to_country_map.get(lang_code, self.default_news_country)
            logger.info(f"Fetching top headlines for country: '{country}' (derived from lang: {lang_code})")
            endpoint = "top-headlines"
            params.update({"country": country, "category": "general"})
        
        url = base_url + endpoint
        try:
            start = time.perf_counter()
            resp = await self.http_client.get(url, params=params)
            latency = time.perf_counter() - start
            logger.debug(f"NewsAPI request URL: {resp.url}")
            resp.raise_for_status()
            self._record_metrics("News", latency)
            if self.db_manager:
                await self.db_manager.log_web_request(str(resp.url), "GET", resp.status_code, int(latency * 1000), None)
            data = resp.json()
            articles_raw = data.get("articles", [])
            
            logger.info(f"NewsAPI returned {len(articles_raw)} articles (totalResults: {data.get('totalResults')}) for params: {params}")

            if not articles_raw:
                return [] # Return empty list if no articles

            # Process articles into a cleaner structure for the LLM
            processed_articles = []
            for article in articles_raw:
                processed_articles.append({
                    "title": article.get("title"),
                    "source": article.get("source", {}).get("name"),
                    "description": article.get("description"), # Short description or snippet
                    "url": article.get("url"),
                    "published_at": article.get("publishedAt")
                })
            return processed_articles
            
        except httpx.HTTPStatusError as e:
            latency = time.perf_counter() - start
            if self.db_manager:
                await self.db_manager.log_web_request(url, "GET", e.response.status_code, int(latency * 1000), str(e))
            logger.error(f"HTTP error fetching news ({url}): {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e:
            latency = time.perf_counter() - start
            if self.db_manager:
                await self.db_manager.log_web_request(url, "GET", None, int(latency * 1000), str(e))
            logger.error(f"Unexpected error fetching structured news: {e}", exc_info=True)
        return None

    async def aclose(self) -> None:
        await self.http_client.aclose()



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/base_module.py ---
======================================================================

# enkibot/modules/base_module.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.'
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Base Module ===
# ==================================================================================================
# This file is intended to hold an Abstract Base Class (ABC) for all functional modules.
# In a future evolution, all modules (e.g., IntentRecognizer, FactExtractor) would inherit
# from this class to ensure a consistent interface, for example, requiring an `execute` method.
# For now, it serves as a structural placeholder.
# ==================================================================================================

class BaseModule:
    """
    Abstract Base Class for all EnkiBot modules.
    """
    def __init__(self, name: str):
        self.name = name

    def execute(self, *args, **kwargs):
        """
        The main method to be implemented by all subclasses.
        """
        raise NotImplementedError("Each module must implement the 'execute' method.")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/community_moderation.py ---
======================================================================

# enkibot/modules/community_moderation.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
"""Community moderation helpers.

This module implements a lightweight, in-memory community moderation
system inspired by more advanced crowd‑moderation examples.  It supports
reporting with a reason picker, weighted voting with basic trust scores
and automatic actions once dynamic thresholds are met.  The state is not
persisted between restarts but provides a foundation that can later be
extended to a database-backed implementation.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, Tuple, Optional

from telegram import (
    Update,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
)
from telegram.ext import ContextTypes

from enkibot.core.language_service import LanguageService


# ---------------------------------------------------------------------------
# Constants and simple helpers
# ---------------------------------------------------------------------------

CONSENSUS_WINDOW_MIN = 15
TRUST_MIN, TRUST_MAX = 0.2, 1.5
VOTE_LIMIT_PER_DAY = 10
MIN_CHAT_TENURE_HOURS = 24

THRESH_HIDE = 2.0
THRESH_TEMPBAN = 5.0
THRESH_PERMABAN = 7.0

TEMPBAN_HOURS = 24

REASON_CODES = ["spam", "scam", "nsfw", "hate", "offtopic", "other"]


def now() -> datetime:
    return datetime.utcnow()


@dataclass
class Case:
    case_id: int
    chat_id: int
    target_user_id: int
    first_msg_id: int
    opened_ts: datetime
    reason: str
    votes: Dict[int, float] = field(default_factory=dict)
    status: str = "open"


class CommunityModerationService:
    """Minimal community moderation manager."""

    def __init__(self, language_service: LanguageService, admin_chat_id: Optional[int] = None) -> None:
        self.language_service = language_service
        self.admin_chat_id = admin_chat_id
        self.cases: Dict[int, Case] = {}
        self.case_index: Dict[Tuple[int, int, int], int] = {}
        self.next_case_id = 1
        self.trust: Dict[Tuple[int, int], float] = {}
        self.mem_state: Dict[Tuple[int, int], Dict[str, any]] = {}

    # ------------------------------------------------------------------
    # Case and trust helpers
    # ------------------------------------------------------------------
    def _get_or_open_case(self, chat_id: int, target_user_id: int, msg_id: int, reason: str) -> Case:
        key = (chat_id, target_user_id, msg_id)
        cid = self.case_index.get(key)
        if cid:
            return self.cases[cid]
        case = Case(self.next_case_id, chat_id, target_user_id, msg_id, now(), reason)
        self.cases[self.next_case_id] = case
        self.case_index[key] = self.next_case_id
        self.next_case_id += 1
        return case

    def _already_voted(self, case: Case, voter_id: int) -> bool:
        return voter_id in case.votes

    def _add_vote(self, case: Case, voter_id: int, base: float = 1.0) -> float:
        trust = self.trust.get((case.chat_id, voter_id), 1.0)
        trust = max(TRUST_MIN, min(TRUST_MAX, trust))
        weight = base * trust
        case.votes[voter_id] = weight
        return weight

    def _effective_score(self, case: Case) -> float:
        window_start = now() - timedelta(minutes=CONSENSUS_WINDOW_MIN)
        return sum(weight for weight in case.votes.values() if window_start <= case.opened_ts)

    # ------------------------------------------------------------------
    # Eligibility tracking
    # ------------------------------------------------------------------
    def _eligible_to_vote(self, update: Update) -> Tuple[bool, str]:
        chat = update.effective_chat
        user = update.effective_user
        key = (chat.id, user.id)
        st = self.mem_state.get(key, {})

        joined_ts = st.get("joined_ts")
        if not joined_ts:
            joined_ts = now()
            st["joined_ts"] = joined_ts
        if now() - joined_ts < timedelta(hours=MIN_CHAT_TENURE_HOURS):
            self.mem_state[key] = st
            return False, self.language_service.get_response_string("community_vote_need_more_time")

        votes = [t for t in st.get("votes", []) if now() - t < timedelta(days=1)]
        if len(votes) >= VOTE_LIMIT_PER_DAY:
            st["votes"] = votes
            self.mem_state[key] = st
            return False, self.language_service.get_response_string("community_vote_daily_limit")
        st["votes"] = votes
        self.mem_state[key] = st
        return True, ""

    def _record_vote_usage(self, update: Update) -> None:
        chat = update.effective_chat
        user = update.effective_user
        key = (chat.id, user.id)
        st = self.mem_state.get(key, {})
        votes = st.get("votes", [])
        votes.append(now())
        st["votes"] = votes
        self.mem_state[key] = st

    # ------------------------------------------------------------------
    # Actions
    # ------------------------------------------------------------------
    async def _hide_message(self, context: ContextTypes.DEFAULT_TYPE, chat_id: int, msg_id: int) -> None:
        try:
            await context.bot.delete_message(chat_id, msg_id)
        except Exception:
            pass

    async def _temp_ban(self, context: ContextTypes.DEFAULT_TYPE, chat_id: int, user_id: int, hours: int = TEMPBAN_HOURS) -> None:
        try:
            until = int((now() + timedelta(hours=hours)).timestamp())
            await context.bot.ban_chat_member(chat_id, user_id, until_date=until)
        except Exception:
            pass

    async def _permaban(self, context: ContextTypes.DEFAULT_TYPE, chat_id: int, user_id: int) -> None:
        try:
            await context.bot.ban_chat_member(chat_id, user_id)
        except Exception:
            pass

    async def _apply_decision(self, context: ContextTypes.DEFAULT_TYPE, case: Case, score: float, msg_id: Optional[int]) -> Optional[str]:
        action = None
        if score >= THRESH_PERMABAN:
            action = "permaban"
        elif score >= THRESH_TEMPBAN:
            action = "tempban"
        elif score >= THRESH_HIDE:
            action = "hide"

        if not action:
            return None

        if action == "hide" and msg_id:
            await self._hide_message(context, case.chat_id, msg_id)
        elif action == "tempban":
            await self._temp_ban(context, case.chat_id, case.target_user_id)
        elif action == "permaban":
            await self._permaban(context, case.chat_id, case.target_user_id)

        if self.admin_chat_id:
            try:
                await context.bot.send_message(
                    self.admin_chat_id,
                    f"Case #{case.case_id} action: {action} (score {score:.2f})",
                )
            except Exception:
                pass
        return action

    # ------------------------------------------------------------------
    # Public command handlers
    # ------------------------------------------------------------------
    async def cmd_report(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.message.reply_to_message:
            await update.effective_message.reply_text(
                self.language_service.get_response_string("community_report_reply_prompt")
            )
            return
        buttons = [
            [
                InlineKeyboardButton(
                    self.language_service.get_response_string(f"report_reason_{code}"),
                    callback_data=f"REPORT:{code}:{update.message.reply_to_message.message_id}",
                )
            ]
            for code in REASON_CODES
        ]
        kb = InlineKeyboardMarkup(buttons)
        await update.effective_message.reply_text(
            self.language_service.get_response_string("community_report_choose_reason"),
            reply_markup=kb,
        )

    async def on_report_reason(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        q = update.callback_query
        await q.answer()
        try:
            _, reason, msg_id_str = q.data.split(":", 2)
            msg_id = int(msg_id_str)
        except Exception:
            return

        ok, why = self._eligible_to_vote(update)
        if not ok:
            await q.edit_message_text(why)
            return

        chat = update.effective_chat
        user = update.effective_user
        try:
            orig = await context.bot.forward_message(chat.id, chat.id, msg_id)
            target_id = orig.forward_from.id if orig.forward_from else None
        except Exception:
            target_id = None
        if not target_id:
            await q.edit_message_text(
                self.language_service.get_response_string("community_report_target_missing")
            )
            return

        case = self._get_or_open_case(chat.id, target_id, msg_id, reason)
        if self._already_voted(case, user.id):
            await q.edit_message_text(
                self.language_service.get_response_string("community_report_already")
            )
            return
        self._add_vote(case, user.id, base=1.0)
        self._record_vote_usage(update)
        score = self._effective_score(case)
        await q.edit_message_text(
            self.language_service.get_response_string("community_report_thanks")
        )
        await self._apply_decision(context, case, score, msg_id)
        if self.admin_chat_id:
            try:
                await context.bot.send_message(
                    self.admin_chat_id,
                    f"Report: case {case.case_id} voter {user.id} reason {reason} score {score:.2f}",
                )
            except Exception:
                pass

    async def cmd_vote(self, update: Update, context: ContextTypes.DEFAULT_TYPE, reason: str = "spam") -> None:
        if not update.message or not update.message.reply_to_message:
            await update.effective_message.reply_text(
                self.language_service.get_response_string("community_vote_reply_prompt")
            )
            return
        voter = update.effective_user
        target = update.message.reply_to_message.from_user
        if voter.id == target.id:
            await update.effective_message.reply_text(
                self.language_service.get_response_string("community_vote_self_vote")
            )
            return

        ok, why = self._eligible_to_vote(update)
        if not ok:
            await update.effective_message.reply_text(why)
            return

        case = self._get_or_open_case(update.effective_chat.id, target.id, update.message.reply_to_message.message_id, reason)
        if self._already_voted(case, voter.id):
            await update.effective_message.reply_text(
                self.language_service.get_response_string("community_vote_already")
            )
            return
        w = self._add_vote(case, voter.id, base=1.0)
        self._record_vote_usage(update)
        score = self._effective_score(case)
        await update.effective_message.reply_text(
            self.language_service.get_response_string("community_vote_recorded")
        )
        await self._apply_decision(context, case, score, update.message.reply_to_message.message_id)
        if self.admin_chat_id:
            try:
                await context.bot.send_message(
                    self.admin_chat_id,
                    f"Vote: case {case.case_id} voter {voter.id} +{w:.2f} score {score:.2f}",
                )
            except Exception:
                pass



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/fact_check.py ---
======================================================================

# enkibot/modules/fact_check.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
"""Minimal fact checking subsystem skeleton.

The real project described a production ready fact checking service.  This
module implements a *very* small portion of that design so the rest of the bot
can start integrating with it.  The goal of the skeleton is to provide the
same public interfaces as the full system so that future patches can increment
ally flesh out the behaviour.

The implementation here does not perform any network requests or heavy
processing – it merely wires together the classes, dataclasses and handler
structure described in the design document.
"""

from __future__ import annotations

import hashlib
import json
import re
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Callable, Dict, List, Optional, TYPE_CHECKING
from urllib.parse import urlparse

from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update, Message
from telegram.ext import (
    Application,
    CallbackQueryHandler,
    CommandHandler,
    ContextTypes,
    MessageHandler,
    filters,
)
from ..utils.message_utils import get_text
from ..core.llm_services import LLMServices
import logging
import json
from types import SimpleNamespace
try:  # pragma: no cover - optional dependency
    import openai
except Exception:  # pragma: no cover
    openai = SimpleNamespace()
from .. import config
from ..utils.database import DatabaseManager
from ..utils.lang_router import normalize as normalize_unicode

if TYPE_CHECKING:  # pragma: no cover - only for type hints
    from .primary_source_hunter import PrimarySourceHunter, SourceHit

# Filter for messages that contain either plain text or a caption
TEXT_OR_CAPTION = (filters.TEXT & ~filters.COMMAND) | filters.CAPTION

logger = logging.getLogger(__name__)

try:  # pragma: no cover - optional dependency for language detection
    from langdetect import detect, DetectorFactory, LangDetectException
    DetectorFactory.seed = 0
except Exception:  # pragma: no cover
    detect = None  # type: ignore
    LangDetectException = Exception  # type: ignore

# ---------------------------------------------------------------------------
# Data models
# ---------------------------------------------------------------------------

@dataclass
class Claim:
    """Represents a normalised claim that can be checked."""

    text_norm: str
    text_orig: str
    lang: Optional[str]
    urls: List[str]
    hash: str


@dataclass
class Evidence:
    """Evidence item returned by a search fetcher."""

    url: str
    domain: str
    stance: str  # support|refute|mixed|na
    note: str
    published_at: Optional[str]
    snapshot_url: Optional[str]
    tier: Optional[int]
    score: float
    book: Optional["BookInfo"] = None


def get_domain_reputation(domain: str) -> str:
    """Return a simple reputation string for known domains."""
    domain = domain.lower()
    if "tass.ru" in domain or "rt.com" in domain:
        return f"Domain {domain} is associated with state-controlled media, often used for propaganda."
    if "reuters.com" in domain or "apnews.com" in domain:
        return f"Domain {domain} is a reputable international news agency."
    return f"Domain {domain} reputation unknown or neutral."


@dataclass
class Verdict:
    """Aggregated verdict for a claim."""

    label: str  # true|mostly_true|needs_context|unverified|false|misleading_media|opinion
    confidence: float
    summary: str
    sources: List[Evidence]
    debug: List[str] = field(default_factory=list)


@dataclass
class SatireDecision:
    """Output of the satire detector."""

    p_meta: float
    p_text: float
    p_vis: float
    p_audio: float
    p_satire: float
    decision: str  # satire|ambiguous|news
    rationale: Dict[str, object]


@dataclass
class BookInfo:
    """Book metadata associated with an evidence item."""

    author: str
    title: str
    edition: Optional[str] = None
    year: Optional[int] = None
    isbn: Optional[str] = None
    page: Optional[str] = None
    chapter: Optional[str] = None
    translator: Optional[str] = None
    quote_exact: Optional[str] = None


@dataclass
class Quote:
    """Represents a book quote to verify."""

    quote: str
    author: Optional[str]
    title: Optional[str]
    lang: Optional[str]
    hash: str


# ---------------------------------------------------------------------------
# Interfaces
# ---------------------------------------------------------------------------

class Fetcher:
    """Interface for web fetchers.

    Real implementations should contact fact checking sites and general web
    search APIs.  The default implementation used here simply returns an empty
    list so the rest of the pipeline can continue to work without external
    services.
    """

    async def fact_checker_search(self, claim: Claim) -> List[Evidence]:
        return []

    async def general_search(self, claim: Claim) -> List[Evidence]:
        return []

    async def reverse_image(self, claim: Claim) -> List[Evidence]:
        return []


class OpenAIWebFetcher(Fetcher):
    """Simple web fetcher using OpenAI's web search tool."""

    async def fact_checker_search(self, claim: Claim) -> List[Evidence]:
        return await self.general_search(claim)

    async def general_search(self, claim: Claim) -> List[Evidence]:
        logger.debug("Web fetcher: starting search for claim '%s'", claim.text_norm)
        if not config.OPENAI_API_KEY:
            logger.warning("Web fetcher: OPENAI_API_KEY missing, skipping web search")
            return []
        client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        extra: Dict[str, object] = {}
        if config.OPENAI_SEARCH_CONTEXT_SIZE:
            extra["search_context_size"] = config.OPENAI_SEARCH_CONTEXT_SIZE
        if config.OPENAI_SEARCH_USER_LOCATION:
            try:
                extra["user_location"] = json.loads(
                    config.OPENAI_SEARCH_USER_LOCATION
                )
            except Exception:
                extra["user_location"] = {
                    "country": config.OPENAI_SEARCH_USER_LOCATION
                }
        try:
            resp = await client.responses.create(
                model=config.OPENAI_DEEP_RESEARCH_MODEL_ID,
                tools=[{"type": "web_search"}],
                tool_choice="auto",
                instructions="Return ONLY a JSON array named 'items' of objects {url, title}.",
                input=claim.text_norm,
                **extra,
            )
            text = (getattr(resp, "output_text", "") or "").strip()
            data = json.loads(text) if text.startswith("{") else {"items": []}
            items = data.get("items", [])
            logger.debug("Web fetcher: received %d search items", len(items))
        except openai.OpenAIError as e:
            # If the configured deep-research model is unavailable (e.g. 403
            # model_not_found), try falling back to the general OpenAI model so
            # web search still works in a limited fashion.
            logger.warning(
                "Web fetcher: deep research model '%s' unavailable: %s",
                config.OPENAI_DEEP_RESEARCH_MODEL_ID,
                e,
            )
            try:
                resp = await client.responses.create(
                    model=config.OPENAI_MODEL_ID,
                    tools=[{"type": "web_search"}],
                    tool_choice="auto",
                    instructions="Return ONLY a JSON array named 'items' of objects {url, title}.",
                    input=claim.text_norm,
                    **extra,
                )
                text = (getattr(resp, "output_text", "") or "").strip()
                data = json.loads(text) if text.startswith("{") else {"items": []}
                items = data.get("items", [])
                logger.debug(
                    "Web fetcher: received %d search items using fallback model",
                    len(items),
                )
            except Exception as inner_e:
                logger.error(
                    "Web fetcher: search failed after fallback: %s", inner_e, exc_info=True
                )
                return []
        except Exception as e:
            logger.error("Web fetcher: search failed: %s", e, exc_info=True)
            return []
        evidences: List[Evidence] = []
        for item in items[:5]:
            url = item.get("url")
            title = item.get("title", "")
            if not url:
                continue
            domain = urlparse(url).netloc or url
            reputation = get_domain_reputation(domain)
            evidences.append(
                Evidence(
                    url=url,
                    domain=domain,
                    stance="support",
                    note=f"{title} — {reputation}",
                    published_at=None,
                    snapshot_url=None,
                    tier=None,
                    score=1.0,
                )
            )
        return evidences


class StanceModel:
    """Assigns a stance/score to each evidence item."""

    async def classify(self, claim: Claim, evidences: List[Evidence]) -> List[Evidence]:
        return evidences


class SatireDetector:
    """Very small satire detector stub.

    The detector returns a constant ``news`` decision so it never blocks fact
    checking.  The interface mirrors the design document and can be extended
    later with real models.
    """

    def __init__(self, cfg_reader: Callable[[int], Dict[str, object]]):
        self.cfg_reader = cfg_reader

    async def predict(self, update: Update, text: str) -> SatireDecision:
        cfg = self.cfg_reader(update.effective_chat.id)
        weights = cfg.get("satire", {}).get(
            "weights", {"meta": 0.4, "text": 0.35, "vis": 0.2, "audio": 0.05}
        )
        p_meta = p_text = p_vis = p_audio = 0.0
        p_sat = 0.0
        return SatireDecision(
            p_meta=p_meta,
            p_text=p_text,
            p_vis=p_vis,
            p_audio=p_audio,
            p_satire=p_sat,
            decision="news",
            rationale={"features": {"meta": p_meta, "text": p_text, "vis": p_vis, "audio": p_audio}},
        )


class NewsGate:
    """Heuristic + classifier gate for detecting news-like text.

    The gate follows the revised high-recall strategy: a fast heuristic pass
    first, falling back to a lightweight keyword based classifier when the
    heuristics are inconclusive.  The goal is to catch genuine news forwards
    (including Russian/Ukrainian) without replying to non-news messages.
    """

    def __init__(self) -> None:
        # Heuristic feature lexicons -------------------------------------------------
        self.source_keywords = ["reuters", "ap", "bbc", "tass", "gov", "минобороны", "мчс"]
        self.news_verbs = [
            "said",
            "announced",
            "reported",
            "claims",
            "denied",
            "заявил",
            "сообщил",
            "отметил",
            "подчеркнул",
            "приведена",
            "признал",
        ]
        self.location_keywords = ["москва", "абхаз", "киев", "washington", "moscow"]
        self.crisis_keywords = ["эвакуац", "санкц", "землетряс", "атака", "боевую готовность"]
        self.time_re = re.compile(
            r"\b(\d{4}|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|"
            r"monday|tuesday|wednesday|thursday|friday|saturday|sunday|today|yesterday|"
            r"сегодня|вчера|понедельник|вторник|среду|четверг|пятницу|субботу|воскресенье)\b",
            re.I,
        )

        # Classifier keywords (used when heuristics are inconclusive) ---------------
        self.classifier_keywords = [
            *self.news_verbs,
            "according to",
            "today",
            "yesterday",
        ]

    # Heuristic pass ---------------------------------------------------------------
    def _heuristic(self, text_l: str) -> bool:
        if any(k in text_l for k in self.source_keywords):
            return True
        if any(v in text_l for v in self.news_verbs):
            return True
        if any(loc in text_l for loc in self.location_keywords):
            return True
        if any(c in text_l for c in self.crisis_keywords):
            return True
        if self.time_re.search(text_l):
            return True
        if re.search(r"https?://", text_l):
            return True
        return False

    # Lightweight classifier -------------------------------------------------------
    def _classifier(self, text_l: str) -> float:
        score = 0.0
        if any(k in text_l for k in self.classifier_keywords):
            score += 0.4
        if self.time_re.search(text_l):
            score += 0.2
        if re.search(r"https?://", text_l):
            score += 0.2
        if len(text_l) > 80:
            score += 0.2
        return min(score, 1.0)

    async def predict(self, text: str) -> float:
        text_l = text.lower()
        if self._heuristic(text_l):
            return 1.0
        return self._classifier(text_l)


class QuoteGate:
    """Simple detector for book-like quotations."""

    def __init__(self) -> None:
        # Require at least two words inside the quotes so short single-word
        # phrases like "«Силовики»" do not trigger quote checking.
        self.quote_re = re.compile(r"[\"«](?:(?![\"»]).)*\s+(?:(?![\"»]).)+[\"»]")
        self.marker_keywords = [
            "как писал",
            "as",
            "wrote",
            "писал",
            "said",
            "\u2014",  # em dash
        ]

    async def predict(self, text: str) -> float:
        score = 0.0
        # Only consider it a potential literary quote if there is a quoted
        # passage with at least one space character inside. This avoids
        # flagging single-word quotations that often appear in news articles.
        if self.quote_re.search(text):
            score += 0.5
        text_l = text.lower()
        if any(k in text_l for k in self.marker_keywords):
            score += 0.3
        if re.search(r"\b\d{4}\b", text_l):
            score += 0.1
        if "\n" in text:
            score += 0.1
        return min(score, 1.0)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

URL_RE = re.compile(r"https?://\S+", re.I)


def normalize_text(s: str) -> str:
    """Apply Unicode normalization and collapse whitespace."""
    s = normalize_unicode(s)
    return re.sub(r"\s+", " ", s).strip()


def hash_claim(text: str, urls: List[str]) -> str:
    canon = normalize_text(text).lower() + "\n" + "|".join(sorted(set(urls)))
    return hashlib.sha256(canon.encode("utf-8")).hexdigest()


# ---------------------------------------------------------------------------
# Fact checking orchestrator
# ---------------------------------------------------------------------------

class FactChecker:
    """Tiny orchestrator coordinating fact checks via an LLM."""

    def __init__(
        self,
        fetcher: Optional[Fetcher] = None,
        stance: Optional[StanceModel] = None,
        llm_services: Optional[LLMServices] = None,
        primary_hunter: Optional["PrimarySourceHunter"] = None,
    ):
        self.fetcher = fetcher
        self.stance = stance or StanceModel()
        self.llm_services = llm_services
        self.primary_hunter = primary_hunter

    async def extract_claim(self, text: str) -> Optional[Claim]:
        logger.debug("Extracting claim from text: %r", text)
        if not text or len(text) < 10:
            logger.debug("Claim extraction aborted: text too short")
            return None
        urls = URL_RE.findall(text)
        text_norm = normalize_text(text)
        lang = None
        if detect:
            try:
                lang = detect(text_norm)
            except LangDetectException:
                lang = None
        claim = Claim(
            text_norm=text_norm,
            text_orig=text,
            lang=lang,
            urls=urls,
            hash=hash_claim(text_norm, urls),
        )
        logger.debug(
            "Claim extracted with hash %s, %d URLs, lang=%s",
            claim.hash,
            len(urls),
            lang,
        )
        return claim

    async def extract_quote(self, text: str) -> Optional[Quote]:
        """Extract a quotation and optional attribution."""
        m = re.search(r"[\"«](.+?)[\"»](?:\s*[\u2014\-]\s*([^\n]+))?", text, re.S)
        if not m:
            return None
        quote = normalize_text(m.group(1))
        # Ignore short single-word "quotes" often used for emphasis in news
        # articles. Only treat the text as a literary quotation if it contains
        # at least one space (i.e. two or more words).
        if " " not in quote:
            return None
        lang = None
        if detect:
            try:
                lang = detect(quote)
            except LangDetectException:
                lang = None
        author = title = None
        if m.group(2):
            parts = [p.strip() for p in re.split(r",|\u2014|-", m.group(2), maxsplit=1) if p.strip()]
            if parts:
                author = parts[0]
                if len(parts) > 1:
                    title = parts[1]
        h = hashlib.sha256(quote.lower().encode("utf-8")).hexdigest()
        return Quote(quote=quote, author=author, title=title, lang=lang, hash=h)

    async def _llm_verdict(self, claim: Claim, debug: List[str]) -> Verdict:
        """Use LLM to generate a verdict for the claim."""
        logger.debug("LLM verdict: generating for claim '%s'", claim.text_orig)
        if not self.llm_services:
            logger.warning("LLM verdict: llm_services not configured")
            debug.append("LLM service unavailable")
            return Verdict(
                label="unverified",
                confidence=0.0,
                summary="LLM service unavailable.",
                sources=[],
            )

        lang_name = {
            "ru": "Russian",
            "uk": "Ukrainian",
            "en": "English",
        }.get(claim.lang or "", "the original language of the claim")

        system_prompt = (
            "You are a fact-checking assistant. Decide if the claim is true, false, "
            "needs_context, or unverified. If unsure, respond with unverified. "
            "Return a JSON object with keys 'label', 'confidence' (0-1), and 'summary' in "
            f"{lang_name}."
        )
        user_prompt = f"Claim: {claim.text_orig}"
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        debug.append(
            f"LLM model: {getattr(self.llm_services, 'openai_deep_research_model_id', 'unknown')}"
        )
        debug.append(f"System prompt: {system_prompt}")
        debug.append(f"User prompt: {user_prompt}")

        try:
            completion = await self.llm_services.call_openai_deep_research(
                messages, max_output_tokens=300
            )
            logger.debug("LLM verdict: received completion %r", completion)
            debug.append(f"Raw response: {completion}")
        except Exception as e:
            logger.error("LLM verdict: call failed: %s", e, exc_info=True)
            debug.append(f"Call failed: {e}")
            completion = None

        label = "unverified"
        confidence = 0.0
        summary = "No analysis available."
        if completion:
            try:
                data = json.loads(completion)
                label = data.get("label", label)
                if isinstance(label, str):
                    label = label.strip().lower()
                confidence = float(data.get("confidence", 0.0))
                summary = data.get("summary", summary)
            except Exception:
                summary = completion.strip()

        logger.debug(
            "LLM verdict: label=%s confidence=%.2f summary=%r",
            label,
            confidence,
            summary,
        )
        return Verdict(label=label, confidence=confidence, summary=summary, sources=[])

    async def _llm_quote_verdict(self, quote: Quote, debug: List[str]) -> Verdict:
        """Verify a book quotation using an LLM stub."""
        if not self.llm_services:
            debug.append("LLM service unavailable for quote check")
            return Verdict(
                label="unverified",
                confidence=0.0,
                summary="LLM service unavailable.",
                sources=[],
            )

        system_prompt = (
            "You verify literary quotations. Determine if the provided quote is "
            "accurate and correctly attributed. Respond with a JSON object "
            "containing keys 'label' (accurate|misquote|misattrib|unverified), "
            "'confidence' (0-1) and 'summary'."
        )
        user_prompt = f"Quote: \"{quote.quote}\"\nAuthor: {quote.author or 'unknown'}\nTitle: {quote.title or 'unknown'}"
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        debug.append(f"LLM model: {getattr(self.llm_services, 'openai_model_id', 'unknown')}")
        debug.append(f"System prompt: {system_prompt}")
        debug.append(f"User prompt: {user_prompt}")

        try:
            completion = await self.llm_services.call_openai_llm(
                messages, temperature=0.0, max_tokens=300
            )
            debug.append(f"Raw response: {completion}")
        except Exception as e:
            logger.error(f"LLM quote-check failed: {e}", exc_info=True)
            debug.append(f"Call failed: {e}")
            completion = None

        label = "unverified"
        confidence = 0.0
        summary = "No analysis available."
        if completion:
            try:
                data = json.loads(completion)
                label = data.get("label", label)
                if isinstance(label, str):
                    label = label.strip().lower()
                confidence = float(data.get("confidence", 0.0))
                summary = data.get("summary", summary)
            except Exception:
                summary = completion.strip()

        return Verdict(label=label, confidence=confidence, summary=summary, sources=[])

    async def research(self, claim: Claim | Quote, track: str = "news") -> Verdict:
        logger.debug("Research: track=%s", track)
        debug: List[str] = [f"track={track}"]
        if track == "book":
            logger.debug("Research: delegating to quote verifier")
            verdict = await self._llm_quote_verdict(claim, debug)  # type: ignore[arg-type]
            verdict.debug = debug
            return verdict

        evidence: List[Evidence] = []
        if isinstance(claim, Claim):
            logger.debug("Research: claim='%s' lang=%s", claim.text_norm, claim.lang)
            debug.append(f"claim='{claim.text_norm}' lang={claim.lang}")
            if self.fetcher:
                try:
                    web_hits = await self.fetcher.fact_checker_search(claim)
                    logger.debug("Research: web fetcher returned %d items", len(web_hits))
                    debug.append(f"web fetcher returned {len(web_hits)} items")
                    evidence.extend(web_hits)
                except Exception as e:
                    logger.error("Research: web fetcher error: %s", e, exc_info=True)
                    debug.append(f"web fetcher error: {e}")
            else:
                logger.debug("Research: no web fetcher configured")
                debug.append("no web fetcher configured")

            if self.primary_hunter:
                try:
                    hits = await self.primary_hunter.hunt(claim.text_norm, claim.lang)
                    logger.debug("Research: primary source hunter returned %d hits", len(hits))
                    debug.append(f"primary source hunter returned {len(hits)} hits")
                    for h in hits:
                        evidence.append(
                            Evidence(
                                url=h.url,
                                domain=h.domain,
                                stance="na",
                                note=h.title,
                                published_at=None,
                                snapshot_url=None,
                                tier=h.tier,
                                score=1.0,
                            )
                        )
                    if evidence:
                        evidence = await self.stance.classify(claim, evidence)
                        logger.debug("Research: stance model processed evidence")
                        debug.append("stance model processed evidence")
                except Exception as e:
                    logger.error("Research: primary source hunter error: %s", e, exc_info=True)
                    debug.append(f"primary source hunter error: {e}")

            if (
                not evidence
                and claim.lang
                and claim.lang != "en"
                and self.llm_services
            ):
                try:
                    messages = [
                        {"role": "system", "content": "Translate the following text to English."},
                        {"role": "user", "content": claim.text_norm},
                    ]
                    translated = await self.llm_services.call_openai_llm(
                        messages,
                        model_id=self.llm_services.openai_translation_model_id,
                        temperature=0.0,
                        max_tokens=1000,
                    )
                    if translated:
                        t_claim = Claim(
                            text_norm=normalize_text(translated),
                            text_orig=claim.text_orig,
                            lang="en",
                            urls=claim.urls,
                            hash=claim.hash,
                        )
                        if self.fetcher:
                            try:
                                web_hits = await self.fetcher.fact_checker_search(t_claim)
                                evidence.extend(web_hits)
                                debug.append(
                                    f"web fetcher returned {len(web_hits)} items after translation"
                                )
                            except Exception as e:
                                logger.error(
                                    "Research: web fetcher error after translation: %s", e, exc_info=True
                                )
                        if self.primary_hunter:
                            try:
                                hits = await self.primary_hunter.hunt(t_claim.text_norm, "en")
                                for h in hits:
                                    evidence.append(
                                        Evidence(
                                            url=h.url,
                                            domain=h.domain,
                                            stance="na",
                                            note=h.title,
                                            published_at=None,
                                            snapshot_url=None,
                                            tier=h.tier,
                                            score=1.0,
                                        )
                                    )
                                if hits:
                                    debug.append(
                                        f"primary source hunter returned {len(hits)} hits after translation"
                                    )
                            except Exception as e:
                                logger.error(
                                    "Research: primary source hunter error after translation: %s",
                                    e,
                                    exc_info=True,
                                )
                except Exception as e:
                    logger.error("Research: translation step failed: %s", e, exc_info=True)

        verdict = await self._llm_verdict(claim, debug)  # type: ignore[arg-type]
        logger.debug(
            "Research: verdict=%s confidence=%.2f", verdict.label, verdict.confidence
        )
        debug.append(f"verdict={verdict.label} confidence={verdict.confidence:.2f}")
        verdict.sources = evidence
        verdict.debug = debug
        return verdict


# ---------------------------------------------------------------------------
# Telegram glue
# ---------------------------------------------------------------------------

class FactCheckBot:
    """Registers Telegram handlers for fact checking."""

    def __init__(
        self,
        app: Application,
        fc: FactChecker,
        satire_detector: Optional[SatireDetector] = None,
        news_gate: Optional[NewsGate] = None,
        quote_gate: Optional[QuoteGate] = None,
        cfg_reader: Callable[[int], Dict[str, object]] | None = None,
        db_manager: Optional[DatabaseManager] = None,
        language_service=None,
    ) -> None:
        self.app = app
        self.fc = fc
        self.satire = satire_detector or SatireDetector(lambda _chat_id: {})
        self.news_gate = news_gate or NewsGate()
        self.quote_gate = quote_gate or QuoteGate()
        self.cfg_reader = cfg_reader or (lambda _chat_id: {})
        self.db_manager = db_manager
        self.language_service = language_service

    # Public API -------------------------------------------------------------
    def register(self) -> None:
        self.app.add_handler(CommandHandler("factcheck", self.cmd_factcheck))
        self.app.add_handler(
            MessageHandler(filters.FORWARDED & TEXT_OR_CAPTION, self.on_forward)
        )
        # Safety net for older PTB versions where Caption filter may not fire
        try:
            # PTB < v22
            document_filter = filters.DOCUMENT
        except AttributeError:
            # PTB v22+
            document_filter = filters.Document.ALL

        self.app.add_handler(
            MessageHandler(
                filters.FORWARDED & (filters.PHOTO | filters.VIDEO | document_filter),
                self.on_forward,
            )
        )
        self.app.add_handler(CallbackQueryHandler(self.on_factconfig_cb, pattern=r"^FC:"))
        self.app.add_handler(CommandHandler("factconfig", self.cmd_factconfig))

    # Handlers --------------------------------------------------------------
    async def on_forward(self, update: Update, ctx: ContextTypes.DEFAULT_TYPE) -> None:
        message = update.effective_message
        chat = getattr(update, "effective_chat", None)
        user = getattr(update, "effective_user", None)
        forward_from = getattr(message, "forward_from_chat", None)
        forward_name = getattr(forward_from, "username", None)
        text_for_logging = get_text(message) or ""
        try:
            logger.info(
                "Forward handler: chat=%s user=%s(%s) msg_id=%s forwarded_from=%s",
                getattr(chat, "id", None),
                getattr(user, "id", None),
                getattr(user, "username", None),
                getattr(message, "message_id", None),
                forward_name,
            )

            # Only handle messages forwarded from channels. Forwards from users or
            # anonymous sources are ignored, as the news/book gates are intended for
            # channel content.
            if not forward_from:
                logger.info("Forward handler: no forward_from_chat, ignoring message")
                return

            text = text_for_logging
            if not text and (
                message.photo or message.video or message.document
            ):
                # Text-first workflow: only invoke OCR if the forward lacks text
                text = await self._ocr_extract(message)
            logger.info("Forward handler: extracted text length %d", len(text))

            forward_username = (
                forward_from.username.lstrip("@").lower()
                if forward_from and getattr(forward_from, "username", None)
                else None
            )
            logger.info("Forward handler: normalized username=%s", forward_username)
            if forward_username and self.db_manager:
                try:
                    raw_sources = await self.db_manager.get_news_channel_usernames()
                    known_sources = {name.lstrip("@").lower() for name in raw_sources}
                    logger.info(
                        "Forward handler: loaded %d known news channels", len(known_sources)
                    )
                except Exception:
                    known_sources = set()
                    logger.exception("Forward handler: failed to load news channel list")
                if forward_username in known_sources:
                    logger.info(
                        "Forward handler: channel %s recognized, triggering news check",
                        forward_username,
                    )
                    await self._run_check(update, ctx, text, track="news")
                    return
                logger.info(
                    "Forward handler: channel %s not in news channel list", forward_username
                )
            else:
                logger.info(
                    "Forward handler: skipping news channel check (username=%s, db_manager=%s)",
                    forward_username,
                    bool(self.db_manager),
                )
            cfg = self.cfg_reader(update.effective_chat.id)
            logger.info(
                "Forward handler: chat %s config %s", update.effective_chat.id, cfg
            )
            if cfg.get("satire", {}).get("enabled", True):
                dec = await self.satire.predict(update, text)
                logger.info("Forward handler: satire decision=%s", dec.decision)
                await self._log_satire(update, dec)
                if dec.decision == "satire":
                    kb = InlineKeyboardMarkup(
                        [[InlineKeyboardButton("Fact check anyway", callback_data="FC:FORCE")]]
                    )
                    await update.effective_message.reply_text(
                        "\ud83c\udccf Looks like satire/parody from this source.", reply_markup=kb
                    )
                    return
            if cfg.get("auto", {}).get("auto_check_news", True):
                p_news = await self.news_gate.predict(text)
                p_book = await self.quote_gate.predict(text)
                logger.info(
                    "Forward handler: gate scores p_news=%.2f p_book=%.2f", p_news, p_book
                )
                if self.db_manager and update.effective_chat:
                    try:
                        await self.db_manager.log_fact_gate(
                            update.effective_chat.id,
                            update.effective_message.message_id,
                            p_news,
                            p_book,
                        )
                    except Exception:
                        pass
                if p_book >= 0.70:
                    logger.info(
                        "Forward handler: p_book %.2f >= 0.70, triggering book check", p_book
                    )
                    await self._run_check(update, ctx, text, track="book")
                    return
                if p_news >= 0.70:
                    logger.info(
                        "Forward handler: p_news %.2f >= 0.70, triggering news check", p_news
                    )
                    await self._run_check(update, ctx, text, track="news")
                    return
                if p_book >= 0.55:
                    logger.info("Forward handler: p_book %.2f >= 0.55 show hint", p_book)
                    hint = (
                        self.language_service.get_response_string("hint_check_quote", "Check quote?")
                        if self.language_service
                        else "Check quote?"
                    )
                    await self._show_author_only_hint(update, ctx, hint, "book")
                    return
                if p_news >= 0.55:
                    logger.info("Forward handler: p_news %.2f >= 0.55 show hint", p_news)
                    hint = (
                        self.language_service.get_response_string("hint_check_news", "Check as news?")
                        if self.language_service
                        else "Check as news?"
                    )
                    await self._show_author_only_hint(update, ctx, hint, "news")
                    return
        finally:
            if chat and user and text_for_logging and self.db_manager:
                try:
                    await self.db_manager.log_chat_message_and_upsert_user(
                        chat_id=chat.id,
                        user_id=user.id,
                        username=getattr(user, "username", None),
                        first_name=getattr(user, "first_name", None),
                        last_name=getattr(user, "last_name", None),
                        message_id=message.message_id,
                        message_text=text_for_logging,
                        preferred_language=getattr(self.language_service, "current_lang", None),
                    )
                    logger.info(
                        "Forward handler: logged forwarded message chat=%s user=%s msg_id=%s",
                        chat.id,
                        user.id,
                        message.message_id,
                    )
                except Exception as exc:
                    logger.error(
                        "Forward handler: failed to log forwarded message: %s", exc
                    )

    async def cmd_factcheck(self, update: Update, ctx: ContextTypes.DEFAULT_TYPE) -> None:
        if update.effective_message.reply_to_message:
            text = get_text(update.effective_message.reply_to_message) or ""
        else:
            text = " ".join(ctx.args)
        await self._run_check(update, ctx, text)

    async def _run_check(
        self,
        update: Update,
        ctx: ContextTypes.DEFAULT_TYPE,
        text: str,
        message: Optional[Message] = None,
        track: str = "news",
    ) -> None:
        """Run a fact check and react to the target message."""
        logger.debug("Run check: track=%s text=%r", track, text)
        if track == "book":
            claim = await self.fc.extract_quote(text)
        else:
            claim = await self.fc.extract_claim(text)
        if not claim:
            logger.debug("Run check: claim extraction failed")
            return

        verdict = await self.fc.research(claim, track=track)
        logger.debug(
            "Run check: verdict label=%s confidence=%.2f", verdict.label, verdict.confidence
        )
        label = verdict.label.strip().lower()

        target_msg = message or update.effective_message
        if self.db_manager and update.effective_chat and target_msg:
            try:
                await self.db_manager.log_fact_check(
                    update.effective_chat.id,
                    target_msg.message_id,
                    getattr(claim, "text_orig", getattr(claim, "quote", "")),
                    label,
                    verdict.confidence,
                    track,
                    "\n".join(verdict.debug) if verdict.debug else None,
                )
            except Exception as e:
                logger.error(f"Failed to log fact check: {e}", exc_info=True)

        try:
            if label in ("true", "mostly_true"):
                await target_msg.set_reaction("👍")
            else:
                await target_msg.set_reaction("👎")
                await target_msg.reply_text(
                    verdict.summary, disable_web_page_preview=True
                )
        except Exception:  # pragma: no cover - reaction support may vary
            if label not in ("true", "mostly_true"):
                await target_msg.reply_text(
                    verdict.summary, disable_web_page_preview=True
                )

    async def _ocr_extract(self, message: Message) -> str:
        """Extract text from media using a vision model (stub)."""
        # Placeholder: a real implementation would call GPT-4o or another OCR
        # service.  Returning an empty string keeps the pipeline silent when no
        # text is available.
        try:
            if self.fc.llm_services:
                # Actual OCR call would go here.
                pass
        except Exception:  # pragma: no cover - best effort
            pass
        return ""

    async def _show_author_only_hint(
        self, update: Update, ctx: ContextTypes.DEFAULT_TYPE, hint: str, track: str
    ) -> None:
        """Send a small hint to the original forwarder only."""
        kb = InlineKeyboardMarkup(
            [[InlineKeyboardButton(hint, callback_data=f"FC:GATE:CHECK:{track}")]]
        )
        try:
            await ctx.bot.send_message(update.effective_user.id, hint, reply_markup=kb)
        except Exception:  # pragma: no cover - best effort
            pass

    def _format_card(self, v: Verdict) -> str:
        icon = {
            "true": "\u2705",
            "mostly_true": "\u2611\ufe0f",
            "needs_context": "\U0001f7e8",
            "unverified": "\ud83d\udd52",
            "false": "\u274c",
            "misleading_media": "\u26a0\ufe0f",
            "opinion": "\ud83d\udcac",
        }.get(v.label, "\u2139\ufe0f")
        lines = [
            f"{icon} Verdict: *{v.label.replace('_', ' ').title()}* ({v.confidence:.0%})",
            v.summary,
            "\nTop sources:",
        ]
        for e in v.sources:
            lines.append(
                f"\u2022 {e.domain} — {e.stance} {('('+e.published_at+')') if e.published_at else ''}"
            )
        return "\n".join(lines)

    # ---- /factconfig panel stubs -----------------------------------------
    async def cmd_factconfig(self, update: Update, ctx: ContextTypes.DEFAULT_TYPE) -> None:
        tr = (
            self.language_service.get_response_string
            if self.language_service
            else lambda k, d: d
        )
        kb = InlineKeyboardMarkup(
            [
                [
                    InlineKeyboardButton(
                        tr("factconfig_tab_preset", "Preset"),
                        callback_data="FC:TAB:Preset",
                    ),
                    InlineKeyboardButton(
                        tr("button_show_sources", "Sources"),
                        callback_data="FC:TAB:Sources",
                    ),
                ],
                [
                    InlineKeyboardButton(
                        tr("factconfig_tab_policy", "Policy"),
                        callback_data="FC:TAB:Policy",
                    ),
                    InlineKeyboardButton(
                        tr("factconfig_tab_limits", "Limits"),
                        callback_data="FC:TAB:Limits",
                    ),
                ],
                [
                    InlineKeyboardButton(
                        tr("factconfig_tab_auto", "Auto"),
                        callback_data="FC:TAB:Auto",
                    ),
                    InlineKeyboardButton(
                        tr("factconfig_tab_danger", "Danger"),
                        callback_data="FC:TAB:Danger",
                    ),
                ],
                [
                    InlineKeyboardButton(
                        tr("factconfig_export_btn", "Export"),
                        callback_data="FC:EXPORT",
                    ),
                    InlineKeyboardButton(
                        tr("factconfig_apply_btn", "Apply"),
                        callback_data="FC:APPLY",
                    ),
                ],
            ]
        )
        await update.effective_message.reply_text(
            tr("factconfig_title", "Fact check config:"), reply_markup=kb
        )

    async def on_factconfig_cb(self, update: Update, ctx: ContextTypes.DEFAULT_TYPE) -> None:
        q = update.callback_query
        data = q.data
        tr = (
            self.language_service.get_response_string
            if self.language_service
            else lambda k, d: d
        )
        await q.answer()
        if data == "FC:FORCE":
            if q.message and q.message.reply_to_message:
                orig = q.message.reply_to_message
                text = get_text(orig) or ""
                await q.edit_message_text(tr("factconfig_checking", "Checking…"))
                await self._run_check(update, ctx, text, message=orig)
            else:
                await q.edit_message_text(
                    tr("factconfig_nothing_to_check", "Nothing to check.")
                )
            return
        if data.startswith("FC:GATE:CHECK"):
            parts = data.split(":")
            track = parts[3] if len(parts) > 3 else "news"
            text = (q.message.text or "").replace(
                "\n\nCheck quote?", "").replace("\n\nCheck as news?", ""
            ).strip()
            await q.edit_message_text(tr("factconfig_checking", "Checking…"))
            await self._run_check(update, ctx, text, track=track)
            return
        if data == "FC:EXPORT":
            await q.edit_message_text(tr("factconfig_export_ok", "Configuration exported."))
            return
        if data == "FC:APPLY":
            await q.edit_message_text(tr("factconfig_apply_ok", "Configuration applied."))
            return
        await q.edit_message_text(tr("factconfig_update_ok", "Config updated."))

    # ------------------------------------------------------------------
    async def _log_satire(self, update: Update, dec: SatireDecision) -> None:
        """Persist satire decisions.

        Real implementation would insert a row into the SQL audit tables.  We
        simply log to console for now.
        """

        try:
            debug_data = json.dumps(dec.rationale)
        except Exception:  # pragma: no cover - best effort
            debug_data = "{}"
        update_str = f"chat={update.effective_chat.id} msg={update.effective_message.message_id}"
        print(f"Satire decision {dec.decision} for {update_str}: {debug_data}")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/fact_extractor.py ---
======================================================================

﻿# enkibot/modules/fact_extractor.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Fact Extractor ===
# ==================================================================================================
# This module uses linguistic rules and morphological analysis to extract specific
# pieces of information from user text, such as a person's name mentioned in a query.
# This is faster and cheaper than using an LLM for simple, well-defined extraction tasks.
# ==================================================================================================

import logging
import re
import pymorphy3

logger = logging.getLogger(__name__)

# Initialize the morphological analyzer for Russian.
try:
    morph = pymorphy3.MorphAnalyzer()
except Exception as e:
    logger.error(f"Could not initialize pymorphy3 MorphAnalyzer: {e}. Fact extraction might fail.")
    morph = None

def find_user_search_query_in_text(text: str) -> str | None:
    """
    Analyzes text by lemmatizing words to their base form and looks for a combination
    of trigger words and prepositions to extract a user name.

    This is a classic Natural Language Processing (NLP) technique for entity extraction.

    Args:
        text: The user's message text.

    Returns:
        The extracted name as a string, or None if no name is found.
    """
    if not morph:
        logger.warning("Morphological analyzer not available. Skipping user search query extraction.")
        return None
        
    # Dictionaries of trigger word lemmas (base forms).
    # This makes the system robust to different word forms (e.g., 'tell', 'tells', 'told').
    TELL_LEMMAS = {'рассказать', 'поведать', 'сообщить', 'описать'}
    INFO_LEMMAS = {'информация', 'инфо', 'справка', 'досье', 'данные'}
    WHO_LEMMAS = {'кто', 'что'}
    EXPLAIN_LEMMAS = {'пояснить', 'объяснить'}
    REMEMBER_LEMMAS = {'помнить', 'напомнить'}

    # Prepositions that typically follow trigger words before a name.
    PREPOSITIONS = {'о', 'про', 'за', 'на', 'по'}

    # Split the text into words
    words = re.findall(r"[\w'-]+", text.lower())
    
    for i, word in enumerate(words):
        try:
            # Get the lemma (normal form) of the word
            lemma = morph.parse(word)[0].normal_form
            
            # Check if the lemma is one of our triggers
            is_trigger = (lemma in TELL_LEMMAS or
                          lemma in INFO_LEMMAS or
                          lemma in WHO_LEMMAS or
                          lemma in EXPLAIN_LEMMAS or
                          lemma in REMEMBER_LEMMAS)

            if is_trigger:
                # We found a trigger word. The name should follow it.
                start_index = i + 1
                
                # If the next word is a preposition, skip it.
                if start_index < len(words) and words[start_index] in PREPOSITIONS:
                    start_index += 1
                
                # Everything that follows (up to 3 words) is considered the name.
                if start_index < len(words):
                    # Capture 1 to 3 words after the trigger/preposition.
                    name_parts = words[start_index : start_index + 3]
                    extracted_name = " ".join(name_parts)
                    logger.info(f"Extracted potential user search query: '{extracted_name}'")
                    return extracted_name

        except Exception as e:
            logger.error(f"Error during lemmatization of word '{word}': {e}")
            continue
            
    return None



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/image_generation_service.py ---
======================================================================

# enkibot/modules/image_generation_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------

import logging
from typing import Optional, TYPE_CHECKING

from telegram import Update
from telegram.ext import ContextTypes
from telegram.constants import ChatAction

# This import makes the image generation tool available to be called.
from TBD_tool_name import image_generation

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer

logger = logging.getLogger(__name__)

class ImageGenerationIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer'):
        logger.info("ImageGenerationIntentHandler initialized.")
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        """
        Handles the full workflow for an image generation request, from prompt
        extraction to sending the final image or an error message.
        """
        if not update.message or not update.effective_chat:
            return

        # 1. Acknowledge the request and send a "typing" or "uploading" action.
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.UPLOAD_PHOTO)
        # Send a preliminary "working on it" message that we can edit or delete later.
        preliminary_reply = await update.message.reply_text(self.language_service.get_response_string("image_generation_start"))

        # 2. Extract a clean, descriptive prompt from the user's full message.
        extractor_prompts = self.language_service.get_llm_prompt_set("image_generation_prompt_extractor")
        clean_prompt = None
        if extractor_prompts and "system" in extractor_prompts:
            clean_prompt = await self.intent_recognizer.extract_image_prompt_with_llm(
                text=user_msg_txt,
                lang_code=self.language_service.current_lang,
                system_prompt=extractor_prompts["system"],
                user_prompt_template=extractor_prompts.get("user", "{text}")
            )
        else:
            logger.error("Image generation prompt extractor prompts are missing!")

        # 3. Handle cases where a clear prompt could not be extracted.
        if not clean_prompt:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=preliminary_reply.message_id,
                text=self.language_service.get_response_string("image_generation_no_prompt")
            )
            return

        # 4. Call the image generation tool and handle the result.
        try:
            # This is the direct call to the image generation tool.
            image_gen_result = image_generation.generate_images(
                prompts=[clean_prompt],
                image_generation_usecase=image_generation.ImageGenerationUsecase.ALTERNATIVES
            )
            
            content_id = None
            if image_gen_result and image_gen_result.results:
                first_result = image_gen_result.results[0]
                if first_result and first_result.generated_images:
                    content_id = first_result.content_id

            # Delete the preliminary "On it! Imagining something..." message.
            await context.bot.delete_message(chat_id=update.effective_chat.id, message_id=preliminary_reply.message_id)

            if content_id:
                # If successful, send the image by replying with its content_id.
                # The Telegram client will render this as the image.
                logger.info(f"Successfully generated image for prompt '{clean_prompt}'. Replying with content_id.")
                await update.message.reply_text(content_id)
            else:
                # Handle the case where the tool ran but failed to produce an image.
                logger.error(f"Image generation tool ran but failed for prompt: {clean_prompt}")
                await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))

        except Exception as e:
            logger.error(f"An exception occurred during the image generation tool call: {e}", exc_info=True)
            # Edit the preliminary message to show an error instead of leaving the user hanging.
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=preliminary_reply.message_id,
                text=self.language_service.get_response_string("image_generation_error")
            )



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/intent_recognizer.py ---
======================================================================

# enkibot/modules/intent_recognizer.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
import logging
import json
import re 
from typing import Dict, Any, Optional, TYPE_CHECKING

from enkibot import config 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices 

logger = logging.getLogger(__name__)

class IntentRecognizer:
    def __init__(self, llm_services: 'LLMServices'): 
        logger.info("IntentRecognizer __init__ STARTING")
        self.llm_services = llm_services
        logger.info("IntentRecognizer __init__ COMPLETED")

    async def classify_master_intent(self, text: str, lang_code: str, 
                                     system_prompt: str, user_prompt_template: str) -> str:
        logger.info(f"Classifying master intent (lang: {lang_code}): '{text[:100]}...'")
        user_prompt = user_prompt_template.format(text_to_classify=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        if classification_model_id and any(model_prefix in classification_model_id for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}

        classified_intent_value = "UNKNOWN_INTENT" 
        completion_str_for_log = "N/A"

        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=classification_model_id, 
                temperature=0.0, 
                max_tokens=100, 
                **response_format_arg 
            )
            completion_str_for_log = completion_str if completion_str is not None else "None"

            if completion_str:
                try:
                    clean_comp_str = completion_str.strip()
                    match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                    if match:
                        clean_comp_str = match.group(1).strip()
                    elif clean_comp_str.startswith("```"): 
                        clean_comp_str = clean_comp_str.strip("` \t\n\r")
                        if clean_comp_str.lower().startswith("json"): 
                            clean_comp_str = clean_comp_str[4:].strip() 
                    
                    data = json.loads(clean_comp_str)
                    intent_from_json = data.get("intent", data.get("INTENT")) 

                    if intent_from_json and isinstance(intent_from_json, str):
                        processed_intent = intent_from_json.strip().strip('_').upper().replace(" ", "_")
                        known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "IMAGE_GENERATION_QUERY", 
                                            "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", 
                                            "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                        if processed_intent in known_categories:
                            classified_intent_value = processed_intent
                            logger.info(f"Master intent classified as: {classified_intent_value} via JSON.")
                        else:
                            logger.warning(f"LLM JSON with unknown category '{processed_intent}'. Raw value: '{intent_from_json}'. Full data: {data}. Defaulting UNKNOWN.")
                    else:
                        logger.warning(f"LLM JSON for master intent missing 'intent' key or not string. Data: {data}. Defaulting UNKNOWN.")
                except json.JSONDecodeError:
                    logger.warning(f"Failed to decode JSON from master_intent_classifier. LLM raw: '{completion_str_for_log}'. Attempting direct parse.")
                    raw_intent = completion_str_for_log.strip().strip('_').upper().replace(" ", "_")
                    if raw_intent.startswith('{') and raw_intent.endswith('}'):
                        try:
                            temp_data = json.loads(raw_intent)
                            extracted_val = temp_data.get("intent", temp_data.get("INTENT", raw_intent))
                            raw_intent = str(extracted_val).strip().strip('_').upper().replace(" ", "_")
                        except json.JSONDecodeError: 
                            pass 

                    known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "IMAGE_GENERATION_QUERY", 
                                        "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", 
                                        "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                    if raw_intent in known_categories:
                        classified_intent_value = raw_intent
                        logger.info(f"Master intent classified as: {classified_intent_value} via direct string parse fallback.")
                    else:
                         logger.warning(f"Direct string parse fallback failed for intent: '{raw_intent}'. Defaulting UNKNOWN.")
            else:
                logger.warning("Master intent classification LLM call returned no content.")
        except Exception as e: 
            logger.error(f"Error during master intent classification LLM call: {e}", exc_info=True)
        
        return classified_intent_value

    async def analyze_weather_request_with_llm(self, text: str, lang_code: str, 
                                               system_prompt: str, user_prompt_template: Optional[str]) -> Dict[str, Any]:
        logger.info(f"Analyzing weather request type (lang: {lang_code}): '{text}' with LLM.")
        user_prompt = (user_prompt_template or "{text}").format(text=text) 
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        if model_to_use and any(model_prefix in model_to_use for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}
        
        default_response = {"type": "current"} 
        completion_str_for_error_log = "N/A"
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=model_to_use,
                temperature=0, 
                **response_format_arg
            )
            completion_str_for_error_log = completion_str if completion_str is not None else "None"
            if completion_str:
                logger.info(f"LLM response for weather analysis: {completion_str}")
                clean_comp_str = completion_str.strip()
                match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                if match: clean_comp_str = match.group(1).strip()
                elif clean_comp_str.startswith("```"): 
                    clean_comp_str = clean_comp_str.strip("` \t\n\r") 
                    if clean_comp_str.lower().startswith("json"): 
                        clean_comp_str = clean_comp_str[4:].strip()              
                return json.loads(clean_comp_str.strip())
            else:
                logger.warning("LLM returned no content for weather analysis.")
        except json.JSONDecodeError as e:
            logger.error(f"Error decoding JSON from LLM for weather analysis: '{completion_str_for_error_log}'. Error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error in LLM call during weather request analysis: {e}", exc_info=True)
        
        return default_response

    async def extract_location_with_llm(self, text: str, lang_code: str, 
                                        system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM location extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"):
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
            
            if not location and self.llm_services.is_provider_configured("groq"): 
                logger.info("OpenAI location extraction failed or not configured, trying Groq.")
                completion = await self.llm_services.call_llm_api(
                    "Groq", self.llm_services.groq_api_key, self.llm_services.groq_endpoint_url, 
                    self.llm_services.groq_model_id, messages_for_api,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM location extraction: {e}", exc_info=True)

        if location and location.lower().strip() not in ["none", "null", "n/a", ""]:
            logger.info(f"LLM successfully extracted location: '{location}'")
            return location
        logger.warning(f"LLM couldn't extract location from: '{text}'.")
        return None

    async def extract_location_from_reply(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: str) -> Optional[str]:
        logger.info(f"Extracting location from user's reply (lang: {lang_code}): '{text}'")
        user_prompt = user_prompt_template.format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID 
        try:
            completion = await self.llm_services.call_openai_llm(
                messages_for_api,
                model_id=model_to_use,
                temperature=0.0,
                max_tokens=30 
            )
            if completion and completion.lower().strip() not in ["none", "null", "n/a", ""]:
                location = completion.strip().strip('"')
                logger.info(f"LLM extracted location from reply: '{location}'")
                return location
            else:
                logger.warning(f"LLM indicated no location in reply or returned 'None' for: '{text}'")
        except Exception as e:
            logger.error(f"Error during LLM location extraction from reply: {e}", exc_info=True)
        
        logger.warning(f"Could not extract a clear location from reply: '{text}'")
        return None

    # In enkibot/modules/intent_recognizer.py
    async def extract_topic_from_reply(self, text: str, lang_code: str, 
                                       system_prompt: str, user_prompt_template: str) -> Optional[str]:
        logger.info(f"Extracting news topic from user's reply (lang: {lang_code}): '{text}'")
        user_prompt = user_prompt_template.format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        topic = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            completion = await self.llm_services.call_openai_llm(
                messages_for_api,
                model_id=model_to_use,
                temperature=0.0, # Be very deterministic
                max_tokens=50 
            )
            if completion:
                cleaned_completion = completion.strip().strip('"') 
                # Check if LLM explicitly returns "None" or if it's very short after cleaning
                if cleaned_completion.lower() not in ["none", "null", "n/a", ""] and len(cleaned_completion) > 1: # Min length for a topic
                    topic = cleaned_completion
                    logger.info(f"LLM extracted topic from reply: '{topic}'")
                    return topic # Return the extracted topic
                else:
                    logger.warning(f"LLM indicated no topic in reply or returned 'None'/'empty' for: '{text}'")
            else:
                logger.warning(f"LLM call for topic extraction from reply returned no content for: '{text}'")
            
        except Exception as e:
            logger.error(f"Error during LLM topic extraction from reply: {e}", exc_info=True)
        
        logger.warning(f"Could not extract a clear topic from reply: '{text}'. Defaulting to None.")
        return None # Return None if no clear topic extracted or error
    
    async def extract_image_prompt_with_llm(self, text: str, lang_code: str, 
                                             system_prompt: str, user_prompt_template: str) -> Optional[str]:
        logger.info(f"Attempting to extract image generation prompt from text (lang: {lang_code}): '{text}'")
        user_prompt = user_prompt_template.format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID 
        
        try:
            completion = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=model_to_use, 
                temperature=0.2, 
                max_tokens=150 
            )
            
            if completion and completion.lower().strip() not in ["none", "null", "n/a", ""]:
                clean_prompt = completion.strip().strip('"')
                # Further cleaning if LLM adds conversational wrappers
                clean_prompt = re.sub(r"^(Okay, here's the prompt:|The image prompt is:|Sure, the prompt is:)\s*", "", clean_prompt, flags=re.IGNORECASE).strip()
                logger.info(f"Extracted image prompt: '{clean_prompt}'")
                return clean_prompt
            else:
                logger.info(f"LLM indicated no specific image prompt could be extracted from: '{text}'")
                return None
        except Exception as e:
            logger.error(f"Error in LLM call during image prompt extraction: {e}", exc_info=True)
            return None

    async def extract_news_topic_with_llm(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM news topic extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        topic = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: topic = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM news topic extraction: {e}", exc_info=True)

        if topic and topic.lower().strip() not in ["none", "null", "n/a", ""]:
            logger.info(f"LLM successfully extracted news topic: '{topic}'")
            return topic
        logger.info(f"LLM found no specific news topic in '{text}'.")
        return None



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/karma_manager.py ---
======================================================================

# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# enkibot/modules/karma_manager.py
# (Your GPLv3 Header)

import logging
import re
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Tuple, TYPE_CHECKING

if TYPE_CHECKING:
    from enkibot.utils.database import DatabaseManager

logger = logging.getLogger(__name__)

KARMA_COOLDOWN_MINUTES = 5

# Regex patterns for simple text-based votes
_THANKS_PATTERN = re.compile(r"^(?:thx|thanks)$", re.IGNORECASE)
_UPVOTE_PATTERN = re.compile(r"^(?:\+|\+\+|\+1|good|great|bravo|well\s*done)$", re.IGNORECASE)
_DOWNVOTE_PATTERN = re.compile(r"^(?:\-|\-\-|\-1|boo|dislike|bad)$", re.IGNORECASE)

# Default emoji/weight mapping from the karma spec
DEFAULT_EMOJI_WEIGHTS: Dict[str, float] = {
    "helpful": 1.0,
    "insightful": 1.2,
    "precise": 1.0,
    "funny": 0.6,
    "thanks": 0.8,
    "low_quality": -0.8,
    "misleading": -1.5,
    "offtopic": -0.5,
}


@dataclass
class KarmaConfig:
    """Minimal representation of per‑chat karma configuration."""
    emoji_map: Dict[str, float] = field(default_factory=lambda: DEFAULT_EMOJI_WEIGHTS.copy())
    decay_msg_days: int = 7
    decay_user_days: int = 45
    allow_downvotes: bool = True
    daily_budget: int = 18
    downvote_quorum: int = 4
    diversity_window_hours: int = 12
    reciprocity_threshold: float = 0.30
    preset: str = "medium"
    auto_tune: bool = True

class KarmaManager:
    def __init__(self, db_manager: 'DatabaseManager'):
        logger.info("KarmaManager initialized.")
        self.db_manager = db_manager
        self.config = KarmaConfig()

    # ------------------------------------------------------------------
    # Parsing & Weight computation
    # ------------------------------------------------------------------
    def parse_vote_token(self, text: str) -> Optional[Tuple[float, str]]:
        """Return (base_weight, tag) for known vote tokens."""
        stripped = text.strip()
        if _THANKS_PATTERN.match(stripped):
            return self.config.emoji_map["thanks"], "thanks"
        if _UPVOTE_PATTERN.match(stripped):
            return self.config.emoji_map["helpful"], "helpful"
        if _DOWNVOTE_PATTERN.match(stripped):
            # Treat all negatives as low_quality for now
            return self.config.emoji_map["low_quality"], "low_quality"
        return None

    async def record_reaction_event(
        self,
        chat_id: int,
        msg_id: int,
        target_user_id: int,
        rater_user_id: int,
        emoji: str,
    ) -> None:
        """Send reaction events to the SQL procedure for persistence."""
        if target_user_id == rater_user_id:
            return
        query = (
            "EXEC dbo.usp_RecordKarmaEvent "
            "@chat_id=?, @msg_id=?, @target_user_id=?, @rater_user_id=?, @emoji=?, @now=?"
        )
        params = (
            chat_id,
            msg_id,
            target_user_id,
            rater_user_id,
            emoji,
            datetime.utcnow(),
        )
        await self.db_manager.execute_query(query, params, commit=True)

    async def handle_text_vote(
        self, giver_id: int, receiver_id: int, chat_id: int, message_text: str
    ) -> Optional[Tuple[str, float]]:
        """Parse a message for karma tokens and apply the change.

        Returns a tuple of (result, base_weight) on success so callers can
        report the delta without reparsing the message.  If the message does
        not contain a valid karma token, returns ``None``.
        """
        parsed = self.parse_vote_token(message_text)
        if not parsed:
            return None
        base, tag = parsed
        result = await self.change_karma(giver_id, receiver_id, chat_id, base, tag)
        return result, base

    async def _get_rater_trust(self, chat_id: int, user_id: int) -> float:
        """Fetch rater trust from the trust_table if present."""
        query = "SELECT trust FROM trust_table WHERE chat_id = ? AND user_id = ?"
        row = await self.db_manager.execute_query(query, (chat_id, user_id), fetch_one=True)
        if row and getattr(row, "trust", None) is not None:
            try:
                return float(row.trust)
            except Exception:
                pass
        return 1.0

    async def change_karma(
        self,
        giver_id: int,
        receiver_id: int,
        chat_id: int,
        base: float,
        tag: Optional[str] = None,
        msg_id: Optional[int] = None,
    ) -> Optional[str]:
        """Apply a karma vote and persist an event with weights."""
        if giver_id == receiver_id:
            return "self_karma_error"  # Key for language file

        # Cooldown check using legacy KarmaLog for now
        cooldown_check_query = """
            SELECT TOP 1 Timestamp FROM KarmaLog
            WHERE GiverUserID = ? AND ReceiverUserID = ? AND ChatID = ?
            ORDER BY Timestamp DESC
        """
        last_karma_time_row = await self.db_manager.execute_query(
            cooldown_check_query, (giver_id, receiver_id, chat_id), fetch_one=True
        )

        if last_karma_time_row and last_karma_time_row[0]:
            last_karma_time = last_karma_time_row[0]
            if datetime.utcnow() < last_karma_time + timedelta(minutes=KARMA_COOLDOWN_MINUTES):
                logger.info(
                    f"Karma cooldown active for {giver_id} -> {receiver_id}."
                )
                return "cooldown_error"

        # Compute weight components
        rater_trust = await self._get_rater_trust(chat_id, giver_id)
        diversity = 1.0
        anti_collusion = 1.0
        novelty = 1.0
        content_factor = 1.0
        weight = base * rater_trust * diversity * anti_collusion * novelty * content_factor

        # Persist to karma_events table
        event_query = """
            INSERT INTO karma_events (
                chat_id, msg_id, target_user_id, rater_user_id, emoji, base,
                rater_trust, diversity, anti_collusion, novelty, content_factor, weight, ts
            ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,SYSUTCDATETIME())
        """
        params = (
            chat_id,
            msg_id,
            receiver_id,
            giver_id,
            tag,
            base,
            rater_trust,
            diversity,
            anti_collusion,
            novelty,
            content_factor,
            weight,
        )
        await self.db_manager.execute_query(event_query, params, commit=True)

        # Legacy KarmaLog for backwards-compatible leaderboards
        points = 1 if weight > 0 else -1
        log_query = (
            "INSERT INTO KarmaLog (ChatID, GiverUserID, ReceiverUserID, Points) VALUES (?, ?, ?, ?)"
        )
        await self.db_manager.execute_query(
            log_query, (chat_id, giver_id, receiver_id, points), commit=True
        )

        receiver_update_query = (
            "UPDATE UserProfiles SET KarmaReceived = ISNULL(KarmaReceived, 0) + ? WHERE UserID = ?"
        )
        await self.db_manager.execute_query(
            receiver_update_query, (points, receiver_id), commit=True
        )

        if points > 0:
            giver_update_query = (
                "UPDATE UserProfiles SET KarmaGiven = ISNULL(KarmaGiven, 0) + 1 WHERE UserID = ?"
            )
        else:
            giver_update_query = (
                "UPDATE UserProfiles SET HateGiven = ISNULL(HateGiven, 0) + 1 WHERE UserID = ?"
            )
        await self.db_manager.execute_query(giver_update_query, (giver_id,), commit=True)

        logger.info(
            f"Karma changed: {giver_id} -> {receiver_id} weight {weight:.2f} in chat {chat_id}."
        )
        return "karma_changed_success"

    async def get_user_stats(self, user_id: int) -> Optional[Dict[str, Any]]:
        """Fetches karma stats for a single user."""
        query = "SELECT FirstName, KarmaReceived, KarmaGiven, HateGiven FROM UserProfiles WHERE UserID = ?"
        row = await self.db_manager.execute_query(query, (user_id,), fetch_one=True)
        if row:
            return {
                "name": row.FirstName,
                "received": row.KarmaReceived or 0,
                "given": row.KarmaGiven or 0,
                "hate": row.HateGiven or 0
            }
        return None

    async def get_top_users(self, chat_id: int, limit: int = 10) -> List[Dict[str, Any]]:
        """Gets the karma leaderboard for a specific chat."""
        # This query joins UserProfiles with ChatLog to find users active in the specific chat.
        query = f"""
            SELECT TOP (?) up.FirstName, up.KarmaReceived
            FROM UserProfiles up
            WHERE up.UserID IN (SELECT DISTINCT UserID FROM ChatLog WHERE ChatID = ?)
            ORDER BY up.KarmaReceived DESC
        """
        rows = await self.db_manager.execute_query(query, (limit, chat_id), fetch_all=True)
        return [{"name": row.FirstName, "score": row.KarmaReceived or 0} for row in rows] if rows else []

    async def get_top_givers(self, chat_id: int, limit: int = 10) -> List[Dict[str, Any]]:
        """Gets the top karma givers leaderboard for a specific chat."""
        query = f"""
            SELECT TOP (?) up.FirstName, up.KarmaGiven, up.HateGiven
            FROM UserProfiles up
            WHERE up.UserID IN (SELECT DISTINCT UserID FROM ChatLog WHERE ChatID = ?)
            ORDER BY (up.KarmaGiven + up.HateGiven) DESC
        """
        rows = await self.db_manager.execute_query(query, (limit, chat_id), fetch_all=True)
        return [{"name": row.FirstName, "given": row.KarmaGiven or 0, "hate": row.HateGiven or 0} for row in rows] if rows else []



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/local_model_manager.py ---
======================================================================

# enkibot/modules/local_model_manager.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Thin wrapper around :mod:`llama_cpp` for loading local models.

The project historically relied on hosted LLM providers.  For users who want
fully local inference we provide :class:`LocalModelManager` which manages two
GGUF models – a *fast* one (7B/8B) and a *deep* one (70B/72B).  Both models are
loaded lazily to keep start‑up time low and to conserve memory.

Example
-------
>>> from enkibot.modules.local_model_manager import LocalModelManager
>>> manager = LocalModelManager(
...     fast_model_path="mistral-7b-instruct.Q5_K_M.gguf",
...     deep_model_path="llama-3-70b-instruct.Q4_K_M.gguf",
... )
>>> text = manager.generate("Write a haiku about modular bots", model="fast")
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Optional
import logging

try:  # llama_cpp is an optional dependency
    from llama_cpp import Llama
except Exception:  # pragma: no cover - library not always available in CI
    Llama = None  # type: ignore

logger = logging.getLogger(__name__)


@dataclass
class ModelConfig:
    """Configuration for a single local model."""

    model_path: str
    n_ctx: int = 4096
    n_threads: int = 8


class LocalModelManager:
    """Load and run local GGUF models via :mod:`llama_cpp`.

    Parameters
    ----------
    fast_model_cfg:
        Configuration for the smaller, faster model (e.g. Mistral‑7B or
        Llama‑3‑8B).
    deep_model_cfg:
        Configuration for the larger, higher quality model (e.g. Llama‑3‑70B or
        Qwen‑2‑72B).
    """

    def __init__(self, fast_model_cfg: ModelConfig, deep_model_cfg: ModelConfig):
        self.fast_cfg = fast_model_cfg
        self.deep_cfg = deep_model_cfg
        self._fast: Optional[Llama] = None
        self._deep: Optional[Llama] = None

    # ------------------------------------------------------------------
    # Loading helpers
    def _load_fast(self) -> Llama:
        if self._fast is None:
            if Llama is None:
                raise RuntimeError("llama_cpp is not installed")
            logger.info("Loading fast model from %s", self.fast_cfg.model_path)
            self._fast = Llama(
                model_path=self.fast_cfg.model_path,
                n_ctx=self.fast_cfg.n_ctx,
                n_threads=self.fast_cfg.n_threads,
            )
        return self._fast

    def _load_deep(self) -> Llama:
        if self._deep is None:
            if Llama is None:
                raise RuntimeError("llama_cpp is not installed")
            logger.info("Loading deep model from %s", self.deep_cfg.model_path)
            self._deep = Llama(
                model_path=self.deep_cfg.model_path,
                n_ctx=self.deep_cfg.n_ctx,
                n_threads=self.deep_cfg.n_threads,
            )
        return self._deep

    # ------------------------------------------------------------------
    # Public API
    def generate(self, prompt: str, model: str = "fast", max_tokens: int = 512,
                 temperature: float = 0.7) -> str:
        """Generate text using the requested model.

        Parameters
        ----------
        prompt:
            User prompt.
        model:
            ``"fast"`` or ``"deep"``.
        max_tokens, temperature:
            Passed to :class:`llama_cpp.Llama`.
        """

        llm = self._load_fast() if model == "fast" else self._load_deep()
        template = f"<<SYS>>You are a helpful assistant.<</SYS>>\n[INST]{prompt}[/INST]"
        result = llm(
            template,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["</s>"],
        )
        return result["choices"][0]["text"]



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/model_router.py ---
======================================================================

# enkibot/modules/model_router.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Routing logic for selecting between fast and deep local models."""
from __future__ import annotations

from dataclasses import dataclass
import logging
import re

from .local_model_manager import LocalModelManager

logger = logging.getLogger(__name__)


@dataclass
class RoutingDecision:
    model: str
    clean_prompt: str
    use_web: bool = False


class ModelRouter:
    """Simple heuristic router for two‑tier local models."""

    def __init__(self, manager: LocalModelManager):
        self.manager = manager

    # ------------------------------------------------------------------
    def _needs_deep_model(self, prompt: str) -> bool:
        if prompt.startswith("/deep"):
            return True
        if len(prompt) > 400 or len(prompt.split()) > 120:
            return True
        return False

    def _needs_web(self, prompt: str) -> bool:
        triggers = ["http://", "https://", "www", "today", "current", "latest"]
        lowered = prompt.lower()
        return any(t in lowered for t in triggers) or prompt.startswith("/web")

    def route(self, prompt: str) -> RoutingDecision:
        use_web = self._needs_web(prompt)
        model = "deep" if self._needs_deep_model(prompt) else "fast"
        clean = re.sub(r"^/(deep|fast|web)\s*", "", prompt).strip()
        logger.debug("Routing prompt to %s model (web=%s)", model, use_web)
        return RoutingDecision(model=model, clean_prompt=clean, use_web=use_web)

    # ------------------------------------------------------------------
    def generate(self, prompt: str, **kwargs) -> str:
        """Convenience wrapper that performs routing and generation."""
        decision = self.route(prompt)
        return self.manager.generate(decision.clean_prompt, model=decision.model, **kwargs)



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/primary_source_hunter.py ---
======================================================================

# enkibot/modules/primary_source_hunter.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
"""Minimal primary source hunter.

This module performs a very small subset of the primary‑source hunting
behaviour described in the project design.  It expands the user query with a
few synonyms and searches official domains first, falling back to wire
services and reputable outlets.  The real system would include more
sophisticated language handling, timestamp extraction and archiving.
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List, Dict

import json
import logging
import openai
from .. import config

logger = logging.getLogger(__name__)


@dataclass
class SourceHit:
    """Lightweight representation of a search hit."""

    url: str
    title: str
    domain: str
    tier: int  # 1=primary, 2=wire, 3=reputable


class PrimarySourceHunter:
    """Searches for primary sources before any summarisation occurs."""

    def __init__(self, max_results: int = 5) -> None:
        self.max_results = max_results
        self.client: openai.AsyncOpenAI | None = None
        if config.OPENAI_API_KEY:
            self.client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        self.model_id = config.OPENAI_DEEP_RESEARCH_MODEL_ID

        self.official_domains = [
            "gov.uk",
            "eeas.europa.eu",
            "idf.il",
            "mod.gov",
            "mfa.gov",
        ]
        self.wire_domains = ["reuters.com", "apnews.com", "afp.com"]
        self.reputable_domains = [
            "bbc.com",
            "ft.com",
            "nytimes.com",
            "washingtonpost.com",
            "theguardian.com",
        ]

        self.synonyms: Dict[str, List[str]] = {
            "joint statement": ["communiqué", "совместное заявление", "заявление"],
            "line of contact": [
                "line of control",
                "front line",
                "текущая линия соприкосновения",
            ],
            "ceasefire brief": [
                "briefing",
                "заявление пресс-секретаря",
                "брифинг",
            ],
        }

    def expand_queries(self, text: str, langs: List[str]) -> List[str]:
        """Expand the query with simple synonym replacements."""
        queries = [text]
        tl = text.lower()
        for phrase, alts in self.synonyms.items():
            if phrase in tl:
                for alt in alts:
                    queries.append(tl.replace(phrase, alt))
        # In a production system translations would be added here.  We simply
        # deduplicate.
        seen = set()
        expanded: List[str] = []
        for q in queries:
            if q not in seen:
                expanded.append(q)
                seen.add(q)
        return expanded

    async def _web_search(self, query: str) -> List[SourceHit]:
        """Use OpenAI's web search tool to find sources."""
        if not self.client:
            return []
        extra: Dict[str, object] = {}
        if config.OPENAI_SEARCH_CONTEXT_SIZE:
            extra["search_context_size"] = config.OPENAI_SEARCH_CONTEXT_SIZE
        if config.OPENAI_SEARCH_USER_LOCATION:
            try:
                extra["user_location"] = json.loads(
                    config.OPENAI_SEARCH_USER_LOCATION
                )
            except Exception:
                extra["user_location"] = {
                    "country": config.OPENAI_SEARCH_USER_LOCATION
                }
        try:
            resp = await self.client.responses.create(
                model=self.model_id,
                tools=[{"type": "web_search"}],
                instructions=(
                    "You are a primary-source hunter. Always include 3-6 sources (at least 1 primary). "
                    "Return ONLY a JSON array named 'items' of objects {url, title}."
                ),
                input=query,
                **extra,
            )
            text = (getattr(resp, "output_text", "") or "").strip()
            data = json.loads(text) if text.startswith("{") else {"items": []}
            items = data.get("items", [])
        except Exception as exc:
            logger.warning("Web search failed: %s", exc)
            return []
        hits: List[SourceHit] = []
        for item in items:
            url = item.get("url", "")
            title = item.get("title", "")
            if not url:
                continue
            domain = url.split("/")[2] if "//" in url else url
            tier = 4
            if any(domain.endswith(d) for d in self.official_domains):
                tier = 1
            elif any(domain.endswith(d) for d in self.wire_domains):
                tier = 2
            elif any(domain.endswith(d) for d in self.reputable_domains):
                tier = 3
            hits.append(SourceHit(url=url, title=title, domain=domain, tier=tier))
        return hits

    async def hunt(self, claim_text: str, lang: str | None = None) -> List[SourceHit]:
        """Return a list of source hits prioritising official domains."""
        langs = [lang] if lang else []
        if "en" not in langs:
            langs.append("en")
        queries = self.expand_queries(claim_text, langs)
        hits: List[SourceHit] = []
        for q in queries:
            results = await self._web_search(q)
            for h in results:
                hits.append(h)
                if len(hits) >= self.max_results:
                    return hits
            if hits:
                break
        return hits



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/profile_manager.py ---
======================================================================

# enkibot/modules/profile_manager.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Profile Manager ===
# ==================================================================================================
# Manages user psychological profiles and name variations using LLMs and database interaction.
# ==================================================================================================
import logging
import json
from typing import Optional, Dict, TYPE_CHECKING

from enkibot.utils.database import DatabaseManager 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices # For type hinting

logger = logging.getLogger(__name__)

class ProfileManager:
    def __init__(self, llm_services: 'LLMServices', db_manager: DatabaseManager):
        self.llm_services = llm_services # Type hint is string literal
        self.db_manager = db_manager
        self.MAX_PROFILE_SIZE = 4000

    async def populate_name_variations_with_llm(self, user_id: int, first_name: Optional[str], 
                                                last_name: Optional[str], username: Optional[str],
                                                system_prompt: str, user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"): # Check specific provider
            logger.warning(f"Name variation for user {user_id} skipped: OpenAI not configured in LLMServices.")
            return

        name_parts = [part for part in [first_name, last_name, username] if part and str(part).strip()]
        if not name_parts:
            logger.info(f"No valid name parts for user {user_id}, skipping name variation.")
            return
            
        name_info = ", ".join(name_parts)
        logger.info(f"Requesting name variations for user {user_id} ({name_info}).")
        user_prompt = user_prompt_template.format(name_info=name_info)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        name_variations = set([p.lower() for p in name_parts])
        
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages, temperature=0.3, response_format={"type": "json_object"}
            ) # Removed model_id, should be handled by call_openai_llm default
            if completion_str:
                data = json.loads(completion_str)
                variations_list = data.get('variations')
                if isinstance(variations_list, list):
                    name_variations.update([str(v).lower().strip() for v in variations_list if v and str(v).strip()])
                    logger.info(f"LLM got {len(variations_list)} raw variations. Total unique: {len(name_variations)} for user {user_id}.")
                else:
                    logger.warning(f"LLM name variations for {user_id} no 'variations' list. Resp: {completion_str[:200]}")
            else:
                logger.warning(f"LLM no content for name variations for user {user_id}.")
        except Exception as e:
            logger.error(f"LLM name variation error (user {user_id}): {e}", exc_info=True)

        if name_variations:
            await self.db_manager.save_user_name_variations(user_id, list(name_variations))
        else:
            logger.info(f"No name variations to save for user {user_id}.")

    async def analyze_and_update_user_profile(self, user_id: int, message_text: str,
                                              create_system_prompt: str, create_user_prompt_template: str,
                                              update_system_prompt: str, update_user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"):
            logger.warning(f"Profiling for {user_id} skipped: OpenAI not configured.")
            return
        logger.info(f"Starting/Updating profile analysis for user {user_id}...")
        current_notes = await self.db_manager.get_user_profile_notes(user_id)
        sys_prompt, user_prompt = "", ""

        if not current_notes:
            logger.info(f"No profile for {user_id}. Creating new.")
            sys_prompt, user_prompt = create_system_prompt, create_user_prompt_template.format(message_text=message_text)
        else:
            logger.info(f"Existing profile for {user_id}. Updating.")
            sys_prompt, user_prompt = update_system_prompt, update_user_prompt_template.format(
                current_profile_notes=current_notes, message_text=message_text)
        
        messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}]
        updated_notes_str: Optional[str] = None
        try:
            updated_notes_str = await self.llm_services.call_openai_llm(messages, temperature=0.5, max_tokens=1000)
        except Exception as e:
            logger.error(f"LLM profile analysis error for {user_id}: {e}", exc_info=True)

        if updated_notes_str and updated_notes_str.strip():
            await self.db_manager.update_user_profile_notes(user_id, updated_notes_str.strip()[:self.MAX_PROFILE_SIZE])
            logger.info(f"Profile updated for {user_id}.")
        else:
            logger.warning(f"Profile analysis no content for {user_id}.")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/rag_service.py ---
======================================================================

# enkibot/modules/rag_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Very small Retrieval-Augmented Generation helper using FAISS."""

from __future__ import annotations

from typing import List, Tuple, Optional
import logging

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

from enkibot import config
from enkibot.core.llm_services import LLMServices

logger = logging.getLogger(__name__)


class RAGService:
    """Vector search over a set of documents with optional OpenAI embeddings."""

    def __init__(
        self,
        llm_services: Optional[LLMServices] = None,
        embedding_model: Optional[str] = None,
    ):
        self.llm_services = llm_services
        self.use_openai = bool(
            llm_services and llm_services.is_provider_configured("openai")
        )
        self.embedding_model = (
            embedding_model
            or (
                config.OPENAI_EMBEDDING_MODEL_ID
                if self.use_openai
                else "intfloat/e5-small-v2"
            )
        )
        self.encoder: Optional[SentenceTransformer] = None
        if not self.use_openai:
            self.encoder = SentenceTransformer(self.embedding_model)
        self.index: Optional[faiss.IndexFlatL2] = None
        self.documents: List[str] = []

    async def _ensure_index(self, dim: int) -> None:
        if self.index is None:
            self.index = faiss.IndexFlatL2(dim)

    async def _embed(self, texts: List[str]) -> Optional[List[List[float]]]:
        if self.use_openai and self.llm_services:
            return await self.llm_services.embed_texts_openai(
                texts, model_id=self.embedding_model
            )
        if self.encoder:
            return self.encoder.encode(texts).tolist()
        return None

    async def add_documents(self, docs: List[str]) -> None:
        """Add raw documents to the index."""
        embeddings = await self._embed(docs)
        if not embeddings:
            return
        await self._ensure_index(len(embeddings[0]))
        self.index.add(np.array(embeddings, dtype="float32"))
        self.documents.extend(docs)
        logger.info("Indexed %d documents", len(docs))

    async def query(self, question: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """Return the top ``top_k`` documents most relevant to ``question``."""
        if not self.documents:
            return []
        emb = await self._embed([question])
        if not emb or self.index is None:
            return []
        scores, ids = self.index.search(np.array(emb, dtype="float32"), top_k)
        results: List[Tuple[str, float]] = []
        for idx, score in zip(ids[0], scores[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(score)))
        return results




======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/response_generator.py ---
======================================================================

# enkibot/modules/response_generator.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Response Generator ===
# ==================================================================================================
# enkibot/modules/intent_recognizer.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Your GPLv3 Header)

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
# print(f"%%%%% EXECUTING INTENT_RECOGNIZER.PY - VERSION FROM: {__file__} %%%%%") # You can uncomment this if you still face import issues
# enkibot/modules/response_generator.py
# (Your GPLv3 Header)

import logging
import json
from typing import List, Dict, Any, Optional, TYPE_CHECKING

from telegram.ext import ContextTypes 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices
    from enkibot.utils.database import DatabaseManager
    from enkibot.modules.intent_recognizer import IntentRecognizer 

from enkibot.modules.fact_extractor import find_user_search_query_in_text 
from enkibot import config

logger = logging.getLogger(__name__)

class ResponseGenerator:
    def __init__(self, 
                 llm_services: 'LLMServices', 
                 db_manager: 'DatabaseManager', 
                 intent_recognizer: 'IntentRecognizer'):
        logger.info("ResponseGenerator __init__ STARTING")
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.intent_recognizer = intent_recognizer 
        logger.info("ResponseGenerator __init__ COMPLETED")

    # ... (analyze_replied_message, compile_weather_forecast_response, compile_news_response, get_orchestrated_llm_response methods remain the same) ...
    async def analyze_replied_message(self, original_text: str, user_question: str, system_prompt: str, user_prompt_template: Optional[str]) -> str:
        logger.info(f"ResponseGenerator: Analyzing replied message. Length: {len(original_text)}, Q: '{user_question}'")
        user_prompt = (user_prompt_template or "Original Text:\n---\n{original_text}\n---\n\nUser's Question:\n---\n{user_question}\n---\n\nYour Analysis:").format(
            original_text=original_text, user_question=user_question )
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        try:
            analysis_model_id = config.OPENAI_MODEL_ID 
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(messages_for_api, model_id=analysis_model_id, temperature=0.5, max_tokens=1000)
                if completion: return completion.strip()
            return "Analysis client not configured or error in API call."
        except Exception as e:
            logger.error(f"Error in LLM call during replied message analysis: {e}", exc_info=True)
            return "Sorry, an error occurred during the text analysis."

    async def fact_check_forwarded_message(
        self,
        forwarded_text: str,
        user_question: str,
        system_prompt: str,
        user_prompt_template: Optional[str],
        fallback_text: str = "I couldn't verify that message right now.",
    ) -> str:
        """Performs a fact-check on a forwarded message using LLM deep-research capabilities."""
        logger.info(
            "ResponseGenerator: Fact-checking forwarded message. Length: %d, Q: '%s'",
            len(forwarded_text), user_question
        )
        user_prompt = (
            user_prompt_template or
            "Forwarded Text:\n---\n{forwarded_text}\n---\n\nUser's Question:\n---\n{user_question}\n---\n\nYour Fact-Check:"
        ).format(forwarded_text=forwarded_text, user_question=user_question)
        extra_instruction = (
            "Respond in the same language as the forwarded text. Use web search in that language "
            "to verify the claims and cite at least one reliable source (preferably three) with URLs."
        )
        messages_for_api = [
            {"role": "system", "content": f"{system_prompt.strip()} {extra_instruction}"},
            {"role": "user", "content": user_prompt},
        ]
        try:
            if self.llm_services.is_provider_configured("openai"):
                fact_check_response = await self.llm_services.call_openai_deep_research(
                    messages_for_api, max_output_tokens=1000
                )
            else:
                fact_check_response = await self.llm_services.race_llm_calls(messages_for_api)
            return fact_check_response or fallback_text
        except Exception as e:
            logger.error(
                "Error in LLM call during forwarded message fact-check: %s", e, exc_info=True
            )
            return fallback_text

    async def compile_weather_forecast_response(self, forecast_data_structured: Dict[str, Any], lang_code: str, system_prompt: str, user_prompt_template: str) -> str:
        location = forecast_data_structured.get("location", "the requested location")
        forecast_data_json_str = json.dumps(forecast_data_structured.get("forecast_days", []), indent=2, ensure_ascii=False)
        user_prompt = user_prompt_template.format(location=location, forecast_data_json=forecast_data_json_str)
        language_name_for_prompt = lang_code 
        try:
            from babel import Locale
            locale = Locale.parse(lang_code)
            language_name_for_prompt = locale.get_display_name('en') 
        except Exception: pass
        if "{location}" in system_prompt or "{language_name}" in system_prompt:
            system_prompt = system_prompt.format(location=location, language_name=language_name_for_prompt)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        logger.info(f"Requesting LLM to compile weather forecast for {location} in {lang_code}")
        compiled_response = await self.llm_services.race_llm_calls(messages)
        return compiled_response or "I found some weather data, but couldn't summarize it for you right now."

    async def compile_news_response(self, articles_structured: List[Dict[str, Any]], topic: Optional[str], lang_code: str, system_prompt: str, user_prompt_template: str) -> str:
        articles_json_str = json.dumps(articles_structured, indent=2, ensure_ascii=False)
        display_topic = topic if topic else "general interest"
        user_prompt = user_prompt_template.format(topic=display_topic, articles_json=articles_json_str)
        language_name_for_prompt = lang_code
        try:
            from babel import Locale
            locale = Locale.parse(lang_code)
            language_name_for_prompt = locale.get_display_name('en')
        except Exception: pass
        if "{topic}" in system_prompt or "{language_name}" in system_prompt:
            system_prompt = system_prompt.format(topic=display_topic, language_name=language_name_for_prompt)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        logger.info(f"Requesting LLM to compile news for topic '{display_topic}' in {lang_code}")
        compiled_response = await self.llm_services.race_llm_calls(messages)
        return compiled_response or "I found some news articles, but couldn't summarize them for you at the moment."

    async def get_orchestrated_llm_response(self, prompt_text: str, chat_id: int, user_id: int, message_id: int, context: ContextTypes.DEFAULT_TYPE, lang_code: str, system_prompt_override: str, user_search_ambiguous_response_template: str, user_search_not_found_response_template: str) -> str:
        logger.info(f"Orchestrating LLM response for: '{prompt_text[:100]}...' in chat {chat_id} (lang: {lang_code})")
        history_from_db: List[Dict[str, str]] = []
        profile_context_messages: List[Dict[str, str]] = []
        keyword_context_messages: List[Dict[str, str]] = [] 
        history_from_db = await self.db_manager.get_conversation_history(chat_id, limit=10)
        search_term_original = find_user_search_query_in_text(prompt_text) 
        if search_term_original:
            logger.info(f"User profile query detected for term: '{search_term_original}'")
            matched_profiles_data = await self.db_manager.find_user_profiles_by_name_variation(search_term_original)
            if len(matched_profiles_data) == 1:
                profile_data = matched_profiles_data[0]
                target_user_id_found = profile_data.get("UserID") 
                user_identifier = profile_data.get("FirstName") or profile_data.get("Username") or f"User ID {target_user_id_found}"
                if profile_data.get("Notes"): 
                    profile_context_messages.append({"role": "system", "content": f"Important Context (User Dossier) for '{user_identifier}':\n---\n{profile_data['Notes']}\n---"})
                if target_user_id_found: 
                    user_specific_messages = await self.db_manager.get_user_messages_from_chat_log(target_user_id_found, chat_id, limit=5)
                    if user_specific_messages:
                        valid_user_messages = [msg for msg in user_specific_messages if isinstance(msg, str) and msg.strip()]
                        if valid_user_messages:
                            formatted_msgs = "\n".join([f'- "{msg_text}"' for msg_text in valid_user_messages])
                            keyword_context_messages.append({"role": "system", "content": f"Additional Context (recent raw messages) from '{user_identifier}'. Use with dossier for freshest insights:\n---\n{formatted_msgs}\n---"})
            elif len(matched_profiles_data) > 1:
                user_options = [ f"@{p.get('Username')}" if p.get('Username') else f"{p.get('FirstName') or ''} {p.get('LastName') or ''}".strip() for p in matched_profiles_data ]
                user_options_str = ", ".join(filter(None, user_options))
                return user_search_ambiguous_response_template.format(user_options=user_options_str)
        messages_for_api = []
        if system_prompt_override and isinstance(system_prompt_override, str):
            messages_for_api.append({"role": "system", "content": system_prompt_override})
        else:
            logger.error(f"system_prompt_override is invalid or None: {system_prompt_override}. Using a default.")
            messages_for_api.append({"role": "system", "content": "You are a helpful AI assistant."})
        messages_for_api.extend(profile_context_messages) 
        messages_for_api.extend(keyword_context_messages) 
        messages_for_api.extend(history_from_db) 
        if prompt_text and isinstance(prompt_text, str):
            messages_for_api.append({"role": "user", "content": prompt_text})
        else:
            logger.error(f"prompt_text is invalid or None: {prompt_text}. Appending a placeholder.")
            messages_for_api.append({"role": "user", "content": "Please provide a general response."})
        MAX_CONTEXT_MSGS = 20 
        if len(messages_for_api) > MAX_CONTEXT_MSGS:
            system_prompts_initial = [m for m in messages_for_api if m.get("role") == "system"]
            other_messages = [m for m in messages_for_api if m.get("role") != "system"]
            num_other_to_keep = MAX_CONTEXT_MSGS - len(system_prompts_initial)
            if num_other_to_keep < 1: num_other_to_keep = 1 
            final_messages_for_api = system_prompts_initial + other_messages[-num_other_to_keep:]
            logger.info(f"Message context truncated from {len(messages_for_api)} to {len(final_messages_for_api)} messages.")
            messages_for_api = final_messages_for_api
        final_reply = await self.llm_services.race_llm_calls(messages_for_api)
        if not final_reply:
            final_reply = "I'm currently unable to get a response from my core AI. Please try again shortly."
            logger.error("All LLM providers failed in race_llm_calls. Final_reply set to fallback.")
        await self.db_manager.save_to_conversation_history(chat_id, user_id, message_id, 'user', prompt_text)
        if context.bot: 
            await self.db_manager.save_to_conversation_history(chat_id, context.bot.id, None, 'assistant', final_reply)
        return final_reply

    # --- NEW METHOD FOR TRANSLATION ---
    async def translate_text(self, 
                             text_to_translate: str, 
                             target_language: str,
                             system_prompt: str,
                             user_prompt_template: str) -> Optional[str]:
        """
        Uses an LLM to translate text to a specified target language.
        """
        logger.info(f"Requesting translation of text to '{target_language}': '{text_to_translate[:70]}...'")
        
        # Format the prompts with the target language and text
        system_prompt = system_prompt.format(target_language=target_language)
        user_prompt = user_prompt_template.format(text_to_translate=text_to_translate)
        
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        # Use a specific translation model or race general ones
        translation_model_id = self.llm_services.openai_translation_model_id
        
        try:
            # For translation, a single high-quality model is often better than racing
            if self.llm_services.is_provider_configured("openai"):
                translated_text = await self.llm_services.call_openai_llm(
                    messages,
                    model_id=translation_model_id,
                    temperature=0.1 # Low temperature for more literal translation
                )
                if translated_text:
                    logger.info("Translation successful.")
                    return translated_text
            else: # Fallback to racing if OpenAI is not available
                logger.warning("OpenAI not configured for translation, falling back to racing general models.")
                translated_text = await self.llm_services.race_llm_calls(messages)
                if translated_text:
                    logger.info("Translation successful via race.")
                    return translated_text

        except Exception as e:
            logger.error(f"An error occurred during translation: {e}", exc_info=True)
        
        logger.error("Translation failed for all providers.")
        return None



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/spam_detector.py ---
======================================================================

# enkibot/modules/spam_detector.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Advanced spam detection with a Zero-Trust approach."""

from __future__ import annotations

import logging
import re
from time import time
from typing import Any, Callable, Dict, Optional

import tldextract
from telegram import ChatPermissions, Update
from telegram.ext import ContextTypes

from enkibot import config as bot_config
from enkibot.modules.base_module import BaseModule
from enkibot.core.llm_services import LLMServices
from enkibot.core.language_service import LanguageService
from enkibot.utils.database import DatabaseManager

logger = logging.getLogger(__name__)


class SpamDetector(BaseModule):
    """Detects spam or disallowed content and takes moderation actions."""

    def __init__(
        self,
        llm_services: LLMServices,
        db_manager: Optional[DatabaseManager] = None,
        language_service: Optional[LanguageService] = None,
        enabled: bool = True,
    ) -> None:
        super().__init__("SpamDetector")
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.language_service = language_service
        self.enabled = enabled
        self.captcha_callback: Optional[Callable[[Any, int, ContextTypes.DEFAULT_TYPE], Any]] = None
        # Track per-user state across chats: joined timestamp, verification and clean message count
        self.user_states: Dict[tuple[int, int], Dict[str, Any]] = {}
        logger.info("SpamDetector initialized. Enabled=%s", self.enabled)

    # ------------------------------------------------------------------
    # Configuration helpers
    def set_captcha_callback(self, callback: Callable[[Any, int, ContextTypes.DEFAULT_TYPE], Any]) -> None:
        """Allows external components to provide a captcha starter."""
        self.captcha_callback = callback

    def _is_new_or_unverified(self, state: Dict[str, Any]) -> bool:
        cfg = bot_config.ZERO_TRUST_SETTINGS
        if not state:
            return True
        if state.get("verified"):
            return False
        if time() - state.get("joined_ts", 0) <= cfg["watch_new_user_window_sec"]:
            return True
        return state.get("clean_msgs", 0) < cfg["watch_first_messages"]

    def _apply_heuristics(self, text: str) -> float:
        cfg = bot_config.ZERO_TRUST_SETTINGS
        heur = cfg["heuristics"]
        lists = cfg["lists"]
        risk = 0.0
        t = text or ""
        urls = re.findall(r"https?://\S+|t\.me/\S+|\S+\.\S+", t, flags=re.I)
        if urls:
            risk += heur["url_in_first_msg"]
            for u in urls:
                dom = tldextract.extract(u)
                fqdn = ".".join(part for part in [dom.subdomain, dom.domain, dom.suffix] if part)
                if fqdn in lists["domain_blocklist"]:
                    risk += 0.10
        if len(re.findall(r"@\w{3,}", t)) >= 3:
            risk += heur["many_mentions"]
        letters = re.findall(r"[A-Za-z]", t)
        if len(t) > 40 and letters:
            caps_ratio = sum(1 for c in letters if c.isupper()) / len(letters)
            if caps_ratio >= 0.8:
                risk += heur["all_caps_long"]
        if re.search(r"(.)\1{3,}", t) or re.search(r"([!?$€£¥]){4,}", t):
            risk += heur["repeated_chars"]
        low = t.lower()
        if any(k in low for k in lists["keyword_blocklist"]):
            risk += heur["keyword_hits"]
        return min(risk, 1.0)

    def _combined_risk(self, spamish: float, heur_score: float, state: Dict[str, Any]) -> float:
        base = max(spamish, heur_score)
        if self._is_new_or_unverified(state):
            base = min(base + 0.10, 1.0)
        return base

    async def _log_action(
        self,
        context: ContextTypes.DEFAULT_TYPE,
        chat_id: int,
        user,
        text: str,
        risk: float,
        scores: Dict[str, float],
        action: str,
    ) -> None:
        cfg = bot_config.ZERO_TRUST_SETTINGS
        admin_chat_id = cfg["logging"].get("admin_chat_id")
        if not admin_chat_id:
            return
        short = (text[:200] + "…") if text and len(text) > 200 else (text or "")
        try:
            await context.bot.send_message(
                admin_chat_id,
                (
                    f"🚫 Zero-Trust {action}\n"
                    f"• Chat: {chat_id}\n"
                    f"• User: @{getattr(user, 'username', user.id)}\n"
                    f"• Risk: {risk:.2f}\n"
                    f"• Scores: {scores}\n"
                    f"• Excerpt: {short}"
                ),
                disable_web_page_preview=True,
            )
        except Exception:
            pass

    # ------------------------------------------------------------------
    async def inspect_message(
        self, update: Update, context: ContextTypes.DEFAULT_TYPE
    ) -> bool:
        """Checks a message and removes it if deemed risky.

        Returns ``True`` if a message was flagged and handled
        (deleted/banned/muted)."""
        if not self.enabled:
            return False
        if not update.message or not update.message.text:
            return False
        user = update.effective_user
        if not user or user.is_bot:
            return False

        chat_id = update.effective_chat.id
        text = update.message.text
        if self.language_service:
            try:
                detected_lang = await self.language_service.determine_language_context(text, chat_id, update)
                logger.debug("Detected message language: %s", detected_lang)
            except Exception as e:
                logger.error("Language detection failed: %s", e)

        # Skip moderation for messages explicitly addressed to the bot
        text_lower = text.lower()
        bot_username_lower = (
            getattr(context.bot, "username", "").lower() if getattr(context.bot, "username", None) else ""
        )
        tokens = re.findall(r"\w+", text_lower, flags=re.UNICODE)
        is_direct_query = False
        if bot_username_lower and f"@{bot_username_lower}" in text_lower:
            is_direct_query = True
        elif any(nick.lower() in tokens for nick in bot_config.BOT_NICKNAMES_TO_CHECK):
            is_direct_query = True
        if is_direct_query:
            return False
        key = (chat_id, user.id)
        state = self.user_states.get(key)
        if state is None:
            state = {"joined_ts": time(), "verified": False, "clean_msgs": 0}
            self.user_states[key] = state

        heur_score = self._apply_heuristics(text)
        moderation_result = await self.llm_services.moderate_text_openai(text)
        scores_obj = moderation_result.get("category_scores", {}) if moderation_result else {}
        if hasattr(scores_obj, "model_dump"):
            scores = scores_obj.model_dump()
        elif hasattr(scores_obj, "dict"):
            scores = scores_obj.dict()
        elif isinstance(scores_obj, dict):
            scores = scores_obj
        else:
            scores = {}
        spamish = max(scores.values()) if scores else 0.0
        risk = self._combined_risk(spamish, heur_score, state)
        if moderation_result and moderation_result.get("flagged"):
            risk = max(risk, bot_config.ZERO_TRUST_SETTINGS["global_thresholds"]["delete"])

        thresholds = bot_config.ZERO_TRUST_SETTINGS["global_thresholds"]
        action: Optional[str] = None
        if risk >= thresholds["ban"]:
            action = "banned"
        elif risk >= thresholds["mute_then_captcha"]:
            action = "muted_captcha"
        elif risk >= thresholds["delete"]:
            action = "deleted"

        if action:
            message_id = update.message.message_id
            try:
                await context.bot.delete_message(chat_id=chat_id, message_id=message_id)
            except Exception as e:
                logger.error("Failed to delete message %s in chat %s: %s", message_id, chat_id, e)

            if self.db_manager and self.db_manager.connection_string and moderation_result:
                try:
                    categories_obj = moderation_result.get("categories", {})
                    if hasattr(categories_obj, "model_dump"):
                        categories = categories_obj.model_dump()
                    elif hasattr(categories_obj, "dict"):
                        categories = categories_obj.dict()
                    elif isinstance(categories_obj, dict):
                        categories = categories_obj
                    else:
                        categories = {}
                    flagged = ", ".join([k for k, v in categories.items() if v])
                    await self.db_manager.log_moderation_action(
                        chat_id=chat_id,
                        user_id=user.id,
                        message_id=message_id,
                        categories=flagged or None,
                    )
                except Exception as e:  # pragma: no cover
                    logger.error("Failed to log moderation action: %s", e, exc_info=True)

            if action == "banned":
                try:
                    await context.bot.ban_chat_member(chat_id=chat_id, user_id=user.id)
                except Exception as e:
                    logger.error("Failed to ban user %s in chat %s: %s", user.id, chat_id, e)
            elif action == "muted_captcha":
                try:
                    perms = ChatPermissions(can_send_messages=False)
                    await context.bot.restrict_chat_member(chat_id, user.id, permissions=perms)
                except Exception as e:
                    logger.error("Failed to restrict user %s in chat %s: %s", user.id, chat_id, e)
                if self.captcha_callback:
                    try:
                        await self.captcha_callback(user, chat_id, context)
                    except Exception as e:
                        logger.error("Failed to start captcha for %s: %s", user.id, e)

            await self._log_action(context, chat_id, user, text, risk, scores, action)
            return True

        # Message deemed clean -> update state
        state["clean_msgs"] = state.get("clean_msgs", 0) + 1
        if state["clean_msgs"] >= bot_config.ZERO_TRUST_SETTINGS["watch_first_messages"]:
            state["verified"] = True
        return False



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/stats_manager.py ---
======================================================================

# enkibot/modules/stats_manager.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Track additional metrics such as hourly activity histograms.
# - Provide graphical representations of statistics.
# - Expand unit tests for edge cases and larger datasets.
# -------------------------------------------------------------------------------
"""Chat statistics tracking and retrieval."""

import logging
import re
from collections import defaultdict
from datetime import datetime
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse

from enkibot.utils.database import DatabaseManager

logger = logging.getLogger(__name__)

class StatsManager:
    """Manage chat and user statistics with optional persistence."""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.memory_stats: Dict[int, Dict[str, Any]] = defaultdict(
            lambda: {
                "total_messages": 0,
                "joins": 0,
                "leaves": 0,
                "users": defaultdict(
                    lambda: {
                        "count": 0,
                        "first_seen": None,
                        "last_active": None,
                        "username": None,
                    }
                ),
                "links": defaultdict(int),
            }
        )

    async def log_message(
        self, chat_id: int, user_id: int, message_text: Optional[str], username: Optional[str] = None
    ):
        """Increment counters for a new message."""
        if self.db_manager.connection_string:
            await self.db_manager.execute_query(
                """
                MERGE ChatStats AS t
                USING (VALUES (?)) AS s(ChatID)
                ON t.ChatID = s.ChatID
                WHEN MATCHED THEN
                    UPDATE SET TotalMessages = t.TotalMessages + 1, LastUpdated = GETDATE()
                WHEN NOT MATCHED THEN
                    INSERT (ChatID, TotalMessages, JoinCount, LeaveCount, LastUpdated)
                    VALUES (s.ChatID, 1, 0, 0, GETDATE());
                """,
                (chat_id,),
                commit=True,
            )
            await self.db_manager.execute_query(
                """
                MERGE ChatUserStats AS t
                USING (VALUES (?, ?)) AS s(ChatID, UserID)
                ON t.ChatID = s.ChatID AND t.UserID = s.UserID
                WHEN MATCHED THEN
                    UPDATE SET MessageCount = t.MessageCount + 1, LastActive = GETDATE()
                WHEN NOT MATCHED THEN
                    INSERT (ChatID, UserID, MessageCount, FirstSeen, LastActive)
                    VALUES (s.ChatID, s.UserID, 1, GETDATE(), GETDATE());
                """,
                (chat_id, user_id),
                commit=True,
            )
            if message_text:
                for domain in self._extract_domains(message_text):
                    await self.db_manager.execute_query(
                        """
                        MERGE ChatLinkStats AS t
                        USING (VALUES (?, ?)) AS s(ChatID, Domain)
                        ON t.ChatID = s.ChatID AND t.Domain = s.Domain
                        WHEN MATCHED THEN
                            UPDATE SET LinkCount = t.LinkCount + 1
                        WHEN NOT MATCHED THEN
                            INSERT (ChatID, Domain, LinkCount) VALUES (s.ChatID, s.Domain, 1);
                        """,
                        (chat_id, domain),
                        commit=True,
                    )
            return

        stats = self.memory_stats[chat_id]
        stats["total_messages"] += 1
        user_stats = stats["users"][user_id]
        user_stats["count"] += 1
        now = datetime.utcnow()
        user_stats["last_active"] = now
        if username:
            user_stats["username"] = username
        if not user_stats["first_seen"]:
            user_stats["first_seen"] = now
        if message_text:
            for domain in self._extract_domains(message_text):
                stats["links"][domain] += 1

    async def log_member_join(
        self, chat_id: int, user_id: int, username: Optional[str] = None
    ):
        if self.db_manager.connection_string:
            await self.db_manager.execute_query(
                """
                MERGE ChatStats AS t USING (VALUES (?)) AS s(ChatID)
                ON t.ChatID = s.ChatID
                WHEN MATCHED THEN UPDATE SET JoinCount = t.JoinCount + 1, LastUpdated = GETDATE()
                WHEN NOT MATCHED THEN INSERT (ChatID, TotalMessages, JoinCount, LeaveCount, LastUpdated)
                    VALUES (s.ChatID, 0, 1, 0, GETDATE());
                """,
                (chat_id,), commit=True,
            )
            await self.db_manager.execute_query(
                """
                MERGE ChatUserStats AS t USING (VALUES (?, ?)) AS s(ChatID, UserID)
                ON t.ChatID = s.ChatID AND t.UserID = s.UserID
                WHEN NOT MATCHED THEN
                    INSERT (ChatID, UserID, MessageCount, FirstSeen, LastActive)
                    VALUES (s.ChatID, s.UserID, 0, GETDATE(), GETDATE());
                """,
                (chat_id, user_id), commit=True,
            )
            return
        stats = self.memory_stats[chat_id]
        stats["joins"] += 1
        user_stats = stats["users"][user_id]
        now = datetime.utcnow()
        if username:
            user_stats["username"] = username
        if not user_stats["first_seen"]:
            user_stats["first_seen"] = now
            user_stats["last_active"] = now

    async def log_member_leave(self, chat_id: int, user_id: int):
        if self.db_manager.connection_string:
            await self.db_manager.execute_query(
                """
                MERGE ChatStats AS t USING (VALUES (?)) AS s(ChatID)
                ON t.ChatID = s.ChatID
                WHEN MATCHED THEN UPDATE SET LeaveCount = t.LeaveCount + 1, LastUpdated = GETDATE()
                WHEN NOT MATCHED THEN INSERT (ChatID, TotalMessages, JoinCount, LeaveCount, LastUpdated)
                    VALUES (s.ChatID, 0, 0, 1, GETDATE());
                """,
                (chat_id,), commit=True,
            )
            return
        stats = self.memory_stats[chat_id]
        stats["leaves"] += 1

    async def get_chat_stats(self, chat_id: int, top_n: int = 3) -> Optional[Dict[str, Any]]:
        if self.db_manager.connection_string:
            row = await self.db_manager.execute_query(
                "SELECT TotalMessages, JoinCount, LeaveCount FROM ChatStats WHERE ChatID = ?",
                (chat_id,), fetch_one=True,
            )
            if not row:
                return None
            top_users = await self.db_manager.execute_query(
                """
                SELECT TOP (?) cus.UserID, cus.MessageCount, up.Username
                FROM ChatUserStats cus
                LEFT JOIN UserProfiles up ON cus.UserID = up.UserID
                WHERE cus.ChatID = ?
                ORDER BY cus.MessageCount DESC
                """,
                (top_n, chat_id), fetch_all=True,
            )
            top_links = await self.db_manager.execute_query(
                "SELECT TOP (?) Domain, LinkCount FROM ChatLinkStats WHERE ChatID = ? ORDER BY LinkCount DESC",
                (top_n, chat_id), fetch_all=True,
            )
            return {
                "total_messages": row.TotalMessages,
                "joins": row.JoinCount,
                "leaves": row.LeaveCount,
                "top_users": [
                    {
                        "user_id": r.UserID,
                        "username": r.Username,
                        "count": r.MessageCount,
                    }
                    for r in top_users
                ] if top_users else [],
                "top_links": [
                    {"domain": r.Domain, "count": r.LinkCount} for r in top_links
                ] if top_links else [],
            }

        stats = self.memory_stats.get(chat_id)
        if not stats:
            return None
        top_users_sorted = sorted(
            (
                {
                    "user_id": uid,
                    "username": info.get("username"),
                    "count": info["count"],
                }
                for uid, info in stats["users"].items()
            ),
            key=lambda x: x["count"],
            reverse=True,
        )[:top_n]
        top_links_sorted = sorted(
            (
                {"domain": d, "count": c} for d, c in stats["links"].items()
            ),
            key=lambda x: x["count"],
            reverse=True,
        )[:top_n]
        return {
            "total_messages": stats["total_messages"],
            "joins": stats["joins"],
            "leaves": stats["leaves"],
            "top_users": top_users_sorted,
            "top_links": top_links_sorted,
        }

    async def get_user_stats(self, chat_id: int, user_id: int) -> Optional[Dict[str, Any]]:
        if self.db_manager.connection_string:
            user_row = await self.db_manager.execute_query(
                """
                SELECT cus.MessageCount, cus.FirstSeen, cus.LastActive, up.Username
                FROM ChatUserStats cus
                LEFT JOIN UserProfiles up ON cus.UserID = up.UserID
                WHERE cus.ChatID = ? AND cus.UserID = ?
                """,
                (chat_id, user_id), fetch_one=True,
            )
            if not user_row:
                return None
            total_row = await self.db_manager.execute_query(
                "SELECT TotalMessages FROM ChatStats WHERE ChatID = ?",
                (chat_id,), fetch_one=True,
            )
            rank_row = await self.db_manager.execute_query(
                "SELECT COUNT(*) + 1 AS Rank FROM ChatUserStats WHERE ChatID = ? AND MessageCount > ?",
                (chat_id, user_row.MessageCount), fetch_one=True,
            )
            total_users_row = await self.db_manager.execute_query(
                "SELECT COUNT(*) AS Cnt FROM ChatUserStats WHERE ChatID = ?",
                (chat_id,), fetch_one=True,
            )
            return {
                "messages": user_row.MessageCount,
                "first_seen": user_row.FirstSeen,
                "last_active": user_row.LastActive,
                "username": user_row.Username,
                "total_messages": total_row.TotalMessages if total_row else 0,
                "rank": rank_row.Rank if rank_row else 1,
                "total_users": total_users_row.Cnt if total_users_row else 1,
            }

        stats = self.memory_stats.get(chat_id)
        if not stats or user_id not in stats["users"]:
            return None
        user_stats = stats["users"][user_id]
        return {
            "messages": user_stats["count"],
            "first_seen": user_stats["first_seen"],
            "last_active": user_stats["last_active"],
            "username": user_stats.get("username"),
            "total_messages": stats["total_messages"],
            "rank": 1 + sum(1 for u in stats["users"].values() if u["count"] > user_stats["count"]),
            "total_users": len(stats["users"]),
        }

    def _extract_domains(self, text: str) -> List[str]:
        urls = re.findall(r"https?://[^\s]+", text)
        domains = []
        for url in urls:
            try:
                parsed = urlparse(url)
                if parsed.netloc:
                    domains.append(parsed.netloc.lower())
            except Exception:
                continue
        return domains



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/web_tool.py ---
======================================================================

# enkibot/modules/web_tool.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Light‑weight web research helper used by local models."""
from __future__ import annotations

from typing import List, Dict
import logging
import json

import openai
import requests
import trafilatura
from .. import config

logger = logging.getLogger(__name__)


def web_research(query: str, k: int = 5) -> List[Dict[str, str]]:
    """Return a list of documents for the given search query.

    Each document is a mapping containing ``title``, ``url`` and ``text`` keys.
    ``text`` is truncated to a reasonable length for prompt‑embedding.
    """

    logger.info("Web research for query: %s", query)
    if not config.OPENAI_API_KEY:
        return []
    client = openai.OpenAI(api_key=config.OPENAI_API_KEY)
    extra: Dict[str, object] = {}
    if config.OPENAI_SEARCH_CONTEXT_SIZE:
        extra["search_context_size"] = config.OPENAI_SEARCH_CONTEXT_SIZE
    if config.OPENAI_SEARCH_USER_LOCATION:
        try:
            extra["user_location"] = json.loads(
                config.OPENAI_SEARCH_USER_LOCATION
            )
        except Exception:
            extra["user_location"] = {"country": config.OPENAI_SEARCH_USER_LOCATION}
    try:
        resp = client.responses.create(
            model=config.OPENAI_DEEP_RESEARCH_MODEL_ID,
            tools=[{"type": "web_search"}],
            instructions=(
                f"Return up to {k} sources as a JSON array named 'items' of objects with 'title' and 'url'."
            ),
            input=query,
            **extra,
        )
        text = (getattr(resp, "output_text", "") or "").strip()
        data = json.loads(text) if text.startswith("{") else {"items": []}
        hits = data.get("items", [])
    except Exception as exc:  # pragma: no cover
        logger.warning("Web search failed: %s", exc)
        return []
    docs: List[Dict[str, str]] = []
    for hit in hits[:k]:
        url = hit.get("url")
        if not url:
            continue
        try:
            html = requests.get(url, timeout=15).text
            text = trafilatura.extract(html) or ""
            if text:
                docs.append({
                    "title": hit.get("title", ""),
                    "url": url,
                    "text": text[:20000],
                })
        except Exception as exc:  # pragma: no cover - network failures common
            logger.warning("Failed to fetch %s: %s", url, exc)
    return docs



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/__init__.py ---
======================================================================

# enkibot/utils/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file makes the 'utils' directory a Python package.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/database.py ---
======================================================================

# enkibot/utils/database.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by

# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Ensure your GPLv3 header is here)

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Your GPLv3 Header)
# ==================================================================================================
# === EnkiBot Database Utilities ===
# ==================================================================================================
import logging
import re
from typing import List, Dict, Any, Optional
from datetime import date
from types import SimpleNamespace

from .news_channels import fetch_channel_usernames

try:  # pragma: no cover - optional dependency
    import pyodbc
except Exception:  # pragma: no cover
    pyodbc = SimpleNamespace(Error=Exception)

from enkibot import config  # For DB_CONNECTION_STRING

logger = logging.getLogger(__name__)


class DatabaseManager:
    def __init__(self, connection_string: Optional[str]):
        self.connection_string = connection_string
        if not self.connection_string:
            logger.warning(
                "Database connection string is not configured. Database operations will be disabled."
            )

    def _log_db_error(
        self, context: str, query: str, params: Optional[tuple], err: Exception
    ) -> None:
        msg = str(err)
        lower = msg.lower()
        hint = ""
        if "no such table" in lower or "does not exist" in lower:
            hint = " Possible missing table."
        elif "no such column" in lower or "unknown column" in lower:
            hint = " Possible missing column."
        logger.error(
            f"{context} DB error on '{query[:100]}...' with params {params}: {msg}{hint}",
            exc_info=True,
        )

    def get_db_connection(self) -> Optional[Any]:
        if not self.connection_string or not hasattr(pyodbc, "connect"):
            logger.warning(
                "Database connection unavailable: missing connection string or pyodbc."
            )
            return None
        try:
            conn = pyodbc.connect(self.connection_string, autocommit=False)
            logger.debug("Database connection established.")
            return conn
        except pyodbc.Error as ex:
            logger.error(
                f"Database connection error: {ex.args[0] if ex.args else ''} - {ex}",
                exc_info=True,
            )
            return None
        except Exception as e:
            logger.error(
                f"Unexpected error establishing database connection: {e}", exc_info=True
            )
            return None

    async def execute_query(
        self,
        query: str,
        params: Optional[tuple] = None,
        fetch_one: bool = False,
        fetch_all: bool = False,
        commit: bool = False,
    ):
        if not self.connection_string:
            logger.warning("Query execution skipped: Database not configured.")
            return None if fetch_one or fetch_all else (True if commit else False)
        conn = self.get_db_connection()
        if not conn:
            return None if fetch_one or fetch_all else (True if commit else False)
        try:
            with conn.cursor() as cursor:
                logger.debug(f"Executing query: {query[:150]}... with params: {params}")
                cursor.execute(query, params) if params else cursor.execute(query)
                if commit:
                    conn.commit()
                    logger.debug("Query committed.")
                    return True
                if fetch_one:
                    row = cursor.fetchone()
                    logger.debug(f"Query fetch_one: {row}")
                    return row
                if fetch_all:
                    rows = cursor.fetchall()
                    logger.debug(f"Query fetch_all count: {len(rows)}")
                    return rows
            return True
        except pyodbc.Error as ex:
            self._log_db_error("Query", query, params, ex)
            try:
                conn.rollback()
                logger.info("Transaction rolled back.")
            except pyodbc.Error as rb_ex:
                logger.error(f"Error during rollback: {rb_ex}", exc_info=True)
            return None if fetch_one or fetch_all else False
        except Exception as e:
            self._log_db_error("Query", query, params, e)
            return None if fetch_one or fetch_all else False
        finally:
            if conn:
                conn.close()
                logger.debug("DB connection closed post-exec.")

    async def get_recent_chat_texts(self, chat_id: int, limit: int = 3) -> List[str]:
        if not self.connection_string:
            return []
        query = """
            SELECT TOP (?) MessageText FROM ChatLog
            WHERE ChatID = ? AND MessageText IS NOT NULL AND RTRIM(LTRIM(MessageText)) != ''
            ORDER BY Timestamp DESC
        """
        actual_limit = max(1, limit)
        rows = await self.execute_query(query, (actual_limit, chat_id), fetch_all=True)
        return (
            [row.MessageText for row in reversed(rows) if row.MessageText]
            if rows
            else []
        )

    async def log_chat_message_and_upsert_user(
        self,
        chat_id: int,
        user_id: int,
        username: Optional[str],
        first_name: Optional[str],
        last_name: Optional[str],
        message_id: int,
        message_text: str,
        preferred_language: Optional[str] = None,
    ) -> Optional[str]:
        if not self.connection_string:
            return None
        upsert_user_sql = """
            MERGE UserProfiles AS t
            USING (VALUES(?,?,?,?,GETDATE(),?)) AS s(UserID,Username,FirstName,LastName,LastSeen,PreferredLanguage)
            ON t.UserID = s.UserID
            WHEN MATCHED THEN
                UPDATE SET Username=s.Username, FirstName=s.FirstName, LastName=s.LastName, LastSeen=s.LastSeen, MessageCount=ISNULL(t.MessageCount,0)+1, PreferredLanguage=ISNULL(s.PreferredLanguage, t.PreferredLanguage)
            WHEN NOT MATCHED THEN
                INSERT(UserID,Username,FirstName,LastName,LastSeen,MessageCount,ProfileLastUpdated, PreferredLanguage)
                VALUES(s.UserID,s.Username,s.FirstName,s.LastName,s.LastSeen,1,GETDATE(),s.PreferredLanguage)
            OUTPUT $action AS Action;
        """
        user_params = (user_id, username, first_name, last_name, preferred_language)
        chat_log_sql = "INSERT INTO ChatLog (ChatID, UserID, Username, FirstName, MessageID, MessageText, Timestamp) VALUES (?, ?, ?, ?, ?, ?, GETDATE())"
        chat_log_params = (
            chat_id,
            user_id,
            username,
            first_name,
            message_id,
            message_text,
        )
        conn = self.get_db_connection()
        if not conn:
            return None
        action_taken = None
        try:
            with conn.cursor() as cursor:
                cursor.execute(upsert_user_sql, user_params)
                row = cursor.fetchone()
                if row:
                    action_taken = row.Action
                cursor.execute(chat_log_sql, chat_log_params)
            conn.commit()
            logger.info(
                f"Logged message for user {user_id}. Profile action: {action_taken}"
            )
            return action_taken
        except pyodbc.Error as ex:
            self._log_db_error(
                "log_chat_message_and_upsert_user",
                upsert_user_sql,
                user_params,
                ex,
            )
            conn.rollback()
        finally:
            if conn:
                conn.close()
        return None  # Ensure a return path if try fails before commit

    async def log_assistant_invocation(
        self,
        chat_id: int,
        user_id: int,
        message_id: int,
        detected: bool,
        alias: Optional[str],
        prompt: str,
        reason: Optional[str],
        lang: Optional[str],
        routed_to_llm: bool,
        llm_ok: bool,
        error: Optional[str],
    ) -> None:
        if not self.connection_string:
            return
        sql = (
            "INSERT INTO assistant_invocations (chat_id, user_id, message_id, detected, alias, prompt, reason, lang, routed_to_llm, llm_ok, error)"
            " VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        )
        params = (
            chat_id,
            user_id,
            message_id,
            int(detected),
            alias,
            prompt,
            reason,
            lang,
            int(routed_to_llm),
            int(llm_ok),
            error,
        )
        await self.execute_query(sql, params, commit=True)

    async def get_conversation_history(
        self, chat_id: int, limit: int = 20
    ) -> List[Dict[str, str]]:
        query = "SELECT TOP (?) Role, Content FROM ConversationHistory WHERE ChatID = ? ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, chat_id), fetch_all=True)
        return (
            [
                {"role": row.Role.lower(), "content": row.Content}
                for row in reversed(rows)
            ]
            if rows
            else []
        )

    async def save_to_conversation_history(
        self,
        chat_id: int,
        entity_id: int,
        message_id_telegram: Optional[int],
        role: str,
        content: str,
    ):
        query = "INSERT INTO ConversationHistory (ChatID, UserID, MessageID, Role, Content, Timestamp) VALUES (?, ?, ?, ?, ?, GETDATE())"
        await self.execute_query(
            query, (chat_id, entity_id, message_id_telegram, role, content), commit=True
        )

    async def get_user_profile_notes(self, user_id: int) -> Optional[str]:
        row = await self.execute_query(
            "SELECT Notes FROM UserProfiles WHERE UserID = ?",
            (user_id,),
            fetch_one=True,
        )
        return row.Notes if row and row.Notes else None

    async def update_user_profile_notes(self, user_id: int, notes: str):
        await self.execute_query(
            "UPDATE UserProfiles SET Notes = ?, ProfileLastUpdated = GETDATE() WHERE UserID = ?",
            (notes, user_id),
            commit=True,
        )
        logger.info(f"Profile notes updated for user {user_id}.")

    async def save_user_name_variations(self, user_id: int, variations: List[str]):
        if not self.connection_string or not variations:
            return
        sql_merge = """
            MERGE INTO UserNameVariations AS t USING (SELECT ? AS UserID, ? AS NameVariation) AS s
            ON (t.UserID = s.UserID AND t.NameVariation = s.NameVariation)
            WHEN NOT MATCHED THEN INSERT (UserID, NameVariation) VALUES (s.UserID, s.NameVariation);
        """
        conn = self.get_db_connection()
        if not conn:
            return
        try:
            with conn.cursor() as cursor:
                params_to_insert = [
                    (user_id, var) for var in variations if var and str(var).strip()
                ]
                if params_to_insert:
                    cursor.executemany(sql_merge, params_to_insert)
            conn.commit()
            logger.info(
                f"Saved/updated {len(params_to_insert)} name vars for user {user_id}."
            )
        except pyodbc.Error as ex:
            logger.error(
                f"DB error saving name vars for user {user_id}: {ex}", exc_info=True
            )
            conn.rollback()
        finally:
            if conn:
                conn.close()

    async def find_user_profiles_by_name_variation(
        self, name_variation_query: str
    ) -> List[Dict[str, Any]]:
        query = """
            SELECT DISTINCT up.UserID, up.FirstName, up.LastName, up.Username, up.Notes
            FROM UserProfiles up JOIN UserNameVariations unv ON up.UserID = unv.UserID
            WHERE unv.NameVariation = ?
        """
        rows = await self.execute_query(
            query, (name_variation_query.lower(),), fetch_all=True
        )
        return (
            [
                {
                    "UserID": r.UserID,
                    "FirstName": r.FirstName,
                    "LastName": r.LastName,
                    "Username": r.Username,
                    "Notes": r.Notes,
                }
                for r in rows
            ]
            if rows
            else []
        )

    async def get_user_messages_from_chat_log(
        self, user_id: int, chat_id: int, limit: int = 10
    ) -> List[str]:  # Kept from previous version
        query = "SELECT TOP (?) MessageText FROM ChatLog WHERE UserID = ? AND ChatID = ? AND MessageText IS NOT NULL AND RTRIM(LTRIM(MessageText)) != '' ORDER BY Timestamp DESC"
        rows = await self.execute_query(
            query, (limit, user_id, chat_id), fetch_all=True
        )
        return [row.MessageText for row in rows] if rows else []

    async def _ensure_user_usage_table(self):
        """Create the UserUsage table if it does not exist."""
        await self.execute_query(
            """
            IF OBJECT_ID('UserUsage', 'U') IS NULL
            CREATE TABLE UserUsage (
                UserID BIGINT NOT NULL,
                UsageDate DATE NOT NULL,
                LlmCount INT NOT NULL DEFAULT 0,
                ImageCount INT NOT NULL DEFAULT 0,
                CONSTRAINT PK_UserUsage PRIMARY KEY (UserID, UsageDate)
            );
            """,
            commit=True,
        )

    async def get_daily_usage(
        self, user_id: int, usage_date: Optional[date] = None
    ) -> Dict[str, int]:
        usage_date = usage_date or date.today()
        await self._ensure_user_usage_table()
        row = await self.execute_query(
            "SELECT LlmCount, ImageCount FROM UserUsage WHERE UserID = ? AND UsageDate = ?",
            (user_id, usage_date),
            fetch_one=True,
        )
        if row:
            return {"llm": row.LlmCount or 0, "image": row.ImageCount or 0}
        return {"llm": 0, "image": 0}

    async def increment_usage(
        self, user_id: int, usage_type: str, usage_date: Optional[date] = None
    ):
        usage_date = usage_date or date.today()
        column = "LlmCount" if usage_type == "llm" else "ImageCount"
        await self._ensure_user_usage_table()
        sql = f"""
            MERGE UserUsage AS t USING (VALUES(?,?)) AS s(UserID,UsageDate)
            ON t.UserID = s.UserID AND t.UsageDate = s.UsageDate
            WHEN MATCHED THEN UPDATE SET {column} = t.{column} + 1
            WHEN NOT MATCHED THEN INSERT (UserID, UsageDate, {column}) VALUES (s.UserID, s.UsageDate, 1);
        """
        await self.execute_query(sql, (user_id, usage_date), commit=True)

    async def check_and_increment_usage(
        self,
        user_id: int,
        usage_type: str,
        daily_limit: int,
        usage_date: Optional[date] = None,
    ) -> bool:
        usage_date = usage_date or date.today()
        usage = await self.get_daily_usage(user_id, usage_date)
        if usage.get(usage_type, 0) >= daily_limit:
            return False
        await self.increment_usage(user_id, usage_type, usage_date)
        return True

    async def add_verified_user(self, user_id: int):
        """Add or update a user in the global verification table."""
        await self.execute_query(
            "MERGE VerifiedUsers AS t USING (VALUES(?)) AS s(UserID) "
            "ON t.UserID = s.UserID "
            "WHEN MATCHED THEN UPDATE SET VerifiedAt = GETDATE() "
            "WHEN NOT MATCHED THEN INSERT (UserID, VerifiedAt) VALUES (s.UserID, GETDATE());",
            (user_id,),
            commit=True,
        )

    async def is_user_verified(self, user_id: int) -> bool:
        row = await self.execute_query(
            "SELECT 1 FROM VerifiedUsers WHERE UserID = ?",
            (user_id,),
            fetch_one=True,
        )
        return bool(row)

    async def add_spam_vote(
        self, chat_id: int, target_user_id: int, reporter_user_id: int
    ) -> bool:
        """Records a spam vote if one doesn't already exist for this reporter/target pair."""
        if not self.connection_string:
            return False
        conn = self.get_db_connection()
        if not conn:
            return False
        sql = (
            "MERGE SpamReports AS t USING (VALUES(?,?,?)) AS s(ChatID,TargetUserID,ReporterUserID) "
            "ON t.ChatID=s.ChatID AND t.TargetUserID=s.TargetUserID AND t.ReporterUserID=s.ReporterUserID "
            "WHEN NOT MATCHED THEN INSERT (ChatID,TargetUserID,ReporterUserID,Timestamp) "
            "VALUES(s.ChatID,s.TargetUserID,s.ReporterUserID,GETDATE()) OUTPUT $action;"
        )
        try:
            with conn.cursor() as cursor:
                cursor.execute(sql, (chat_id, target_user_id, reporter_user_id))
                row = cursor.fetchone()
            conn.commit()
            return bool(row and row.Action == "INSERT")
        except pyodbc.Error as ex:
            logger.error(f"DB error adding spam vote: {ex}", exc_info=True)
            try:
                conn.rollback()
            except pyodbc.Error:
                pass
            return False
        finally:
            if conn:
                conn.close()

    async def count_spam_votes(
        self, chat_id: int, target_user_id: int, window_minutes: int
    ) -> int:
        """Returns the number of unique reporters within the given timeframe."""
        query = (
            "SELECT COUNT(DISTINCT ReporterUserID) AS VoteCount FROM SpamReports "
            "WHERE ChatID = ? AND TargetUserID = ? AND Timestamp > DATEADD(MINUTE, ?, GETDATE())"
        )
        row = await self.execute_query(
            query, (chat_id, target_user_id, -abs(window_minutes)), fetch_one=True
        )
        return row.VoteCount if row and hasattr(row, "VoteCount") else 0

    async def clear_spam_votes(self, chat_id: int, target_user_id: int):
        await self.execute_query(
            "DELETE FROM SpamReports WHERE ChatID = ? AND TargetUserID = ?",
            (chat_id, target_user_id),
            commit=True,
        )

    async def get_spam_vote_threshold(
        self, chat_id: int, default_threshold: int
    ) -> int:
        row = await self.execute_query(
            "SELECT SpamVoteThreshold FROM ChatSettings WHERE ChatID = ?",
            (chat_id,),
            fetch_one=True,
        )
        if row and hasattr(row, "SpamVoteThreshold") and row.SpamVoteThreshold:
            return row.SpamVoteThreshold
        await self.execute_query(
            "INSERT INTO ChatSettings (ChatID, SpamVoteThreshold) VALUES (?, ?)",
            (chat_id, default_threshold),
            commit=True,
        )
        return default_threshold

    async def set_spam_vote_threshold(self, chat_id: int, threshold: int):
        await self.execute_query(
            "MERGE ChatSettings AS t USING (VALUES(?,?)) AS s(ChatID,SpamVoteThreshold) "
            "ON t.ChatID=s.ChatID "
            "WHEN MATCHED THEN UPDATE SET SpamVoteThreshold=s.SpamVoteThreshold "
            "WHEN NOT MATCHED THEN INSERT (ChatID,SpamVoteThreshold) VALUES (s.ChatID,s.SpamVoteThreshold);",
            (chat_id, threshold),
            commit=True,
        )

    async def _ensure_nsfw_columns(self):
        """Ensure NSFW-related columns exist in ChatSettings."""
        await self.execute_query(
            "IF COL_LENGTH('ChatSettings', 'NSFWFilterEnabled') IS NULL "
            "ALTER TABLE ChatSettings ADD NSFWFilterEnabled BIT NOT NULL DEFAULT 0;",
            commit=True,
        )
        await self.execute_query(
            f"IF COL_LENGTH('ChatSettings', 'NSFWThreshold') IS NULL "
            f"ALTER TABLE ChatSettings ADD NSFWThreshold FLOAT NOT NULL DEFAULT {config.NSFW_DETECTION_THRESHOLD};",
            commit=True,
        )

    async def get_nsfw_filter_enabled(self, chat_id: int) -> bool:
        await self._ensure_nsfw_columns()
        row = await self.execute_query(
            "SELECT NSFWFilterEnabled FROM ChatSettings WHERE ChatID = ?",
            (chat_id,),
            fetch_one=True,
        )
        if row and hasattr(row, "NSFWFilterEnabled"):
            return bool(row.NSFWFilterEnabled)
        await self.execute_query(
            "INSERT INTO ChatSettings (ChatID, SpamVoteThreshold, NSFWFilterEnabled, NSFWThreshold) VALUES (?, ?, ?, ?)",
            (
                chat_id,
                config.DEFAULT_SPAM_VOTE_THRESHOLD,
                int(config.NSFW_FILTER_DEFAULT_ENABLED),
                config.NSFW_DETECTION_THRESHOLD,
            ),
            commit=True,
        )
        return bool(config.NSFW_FILTER_DEFAULT_ENABLED)

    async def set_nsfw_filter_enabled(self, chat_id: int, enabled: bool):
        await self._ensure_nsfw_columns()
        await self.execute_query(
            "MERGE ChatSettings AS t USING (VALUES(?,?,?,?)) AS s(ChatID,SpamVoteThreshold,NSFWFilterEnabled,NSFWThreshold) "
            "ON t.ChatID=s.ChatID "
            "WHEN MATCHED THEN UPDATE SET NSFWFilterEnabled=s.NSFWFilterEnabled "
            "WHEN NOT MATCHED THEN INSERT (ChatID,SpamVoteThreshold,NSFWFilterEnabled,NSFWThreshold) VALUES (s.ChatID,s.SpamVoteThreshold,s.NSFWFilterEnabled,s.NSFWThreshold);",
            (
                chat_id,
                config.DEFAULT_SPAM_VOTE_THRESHOLD,
                int(enabled),
                config.NSFW_DETECTION_THRESHOLD,
            ),
            commit=True,
        )

    async def get_nsfw_threshold(self, chat_id: int) -> float:
        await self._ensure_nsfw_columns()
        row = await self.execute_query(
            "SELECT NSFWThreshold FROM ChatSettings WHERE ChatID = ?",
            (chat_id,),
            fetch_one=True,
        )
        if row and hasattr(row, "NSFWThreshold") and row.NSFWThreshold is not None:
            return float(row.NSFWThreshold)
        await self.execute_query(
            "INSERT INTO ChatSettings (ChatID, SpamVoteThreshold, NSFWFilterEnabled, NSFWThreshold) VALUES (?, ?, ?, ?)",
            (
                chat_id,
                config.DEFAULT_SPAM_VOTE_THRESHOLD,
                int(config.NSFW_FILTER_DEFAULT_ENABLED),
                config.NSFW_DETECTION_THRESHOLD,
            ),
            commit=True,
        )
        return float(config.NSFW_DETECTION_THRESHOLD)

    async def set_nsfw_threshold(self, chat_id: int, threshold: float):
        await self._ensure_nsfw_columns()
        await self.execute_query(
            "MERGE ChatSettings AS t USING (VALUES(?,?,?,?)) AS s(ChatID,SpamVoteThreshold,NSFWFilterEnabled,NSFWThreshold) "
            "ON t.ChatID=s.ChatID "
            "WHEN MATCHED THEN UPDATE SET NSFWThreshold=s.NSFWThreshold "
            "WHEN NOT MATCHED THEN INSERT (ChatID,SpamVoteThreshold,NSFWFilterEnabled,NSFWThreshold) VALUES (s.ChatID,s.SpamVoteThreshold,s.NSFWFilterEnabled,s.NSFWThreshold);",
            (
                chat_id,
                config.DEFAULT_SPAM_VOTE_THRESHOLD,
                int(config.NSFW_FILTER_DEFAULT_ENABLED),
                threshold,
            ),
            commit=True,
        )

    async def add_warning(
        self, chat_id: int, user_id: int, reason: Optional[str] = None
    ) -> int:
        """Increment warning count for a user and return new count."""
        if not self.connection_string:
            return 0
        conn = self.get_db_connection()
        if not conn:
            return 0
        sql = (
            "MERGE UserWarnings AS t USING (VALUES(?,?,?)) AS s(ChatID,UserID,Reason) "
            "ON t.ChatID=s.ChatID AND t.UserID=s.UserID "
            "WHEN MATCHED THEN UPDATE SET WarnCount=ISNULL(t.WarnCount,0)+1, LastReason=s.Reason, LastWarned=GETDATE() "
            "WHEN NOT MATCHED THEN INSERT (ChatID,UserID,WarnCount,LastReason,LastWarned) "
            "VALUES (s.ChatID,s.UserID,1,s.Reason,GETDATE()) OUTPUT inserted.WarnCount;"
        )
        try:
            with conn.cursor() as cursor:
                cursor.execute(sql, (chat_id, user_id, reason))
                row = cursor.fetchone()
            conn.commit()
            return row.WarnCount if row and hasattr(row, "WarnCount") else 0
        except pyodbc.Error as ex:
            logger.error(f"DB error adding warning: {ex}", exc_info=True)
            try:
                conn.rollback()
            except pyodbc.Error:
                pass
            return 0
        finally:
            if conn:
                conn.close()

    async def get_warning_count(self, chat_id: int, user_id: int) -> int:
        row = await self.execute_query(
            "SELECT WarnCount FROM UserWarnings WHERE ChatID = ? AND UserID = ?",
            (chat_id, user_id),
            fetch_one=True,
        )
        return row.WarnCount if row and hasattr(row, "WarnCount") else 0

    async def clear_warnings(self, chat_id: int, user_id: int):
        await self.execute_query(
            "DELETE FROM UserWarnings WHERE ChatID = ? AND UserID = ?",
            (chat_id, user_id),
            commit=True,
        )

    async def list_warnings(self, chat_id: int):
        rows = await self.execute_query(
            "SELECT UserID, WarnCount FROM UserWarnings WHERE ChatID = ?",
            (chat_id,),
            fetch_all=True,
        )
        return [(r.UserID, r.WarnCount) for r in rows] if rows else []

    async def log_moderation_action(
        self,
        chat_id: int,
        user_id: Optional[int],
        message_id: int,
        categories: Optional[str],
    ):
        """Persist a moderation action to the database for audit purposes."""
        await self.execute_query(
            "INSERT INTO ModerationLog (ChatID, UserID, MessageID, Categories) VALUES (?, ?, ?, ?)",
            (chat_id, user_id, message_id, categories),
            commit=True,
        )

    async def log_fact_check(
        self,
        chat_id: int,
        message_id: int,
        claim_text: str,
        verdict: str,
        confidence: float,
        track: str = "news",
        details: Optional[str] = None,
    ):
        """Persist fact-check outcomes for auditing.

        Handles optional ``Track`` and ``Details`` columns for backward
        compatibility.
        """

        columns = ["ChatID", "MessageID", "ClaimText", "Verdict", "Confidence"]
        params: List[Any] = [chat_id, message_id, claim_text, verdict, confidence]

        track_exists = await self.execute_query(
            "SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'FactCheckLog' AND COLUMN_NAME = 'Track'",
            fetch_one=True,
        )
        if track_exists:
            columns.append("Track")
            params.append(track)

        details_exists = await self.execute_query(
            "SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'FactCheckLog' AND COLUMN_NAME = 'Details'",
            fetch_one=True,
        )
        if details_exists:
            columns.append("Details")
            params.append(details)

        placeholders = ", ".join(["?"] * len(params))
        query = (
            f"INSERT INTO FactCheckLog ({', '.join(columns)}) VALUES ({placeholders})"
        )

        await self.execute_query(query, tuple(params), commit=True)

    async def log_fact_gate(
        self, chat_id: int, message_id: int, p_news: float, p_book: float
    ) -> None:
        await self.execute_query(
            "INSERT INTO FactGateLog (ChatID, MessageID, PNews, PBook) VALUES (?, ?, ?, ?)",
            (chat_id, message_id, p_news, p_book),
            commit=True,
        )

    async def replace_news_channels(self, usernames: List[str]) -> None:
        if not self.connection_string:
            logger.warning(
                "News channel update skipped: Database not configured."
            )
            return
        conn = self.get_db_connection()
        if not conn:
            return
        try:
            with conn.cursor() as cursor:
                logger.info("Clearing existing NewsChannels entries")
                cursor.execute("DELETE FROM NewsChannels")
                for name in usernames:
                    cursor.execute(
                        "INSERT INTO NewsChannels (Username, UpdatedAt) VALUES (?, GETDATE())",
                        name.strip().lstrip("@").lower(),
                    )
            logger.info("Inserted %d news channels", len(usernames))
            conn.commit()
        except Exception as e:
            self._log_db_error(
                "replace_news_channels",
                "bulk update NewsChannels",
                None,
                e,
            )
            try:
                conn.rollback()
            except Exception:
                pass
        finally:
            conn.close()

    async def get_news_channel_usernames(self) -> set[str]:
        rows = await self.execute_query(
            "SELECT Username FROM NewsChannels",
            fetch_all=True,
        )
        return (
            {row.Username.strip().lstrip("@").lower() for row in rows} if rows else set()
        )

    async def refresh_news_channels(self) -> None:
        logger.info("Refreshing news channel list from remote source")
        names = await fetch_channel_usernames()
        if names:
            logger.info("Updating database with %d channels", len(names))
            await self.replace_news_channels(names)
        else:
            logger.warning("No channel usernames fetched; database not updated")

    async def log_web_request(
        self,
        url: str,
        method: str,
        status_code: Optional[int],
        duration_ms: Optional[int],
        error: Optional[str] = None,
    ) -> None:
        """Record an outbound HTTP request for auditing and diagnostics."""
        await self.execute_query(
            "INSERT INTO WebRequestLog (Url, Method, StatusCode, DurationMs, Error) VALUES (?, ?, ?, ?, ?)",
            (url, method, status_code, duration_ms, error),
            commit=True,
        )

    async def log_answer_evidence(
        self,
        chat_id: int,
        asked_by: int,
        intent: str,
        query_text: Optional[str],
        lang: Optional[str],
        items: List[Dict[str, Any]],
    ) -> Optional[int]:
        """Persist evidence snippets for a given answer.

        Returns the generated answer_id if successful.
        """
        if not self.connection_string:
            return None
        conn = self.get_db_connection()
        if not conn:
            return None
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    (
                        "INSERT INTO answer_evidence (chat_id, asked_by, intent, query_text, lang) "
                        "VALUES (?, ?, ?, ?, ?); SELECT SCOPE_IDENTITY();"
                    ),
                    (chat_id, asked_by, intent, query_text, lang),
                )
                row = cursor.fetchone()
                if not row:
                    conn.rollback()
                    return None
                answer_id = int(row[0])
                if items:
                    params = [
                        (
                            answer_id,
                            it.get("message_id"),
                            it.get("rank", 0),
                            it.get("snippet"),
                            it.get("reason"),
                        )
                        for it in items
                    ]
                    cursor.executemany(
                        "INSERT INTO answer_evidence_items (answer_id, message_id, rank, snippet, reason)"
                        " VALUES (?, ?, ?, ?, ?)",
                        params,
                    )
            conn.commit()
            return answer_id
        except pyodbc.Error as ex:
            logger.error(f"DB error logging answer evidence: {ex}", exc_info=True)
            try:
                conn.rollback()
            except pyodbc.Error:
                pass
            return None
        finally:
            if conn:
                conn.close()

    async def save_user_persona_version(
        self,
        chat_id: int,
        user_id: int,
        portrait_md: str,
        traits_json: str,
        signals_json: Optional[str] = None,
    ) -> bool:
        """Store a new persona version for the given user.

        Automatically increments the version number for that chat/user pair.
        """
        if not self.connection_string:
            return False
        conn = self.get_db_connection()
        if not conn:
            return False
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "SELECT ISNULL(MAX(version),0) + 1 AS next_version FROM user_persona_versions WHERE chat_id = ? AND user_id = ?",
                    (chat_id, user_id),
                )
                row = cursor.fetchone()
                version = int(
                    row.next_version if row and hasattr(row, "next_version") else 1
                )
                cursor.execute(
                    "INSERT INTO user_persona_versions (chat_id, user_id, version, portrait_md, traits_json, signals_json) "
                    "VALUES (?, ?, ?, ?, ?, ?)",
                    (chat_id, user_id, version, portrait_md, traits_json, signals_json),
                )
            conn.commit()
            return True
        except pyodbc.Error as ex:
            logger.error(f"DB error saving persona version: {ex}", exc_info=True)
            try:
                conn.rollback()
            except pyodbc.Error:
                pass
            return False
        finally:
            if conn:
                conn.close()

    async def get_latest_user_persona(
        self, chat_id: int, user_id: int
    ) -> Optional[Dict[str, Any]]:
        """Retrieve the most recent persona version for a user."""
        row = await self.execute_query(
            "SELECT TOP 1 version, portrait_md, traits_json, signals_json "
            "FROM user_persona_versions WHERE chat_id = ? AND user_id = ? ORDER BY version DESC",
            (chat_id, user_id),
            fetch_one=True,
        )
        if not row:
            return None
        return {
            "version": row.version if hasattr(row, "version") else None,
            "portrait_md": row.portrait_md if hasattr(row, "portrait_md") else None,
            "traits_json": row.traits_json if hasattr(row, "traits_json") else None,
            "signals_json": row.signals_json if hasattr(row, "signals_json") else None,
        }


def initialize_database():  # This function defines and uses DatabaseManager locally
    if not config.DB_CONNECTION_STRING:
        logger.warning("Cannot initialize database: Connection string not configured.")
        return

    db_mngr = DatabaseManager(config.DB_CONNECTION_STRING)
    conn = db_mngr.get_db_connection()
    if not conn:
        logger.error(
            "Failed to connect to database for initialization (initialize_database)."
        )
        return

    conn.autocommit = True
    table_queries = {
        "UserProfiles": "CREATE TABLE UserProfiles (UserID BIGINT PRIMARY KEY, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, LastName NVARCHAR(255) NULL, LastSeen DATETIME2 DEFAULT GETDATE(), MessageCount INT DEFAULT 0, PreferredLanguage NVARCHAR(10) NULL, Notes NVARCHAR(MAX) NULL, ProfileLastUpdated DATETIME2 DEFAULT GETDATE(), KarmaReceived INT NOT NULL DEFAULT 0, KarmaGiven INT NOT NULL DEFAULT 0, HateGiven INT NOT NULL DEFAULT 0);",
        "UserNameVariations": "CREATE TABLE UserNameVariations (VariationID INT IDENTITY(1,1) PRIMARY KEY, UserID BIGINT NOT NULL, NameVariation NVARCHAR(255) NOT NULL, FOREIGN KEY (UserID) REFERENCES UserProfiles(UserID) ON DELETE CASCADE);",
        "IX_UserNameVariations_UserID_NameVariation": "CREATE UNIQUE INDEX IX_UserNameVariations_UserID_NameVariation ON UserNameVariations (UserID, NameVariation);",
        "ConversationHistory": "CREATE TABLE ConversationHistory (MessageDBID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageID BIGINT NULL, Role NVARCHAR(50) NOT NULL, Content NVARCHAR(MAX) NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ConversationHistory_ChatID_Timestamp": "CREATE INDEX IX_ConversationHistory_ChatID_Timestamp ON ConversationHistory (ChatID, Timestamp DESC);",
        "ChatLog": "CREATE TABLE ChatLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, MessageID BIGINT NOT NULL, MessageText NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ChatLog_ChatID_Timestamp": "CREATE INDEX IX_ChatLog_ChatID_Timestamp ON ChatLog (ChatID, Timestamp DESC);",
        "IX_ChatLog_UserID": "CREATE INDEX IX_ChatLog_UserID ON ChatLog (UserID);",
        "UserUsage": "CREATE TABLE UserUsage (UserID BIGINT NOT NULL, UsageDate DATE NOT NULL, LlmCount INT NOT NULL DEFAULT 0, ImageCount INT NOT NULL DEFAULT 0, PRIMARY KEY (UserID, UsageDate));",
        "ChatStats": "CREATE TABLE ChatStats (ChatID BIGINT PRIMARY KEY, TotalMessages INT NOT NULL DEFAULT 0, JoinCount INT NOT NULL DEFAULT 0, LeaveCount INT NOT NULL DEFAULT 0, LastUpdated DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "ChatUserStats": "CREATE TABLE ChatUserStats (ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageCount INT NOT NULL DEFAULT 0, FirstSeen DATETIME2 DEFAULT GETDATE() NOT NULL, LastActive DATETIME2 DEFAULT GETDATE() NOT NULL, PRIMARY KEY (ChatID, UserID));",
        "IX_ChatUserStats_ChatID_MessageCount": "CREATE INDEX IX_ChatUserStats_ChatID_MessageCount ON ChatUserStats (ChatID, MessageCount DESC);",
        "ChatLinkStats": "CREATE TABLE ChatLinkStats (ChatID BIGINT NOT NULL, Domain NVARCHAR(255) NOT NULL, LinkCount INT NOT NULL DEFAULT 0, PRIMARY KEY (ChatID, Domain));",
        "IX_ChatLinkStats_ChatID_Count": "CREATE INDEX IX_ChatLinkStats_ChatID_Count ON ChatLinkStats (ChatID, LinkCount DESC);",
        "NewsChannels": "CREATE TABLE NewsChannels (Username NVARCHAR(255) PRIMARY KEY, UpdatedAt DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "ErrorLog": "CREATE TABLE ErrorLog (ErrorID INT IDENTITY(1,1) PRIMARY KEY, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, LogLevel NVARCHAR(50) NOT NULL, LoggerName NVARCHAR(255) NULL, ModuleName NVARCHAR(255) NULL, FunctionName NVARCHAR(255) NULL, LineNumber INT NULL, ErrorMessage NVARCHAR(MAX) NOT NULL, ExceptionInfo NVARCHAR(MAX) NULL);",
        "IX_ErrorLog_Timestamp": "CREATE INDEX IX_ErrorLog_Timestamp ON ErrorLog (Timestamp DESC);",
        "WebRequestLog": (
            "CREATE TABLE WebRequestLog ("
            "LogID INT IDENTITY(1,1) PRIMARY KEY,"
            "Url NVARCHAR(1024) NOT NULL,"
            "Method NVARCHAR(16) NOT NULL,"
            "StatusCode INT NULL,"
            "DurationMs INT NULL,"
            "Error NVARCHAR(512) NULL,"
            "Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL"
            ");",
        ),
        "IX_WebRequestLog_Timestamp": "CREATE INDEX IX_WebRequestLog_Timestamp ON WebRequestLog (Timestamp DESC);",
        "ChatSettings": f"CREATE TABLE ChatSettings (ChatID BIGINT PRIMARY KEY, SpamVoteThreshold INT NOT NULL DEFAULT 3, NSFWFilterEnabled BIT NOT NULL DEFAULT 0, NSFWThreshold FLOAT NOT NULL DEFAULT {config.NSFW_DETECTION_THRESHOLD});",
        "SpamReports": "CREATE TABLE SpamReports (ReportID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, TargetUserID BIGINT NOT NULL, ReporterUserID BIGINT NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, CONSTRAINT UQ_SpamReports UNIQUE (ChatID, TargetUserID, ReporterUserID));",
        "IX_SpamReports_Chat_Target": "CREATE INDEX IX_SpamReports_Chat_Target ON SpamReports (ChatID, TargetUserID);",
        "KarmaLog": "CREATE TABLE KarmaLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, GiverUserID BIGINT NOT NULL, ReceiverUserID BIGINT NOT NULL, Points INT NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_KarmaLog_ChatID_Timestamp": "CREATE INDEX IX_KarmaLog_ChatID_Timestamp ON KarmaLog (ChatID, Timestamp DESC);",
        "VerifiedUsers": "CREATE TABLE VerifiedUsers (UserID BIGINT PRIMARY KEY, VerifiedAt DATETIME2 DEFAULT GETDATE());",
        "ModerationLog": "CREATE TABLE ModerationLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NULL, MessageID BIGINT NOT NULL, Categories NVARCHAR(255) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ModerationLog_ChatID_Timestamp": "CREATE INDEX IX_ModerationLog_ChatID_Timestamp ON ModerationLog (ChatID, Timestamp DESC);",
        "FactCheckLog": "CREATE TABLE FactCheckLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, MessageID BIGINT NULL, ClaimText NVARCHAR(MAX) NOT NULL, Verdict NVARCHAR(50) NOT NULL, Confidence FLOAT NOT NULL, Track NVARCHAR(8) NULL CHECK (Track IN ('news','book')), Details NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_FactCheckLog_ChatID_Timestamp": "CREATE INDEX IX_FactCheckLog_ChatID_Timestamp ON FactCheckLog (ChatID, Timestamp DESC);",
        "FactGateLog": "CREATE TABLE FactGateLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, MessageID BIGINT NOT NULL, PNews FLOAT NULL, PBook FLOAT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "assistant_invocations": (
            "CREATE TABLE assistant_invocations ("
            "id BIGINT IDENTITY PRIMARY KEY,"
            "chat_id BIGINT,"
            "user_id BIGINT,"
            "message_id BIGINT,"
            "detected BIT,"
            "alias NVARCHAR(32),"
            "prompt NVARCHAR(MAX),"
            "reason NVARCHAR(64),"
            "lang NVARCHAR(8),"
            "routed_to_llm BIT,"
            "llm_ok BIT,"
            "error NVARCHAR(512),"
            "ts DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME()"
            ");",
        ),
        "FactBookSources": "CREATE TABLE FactBookSources (id BIGINT IDENTITY PRIMARY KEY, author NVARCHAR(256) NOT NULL, title NVARCHAR(512) NOT NULL, edition NVARCHAR(128) NULL, year INT NULL, isbn NVARCHAR(32) NULL, translator NVARCHAR(256) NULL, source_url NVARCHAR(1024) NULL, snapshot_url NVARCHAR(1024) NULL, first_seen DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME());",
        "FactBookMatches": "CREATE TABLE FactBookMatches (match_id BIGINT IDENTITY PRIMARY KEY, run_id BIGINT NOT NULL, book_source_id BIGINT NULL, quote_exact NVARCHAR(MAX) NULL, quote_lang NVARCHAR(8) NULL, page NVARCHAR(32) NULL, chapter NVARCHAR(64) NULL, stance NVARCHAR(12) NULL, score FLOAT NULL);",
        "IX_FactBookMatches_Run": "CREATE INDEX IX_FactBookMatches_Run ON FactBookMatches(run_id);",
        # ------------------------------------------------------------------
        # Memory/portrait feature tables
        # ------------------------------------------------------------------
        "answer_evidence": (
            "CREATE TABLE answer_evidence ("
            "answer_id BIGINT IDENTITY PRIMARY KEY,"
            "chat_id BIGINT NOT NULL,"
            "asked_by BIGINT NOT NULL,"
            "intent NVARCHAR(32) NOT NULL,"
            "query_text NVARCHAR(MAX) NULL,"
            "lang NVARCHAR(8) NULL,"
            "created_at DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME()"
            ");"
        ),
        "answer_evidence_items": (
            "CREATE TABLE answer_evidence_items ("
            "answer_id BIGINT NOT NULL,"
            "message_id BIGINT NOT NULL,"
            "rank INT NOT NULL,"
            "snippet NVARCHAR(MAX) NULL,"
            "reason NVARCHAR(64) NULL,"
            "PRIMARY KEY(answer_id, message_id)"
            ");"
        ),
        "user_persona_versions": (
            "CREATE TABLE user_persona_versions ("
            "chat_id BIGINT NOT NULL,"
            "user_id BIGINT NOT NULL,"
            "version INT NOT NULL,"
            "created_at DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME(),"
            "portrait_md NVARCHAR(MAX) NOT NULL,"
            "traits_json NVARCHAR(MAX) NOT NULL,"
            "signals_json NVARCHAR(MAX) NULL,"
            "PRIMARY KEY(chat_id, user_id, version)"
            ");"
        ),
        "IX_answer_evidence_chat": "CREATE INDEX IX_answer_evidence_chat ON answer_evidence(chat_id, created_at DESC);",
        "IX_user_persona_versions_lookup": "CREATE INDEX IX_user_persona_versions_lookup ON user_persona_versions(chat_id, user_id, version DESC);",
        # ------------------------------------------------------------------
        # Advanced karma system tables
        # ------------------------------------------------------------------
        "karma_events": (
            "CREATE TABLE karma_events ("
            "event_id BIGINT IDENTITY(1,1) PRIMARY KEY,"
            "chat_id BIGINT NOT NULL,"
            "msg_id BIGINT NULL,"
            "target_user_id BIGINT NOT NULL,"
            "rater_user_id BIGINT NOT NULL,"
            "emoji NVARCHAR(16) NULL,"
            "base FLOAT NOT NULL,"
            "rater_trust FLOAT NOT NULL,"
            "diversity FLOAT NOT NULL,"
            "anti_collusion FLOAT NOT NULL,"
            "novelty FLOAT NOT NULL,"
            "content_factor FLOAT NOT NULL,"
            "weight FLOAT NOT NULL,"
            "ts DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME()"
            ");"
        ),
        "IX_events_chat_msg": "CREATE INDEX IX_events_chat_msg ON karma_events(chat_id, msg_id) INCLUDE (ts, weight);",
        "message_scores": (
            "CREATE TABLE message_scores ("
            "chat_id BIGINT NOT NULL,"
            "msg_id BIGINT NOT NULL,"
            "author_id BIGINT NOT NULL,"
            "score_current FLOAT NOT NULL DEFAULT 0,"
            "pos INT NOT NULL DEFAULT 0,"
            "neg INT NOT NULL DEFAULT 0,"
            "last_update DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME(),"
            "CONSTRAINT PK_message_scores PRIMARY KEY (chat_id, msg_id)"
            ");"
        ),
        "IX_msg_author": "CREATE INDEX IX_msg_author ON message_scores(author_id, last_update DESC);",
        "user_rep_rollup": (
            "CREATE TABLE user_rep_rollup ("
            "chat_id BIGINT NOT NULL,"
            "user_id BIGINT NOT NULL,"
            "day DATE NOT NULL,"
            "delta_score FLOAT NOT NULL,"
            "pos INT NOT NULL,"
            "neg INT NOT NULL,"
            "skills_json NVARCHAR(MAX) NULL,"
            "CONSTRAINT PK_user_rep_rollup PRIMARY KEY (chat_id, user_id, day)"
            ");"
        ),
        "IX_user_rep_rollup_day": "CREATE INDEX IX_user_rep_rollup_day ON user_rep_rollup(day);",
        "user_rep_current": (
            "CREATE TABLE user_rep_current ("
            "chat_id BIGINT NOT NULL,"
            "user_id BIGINT NOT NULL,"
            "rep FLOAT NOT NULL DEFAULT 0,"
            "rep_global FLOAT NOT NULL DEFAULT 0,"
            "streak_days INT NOT NULL DEFAULT 0,"
            "last_seen DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME(),"
            "badges_json NVARCHAR(MAX) NULL,"
            "CONSTRAINT PK_user_rep_current PRIMARY KEY (chat_id, user_id)"
            ");"
        ),
        "IX_user_rep_rank": "CREATE INDEX IX_user_rep_rank ON user_rep_current(chat_id, rep DESC);",
        "skill_rep_current": (
            "CREATE TABLE skill_rep_current ("
            "chat_id BIGINT NOT NULL,"
            "user_id BIGINT NOT NULL,"
            "tag NVARCHAR(64) NOT NULL,"
            "rep FLOAT NOT NULL,"
            "CONSTRAINT PK_skill_rep_current PRIMARY KEY (chat_id, user_id, tag)"
            ");"
        ),
        "IX_skill_tag": "CREATE INDEX IX_skill_tag ON skill_rep_current(tag, rep DESC);",
        "karmaconfig": (
            "CREATE TABLE karmaconfig ("
            "chat_id BIGINT PRIMARY KEY,"
            "emoji_map_json NVARCHAR(MAX) NULL,"
            "decay_msg_days INT NOT NULL DEFAULT 7,"
            "decay_user_days INT NOT NULL DEFAULT 45,"
            "allow_downvotes BIT NOT NULL DEFAULT 1,"
            "daily_budget INT NOT NULL DEFAULT 18,"
            "downvote_quorum INT NOT NULL DEFAULT 4,"
            "diversity_window_hours INT NOT NULL DEFAULT 12,"
            "reciprocity_threshold FLOAT NOT NULL DEFAULT 0.30,"
            "preset NVARCHAR(32) NULL,"
            "auto_tune BIT NOT NULL DEFAULT 1"
            ");"
        ),
        "trust_table": (
            "CREATE TABLE trust_table ("
            "chat_id BIGINT NOT NULL,"
            "user_id BIGINT NOT NULL,"
            "trust FLOAT NOT NULL,"
            "upheld INT NOT NULL DEFAULT 0,"
            "overturned INT NOT NULL DEFAULT 0,"
            "tenure_days INT NOT NULL DEFAULT 0,"
            "phone_verified BIT NOT NULL DEFAULT 0,"
            "last_update DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME(),"
            "CONSTRAINT PK_trust_table PRIMARY KEY (chat_id, user_id)"
            ");"
        ),
        "karmabans": (
            "CREATE TABLE karmabans ("
            "chat_id BIGINT NOT NULL,"
            "user_id BIGINT NOT NULL,"
            "reason NVARCHAR(256) NULL,"
            "until_ts DATETIME2 NULL,"
            "CONSTRAINT PK_karmabans PRIMARY KEY (chat_id, user_id)"
            ");"
        ),
    }
    try:
        with conn.cursor() as cursor:
            logger.info("Initializing database tables...")
            for name, query in table_queries.items():
                # Some queries are defined as tuples for readability; join them into a single string.
                query_str = (
                    " ".join(query) if isinstance(query, (tuple, list)) else query
                )

                is_idx = name.startswith("IX_")
                obj_type = "INDEX" if is_idx else "TABLE"
                obj_name_to_check = name  # For tables, this is the table name. For indexes, this is the index name.
                table_for_index = ""
                if is_idx:
                    # Extract table name from the CREATE INDEX statement.
                    # Use a word boundary before 'ON' so we don't match strings like
                    # 'NameVariation ON'.
                    match = re.search(r"\bON\s+([\w\.]+)", query_str, re.IGNORECASE)
                    if match:
                        table_for_index = match.group(1)
                    else:
                        logger.warning(f"Could not determine table for index {name}")
                        continue

                check_q = (
                    "SELECT 1 FROM sys.indexes i INNER JOIN sys.objects o ON i.object_id = o.object_id "
                    "WHERE i.name = ? AND o.name = ?"
                    if is_idx
                    else "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = ?"
                )
                params_check = (
                    (obj_name_to_check, table_for_index)
                    if is_idx
                    else (obj_name_to_check,)
                )

                cursor.execute(check_q, params_check)
                if cursor.fetchone():
                    logger.info(f"{obj_type} '{obj_name_to_check}' already exists.")
                else:
                    logger.info(
                        f"{obj_type} '{obj_name_to_check}' not found. Creating..."
                    )
                    cursor.execute(query_str)
                    logger.info(f"{obj_type} '{obj_name_to_check}' created.")

            # Ensure karma-related columns exist on UserProfiles for backwards compatibility
            karma_columns = {
                "KarmaReceived": "INT NOT NULL DEFAULT 0",
                "KarmaGiven": "INT NOT NULL DEFAULT 0",
                "HateGiven": "INT NOT NULL DEFAULT 0",
            }
            for col_name, definition in karma_columns.items():
                cursor.execute(
                    "SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'UserProfiles' AND COLUMN_NAME = ?",
                    (col_name,),
                )
                if cursor.fetchone():
                    logger.info(
                        f"Column '{col_name}' already exists in 'UserProfiles'."
                    )
                else:
                    logger.info(
                        f"Column '{col_name}' missing in 'UserProfiles'. Adding..."
                    )
                    cursor.execute(
                        f"ALTER TABLE UserProfiles ADD {col_name} {definition}"
                    )
                    logger.info(f"Column '{col_name}' added to 'UserProfiles'.")

            # Ensure fact-check log has optional columns
            fc_columns = {
                "Track": "NVARCHAR(8) NULL CHECK (Track IN ('news','book'))",
                "Details": "NVARCHAR(MAX) NULL",
            }
            for col_name, definition in fc_columns.items():
                cursor.execute(
                    "SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'FactCheckLog' AND COLUMN_NAME = ?",
                    (col_name,),
                )
                if cursor.fetchone():
                    logger.info(
                        f"Column '{col_name}' already exists in 'FactCheckLog'."
                    )
                else:
                    logger.info(
                        f"Column '{col_name}' missing in 'FactCheckLog'. Adding..."
                    )
                    cursor.execute(
                        f"ALTER TABLE FactCheckLog ADD {col_name} {definition}"
                    )
                    logger.info(
                        f"Column '{col_name}' added to 'FactCheckLog'."
                    )

            logger.info("Database initialization check complete.")
    except Exception as e:
        logger.error(f"DB init error: {e}", exc_info=True)
    finally:
        if conn:
            conn.autocommit = False
            conn.close()



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/lang_router.py ---
======================================================================

import re
import unicodedata


def normalize(s: str) -> str:
    """Normalize unicode string and drop zero-width characters.

    This utility applies NFKC normalization and removes common zero-width
    characters that may cause message truncation or other subtle bugs.
    """
    s = unicodedata.normalize("NFKC", s or "")
    return re.sub(r"[\u200B\u200C\u200D\u200E\u2060]", "", s).strip()



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/logging_config.py ---
======================================================================

# enkibot/utils/logging_config.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
# === EnkiBot Logging Configuration ===
# ==================================================================================================
# Sets up application-wide structured logging to both file and console.
# Includes a custom logging handler to write critical errors to the SQL database
# and clears the previous log file on startup.
# ==================================================================================================

import logging
import traceback
import pyodbc
import os  # <--- IMPORT OS MODULE
from enkibot import config

def setup_logging():
    """Initializes the logging configuration for the entire application."""

    # --- START: CLEAR PREVIOUS LOG FILE ---
    log_file_name = "bot_activity.log"
    try:
        if os.path.exists(log_file_name):
            os.remove(log_file_name)
            # This print goes to console before logging is fully set up for the bot's logger.
            print(f"INFO: Previous log file '{log_file_name}' removed successfully.")
    except OSError as e:
        # This print also goes to console.
        print(f"WARNING: Error removing previous log file '{log_file_name}': {e}")
    # --- END: CLEAR PREVIOUS LOG FILE ---
    
    # Define a custom handler for logging errors to the database
    class SQLDBLogHandler(logging.Handler):
        """
        A logging handler that writes log records with level ERROR or higher
        to a dedicated table in the SQL Server database.
        """
        def __init__(self):
            super().__init__()
            self.conn = None

        def _get_db_conn_for_logging(self):
            """Establishes a database connection specifically for logging."""
            if not config.DB_CONNECTION_STRING:
                return None
            try:
                # Use autocommit=True for logging to ensure errors are written immediately.
                return pyodbc.connect(config.DB_CONNECTION_STRING, autocommit=True)
            except pyodbc.Error as e:
                # If the DB is down, we can't log to it. Provide immediate console feedback.
                print(
                    "WARNING: SQLDBLogHandler could not connect to the database for logging:",
                    e,
                )
                return None

        def emit(self, record: logging.LogRecord):
            """
            Writes the log record to the ErrorLog table in the database.
            """
            if self.conn is None:
                self.conn = self._get_db_conn_for_logging()

            if self.conn:
                try:
                    msg = self.format(record)
                    exc_info_str = traceback.format_exc() if record.exc_info else None
                    sql = (
                        "INSERT INTO ErrorLog (LogLevel, LoggerName, ModuleName, FunctionName, "
                        "LineNumber, ErrorMessage, ExceptionInfo) VALUES (?, ?, ?, ?, ?, ?, ?)"
                    )
                    with self.conn.cursor() as cursor:
                        cursor.execute(
                            sql,
                            record.levelname,
                            record.name,
                            record.module,
                            record.funcName,
                            record.lineno,
                            msg,
                            exc_info_str,
                        )
                except pyodbc.Error as e:
                    # If an error occurs during logging, handle it and sever the connection.
                    print(
                        "WARNING: SQLDBLogHandler failed to write log record to database:",
                        e,
                    )
                    self.handleError(record)
                    self.conn = None  # Reset connection to be re-established on next emit.
        
        def close(self):
            """Closes the database connection if it's open."""
            if self.conn:
                try:
                    self.conn.close()
                except pyodbc.Error:
                    pass
            super().close()

    # --- Main Logging Configuration ---
    log_level_name = os.getenv("ENKI_LOG_LEVEL", "DEBUG").upper()
    log_level = getattr(logging, log_level_name, logging.DEBUG)
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s'
    
    # Configure root logger
    # By using basicConfig with force=True (Python 3.8+), we can reconfigure if needed.
    # For older Python, ensure basicConfig is called only once application-wide.
    # Given setup_logging() is called once from main.py, this should be fine.
    logging.basicConfig(
        format=log_format,
        level=log_level,
        handlers=[
            logging.FileHandler(log_file_name, encoding='utf-8'), # Use variable
            logging.StreamHandler()
        ]
        # force=True # Add this if using Python 3.8+ and re-running setup_logging,
                   # but it should not be necessary with current structure.
    )
    
    module_logger = logging.getLogger(__name__) # Logger for this specific module (logging_config.py)

    # Add the custom DB handler if the database is configured
    if config.DB_CONNECTION_STRING:
        db_log_handler = SQLDBLogHandler()
        db_log_handler.setLevel(logging.ERROR) # Only log ERROR and CRITICAL to DB
        formatter = logging.Formatter(log_format)
        db_log_handler.setFormatter(formatter)
        logging.getLogger().addHandler(db_log_handler) # Add to the root logger
        module_logger.info("Configured logging of ERROR-level messages to the SQL database.")
    else:
        module_logger.warning("Logging to SQL database is NOT configured (DB_CONNECTION_STRING is missing).")



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/message_utils.py ---
======================================================================

# enkibot/utils/message_utils.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY and FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
#
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------

"""Helper utilities for working with Telegram messages."""

from __future__ import annotations

from typing import Any
import logging
import re
from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode

logger = logging.getLogger(__name__)


def is_forwarded_message(message: Any) -> bool:
    """Return ``True`` if the provided Telegram *message* appears to be forwarded.

    This checks several possible attributes that may indicate a forwarded
    message across different versions of the Telegram Bot API. Any missing
    attributes are safely ignored to avoid raising :class:`AttributeError`.
    """
    if message is None:
        return False

    forward_from = getattr(message, "forward_from", None)
    forward_from_chat = getattr(message, "forward_from_chat", None)
    if forward_from or forward_from_chat:
        logger.info("Forwarded message from %s", forward_from or forward_from_chat)
        return True

    attrs_to_check = (
        "forward_origin",
        "forward_sender_name",
        "forward_date",
    )
    for attr in attrs_to_check:
        try:
            if getattr(message, attr):
                return True
        except AttributeError:
            continue

    try:
        return bool(getattr(message, "is_automatic_forward"))
    except AttributeError:
        return False


def get_text(message: Any) -> str | None:
    """Return text or caption from a Telegram *message*.

    Many messages such as photos or videos carry their textual content in the
    ``caption`` field rather than ``text``.  This helper consolidates both
    attributes so handlers can process any user supplied text without worrying
    about the underlying message type.
    """

    if message is None:
        return None

    text = getattr(message, "text", None)
    caption = getattr(message, "caption", None)
    return text or caption


URL_PATTERN = re.compile(r"https?://\S+")


def _strip_utm_source(url: str) -> str:
    """Remove ``utm_source=openai`` query parameters from a single *url*."""
    parts = urlsplit(url)
    query_params = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if not (k == "utm_source" and v == "openai")]
    new_query = urlencode(query_params)
    return urlunsplit((parts.scheme, parts.netloc, parts.path, new_query, parts.fragment))


def clean_output_text(text: str | None) -> str | None:
    """Sanitize bot replies by removing tracking parameters and duplicate lines.

    - Strips ``utm_source=openai`` from any URLs in *text*.
    - Removes ``?utm_source=openai`` fragments that appear outside of URLs.
    - Removes consecutive duplicate lines to avoid repeated content.

    Returns ``None`` if the cleaned text would be empty.
    """
    if not text:
        return None

    def repl(match: re.Match[str]) -> str:
        url = match.group(0)
        trailing = ""
        while url and url[-1] in ").,]":
            trailing = url[-1] + trailing
            url = url[:-1]
        return _strip_utm_source(url) + trailing

    cleaned = URL_PATTERN.sub(repl, text)
    # Also drop tracking fragments left as plain text
    cleaned = re.sub(r"[?&]utm_source=openai", "", cleaned)

    deduped_lines: list[str] = []
    prev_line: str | None = None
    for line in cleaned.splitlines():
        if line != prev_line:
            deduped_lines.append(line)
        prev_line = line

    cleaned_joined = "\n".join(deduped_lines).strip()
    return cleaned_joined or None



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/news_channels.py ---
======================================================================

# enkibot/utils/news_channels.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
"""Utilities for retrieving and parsing Telegram news channel usernames."""

from __future__ import annotations

import logging
import re
from typing import List

import httpx

logger = logging.getLogger(__name__)

NEWS_CHANNELS_URL = "https://tlgrm.ru/channels/news"
# Additional category to scrape alongside the default news page.  The exact
# category is not critical for tests as network calls are mocked, but in
# production this points to a second TLGRM catalogue page.
TECH_CHANNELS_URL = "https://tlgrm.ru/channels/technology"

# List of TLGRM catalogue pages that should be scraped.  The fetch helper will
# iterate over all URLs in this sequence.
CHANNEL_CATEGORY_URLS = [NEWS_CHANNELS_URL, TECH_CHANNELS_URL]

# Only pick channel usernames from links to tlgrm or tg://resolve to avoid
# matching image resolution hints like ``@2x`` that appear in the page markup.
_CHANNEL_PATTERN = re.compile(
    r'href=["\'](?:https://tlgrm\.ru/channels/@|tg://resolve\?domain=)([A-Za-z0-9_]+)["\']'
)


def extract_channel_usernames(html: str) -> List[str]:
    """Extract unique channel usernames from *html* content.

    Returned usernames do not include the leading '@'.
    """
    usernames = sorted(set(_CHANNEL_PATTERN.findall(html)))
    logger.info("Extracted %d channel usernames from HTML", len(usernames))
    return usernames


async def fetch_channel_usernames() -> List[str]:
    """Fetch TLGRM channel directories and return unique usernames.

    Each page listed in :data:`CHANNEL_CATEGORY_URLS` is retrieved.  The TLGRM
    catalogue uses endless scrolling and returns only the first page of
    channels on the initial request.  Additional pages are accessible via the
    ``?page=N`` query parameter.  This helper follows the pagination links for
    every configured page so that callers receive the full list of available
    channels.
    """

    logger.info(
        "Requesting channel directories from %s", ", ".join(CHANNEL_CATEGORY_URLS)
    )
    try:
        usernames: set[str] = set()
        total_pages = 0
        async with httpx.AsyncClient(timeout=30.0) as client:
            for base_url in CHANNEL_CATEGORY_URLS:
                page = 1
                last_page = None
                while True:
                    url = base_url if page == 1 else f"{base_url}?page={page}"
                    resp = await client.get(url)
                    logger.info(
                        "Received response %s with %d bytes for %s page %d",
                        resp.status_code,
                        len(resp.text),
                        base_url,
                        page,
                    )
                    resp.raise_for_status()

                    usernames.update(extract_channel_usernames(resp.text))

                    if last_page is None:
                        match = re.search(r'data-last-page="(\d+)"', resp.text)
                        last_page = int(match.group(1)) if match else 1
                        logger.info("Detected %d pages in total for %s", last_page, base_url)

                    if page >= last_page:
                        break
                    page += 1
                total_pages += page

        names = sorted(usernames)
        logger.info(
            "Fetched %d total usernames from %d pages across %d directories",
            len(names),
            total_pages,
            len(CHANNEL_CATEGORY_URLS),
        )
        return names
    except Exception as exc:  # pragma: no cover - network errors
        logger.error("Failed to fetch channel directories: %s", exc)
        return []



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/provider_metrics.py ---
======================================================================

from dataclasses import dataclass

@dataclass
class ProviderMetrics:
    """Simple container for provider performance statistics."""
    total_requests: int = 0
    total_latency: float = 0.0
    total_tokens: int = 0
    total_cost: float = 0.0

    def record(self, latency: float, tokens: int = 0, cost_per_1k: float = 0.0) -> None:
        """Record a new request's metrics."""
        self.total_requests += 1
        self.total_latency += latency
        self.total_tokens += tokens
        self.total_cost += (tokens / 1000.0) * cost_per_1k



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/quota_middleware.py ---
======================================================================

import logging
from enkibot import config
from .database import DatabaseManager

logger = logging.getLogger(__name__)

async def enforce_user_quota(db_manager: DatabaseManager, user_id: int, usage_type: str) -> bool:
    """Check and increment usage for a user. Returns True if under quota."""
    usage_type = usage_type.lower()
    if usage_type not in {"llm", "image"}:
        logger.error("Invalid usage_type '%s' for quota check", usage_type)
        return False
    limit = config.DAILY_LLM_QUOTA if usage_type == "llm" else config.DAILY_IMAGE_QUOTA
    if limit <= 0:
        return True
    allowed = await db_manager.check_and_increment_usage(user_id, usage_type, limit)
    if not allowed:
        logger.info("User %s exceeded %s quota", user_id, usage_type)
    return allowed



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/text_splitter.py ---
======================================================================

# enkibot/utils/text_splitter.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -------------------------------------------------------------------------------
# Future Improvements:
# - Improve modularity to support additional features and services.
# - Enhance error handling and logging for better maintenance.
# - Expand unit tests to cover more edge cases.
# -------------------------------------------------------------------------------
"""Utility helpers for splitting long text into smaller chunks."""

from __future__ import annotations

from typing import List
import re

MAX_TELEGRAM_MESSAGE_LENGTH = 4096


def split_text_into_chunks(text: str, max_chunk_size: int = MAX_TELEGRAM_MESSAGE_LENGTH) -> List[str]:
    """Split *text* into chunks not exceeding *max_chunk_size* characters.

    The function attempts to respect whitespace boundaries so that words or
    existing newlines are not arbitrarily broken whenever possible.
    """
    if not text:
        return []

    tokens = re.split(r"(\s+)", text)
    chunks: List[str] = []
    current = ""

    for token in tokens:
        if len(current) + len(token) <= max_chunk_size:
            current += token
        else:
            if current:
                chunks.append(current.rstrip())
            current = token.lstrip()
            # If a single token itself exceeds max_chunk_size, hard split it
            while len(current) > max_chunk_size:
                chunks.append(current[:max_chunk_size])
                current = current[max_chunk_size:]

    if current:
        chunks.append(current.rstrip())

    return [c for c in chunks if c]



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/trigger_extractor.py ---
======================================================================

import re
import unicodedata
from typing import Iterable, Tuple

NAME_ALIASES_DEFAULT = [
    r"энки", r"енки", r"энкі", r"енкі", r"enki",
]


def _normalize(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("\u200b", "")
    return text.strip()


def extract_assistant_prompt(text: str, aliases: Iterable[str], bot_username: str | None = None) -> Tuple[bool, str, str]:
    """Return (triggered, content, alias) from text after bot-name prefix.

    Args:
        text: Original user text.
        aliases: Iterable of alias strings to match.
        bot_username: Telegram username of the bot, if available.
    """
    t = _normalize(text)

    # Normalize and case-fold aliases for robust matching across scripts
    alias_list = {_normalize(a).casefold() for a in aliases if a}
    alias_list.update({_normalize(a).casefold() for a in NAME_ALIASES_DEFAULT})
    patterns = [re.escape(a) for a in alias_list]
    if bot_username:
        bot_username = _normalize(bot_username).casefold()
        patterns.append(re.escape(bot_username))
        patterns.append("@" + re.escape(bot_username))
        if not bot_username.endswith("bot"):
            patterns.append("@" + re.escape(bot_username) + "bot")
    if not patterns:
        return False, "", ""
    alias_group = "|".join(patterns)
    name_re = re.compile(
        rf"^(?:эй[,!:]?\s+|hey[,!:]?\s+)?(?P<alias>{alias_group})[\s,.:;!–—-]*(?P<content>.*)$",
        re.IGNORECASE | re.UNICODE,
    )
    m = name_re.match(t)
    if not m:
        return False, "", ""
    return True, m.group("content").strip(), m.group("alias")



