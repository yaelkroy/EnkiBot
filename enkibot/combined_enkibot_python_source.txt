======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/EnkiBot V1.py ---
======================================================================

﻿# ==================================================================================================
# === EnkiBot - Умный ассистент для Telegram с долгосрочной памятью и AI-функциями ===
# ==================================================================================================
#
# Ключевые особенности:
# - Оркестрация нескольких LLM: Одновременные запросы к разным моделям (OpenAI, Groq и др.)
#   с выбором самого быстрого ответа для скорости и надежности.
# - Долгосрочная память: Интеграция с базой данных MS SQL Server для хранения истории чатов,
#   профилей пользователей и логов.
# - Контекстный поиск по памяти: Возможность спрашивать о прошлых событиях или людях в чате.
#   Бот динамически находит релевантную информацию в логах.
# - Динамическое распознавание имен: Понимает имена пользователей (включая транслит) и уточняет,
#   если запрос неоднозначен.
# - Автоматическое профилирование пользователей: Бот анализирует сообщения пользователя для
#   создания и обновления профиля его интересов.
# - Встроенные функции: Получение новостей и актуального прогноза погоды.
#
# ==================================================================================================

import logging
import json
import os
import openai # Для OpenAI API
import pyodbc # Для MS SQL Server
import requests # Для синхронных запросов к News API
import httpx # Для асинхронных HTTP-запросов к API
import traceback # Для детального логирования исключений
import re # Для регулярных выражений
from telegram import Update, ForceReply
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.constants import ChatAction
import asyncio
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
import pymorphy3 # Для морфологии русского языка
from datetime import datetime # Для работы с датой и временем
from transliterate import translit
from transliterate.exceptions import LanguagePackNotFound # ИСПРАВЛЕННЫЙ ИМПОРТ

# --- Инициализация библиотек ---
DetectorFactory.seed = 0
morph = pymorphy3.MorphAnalyzer()

# --- Конфигурация: Загрузка ключей и настроек из переменных окружения ---
# Важно: Никогда не храните ключи API прямо в коде.
TELEGRAM_BOT_TOKEN = os.getenv('ENKI_BOT_TOKEN')
NEWS_API_KEY = os.getenv('ENKI_BOT_NEWS_API_KEY')
WEATHER_API_KEY = os.getenv('ENKI_BOT_WEATHER_API_KEY') # Ключ для OpenWeatherMap

# Настройки подключения к базе данных
SQL_SERVER_NAME = os.getenv('ENKI_BOT_SQL_SERVER_NAME')
SQL_DATABASE_NAME = os.getenv('ENKI_BOT_SQL_DATABASE_NAME')

# Ключи и модели для различных LLM-провайдеров
OPENAI_API_KEY = os.getenv('ENKI_BOT_OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MODEL_ID', 'gpt-4o-mini')
GROQ_API_KEY = os.getenv('ENKI_BOT_GROQ_API_KEY')
GROQ_MODEL_ID = os.getenv('ENKI_BOT_GROQ_MODEL_ID', 'llama3-8b-8192')
GROQ_ENDPOINT_URL = "https://api.groq.com/openai/v1/chat/completions"
OPENROUTER_API_KEY = os.getenv('ENKI_BOT_OPENROUTER_API_KEY')
OPENROUTER_MODEL_ID = os.getenv('ENKI_BOT_OPENROUTER_MODEL_ID', 'mistralai/mistral-7b-instruct:free')
OPENROUTER_ENDPOINT_URL = "https://openrouter.ai/api/v1/chat/completions"
GOOGLE_AI_API_KEY = os.getenv('ENKI_BOT_GOOGLE_AI_API_KEY')
GOOGLE_AI_MODEL_ID = os.getenv('ENKI_BOT_GOOGLE_AI_MODEL_ID', 'gemini-1.5-flash-latest')

# --- Константы ---
# Имена и никнеймы, на которые бот будет реагировать в групповых чатах
BOT_NICKNAMES_TO_CHECK = [ "enki", "enkibot", "энки", "энкибот", "бот", "bot" ]

# --- Настройка логирования ---
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler("bot_activity.log", encoding='utf-8'), # Логи в файл
        logging.StreamHandler() # Логи в консоль
    ]
)
logger = logging.getLogger(__name__)

# --- Конфигурация разрешенных групп ---
ALLOWED_GROUP_IDS_STR = os.getenv('ENKI_BOT_ALLOWED_GROUP_IDS') 
ALLOWED_GROUP_IDS = set()
if ALLOWED_GROUP_IDS_STR:
    try:
        # Эта логика для разбора строки у вас правильная
        ALLOWED_GROUP_IDS = set(int(id_str.strip()) for id_str in ALLOWED_GROUP_IDS_STR.split(','))
        if ALLOWED_GROUP_IDS:
            logger.info(f"Бот ограничен группами с ID: {ALLOWED_GROUP_IDS}")
    except ValueError:
        # Эта логика для обработки ошибок у вас тоже правильная
        logger.error(f"Неверный формат ENKI_BOT_ALLOWED_GROUP_IDS: '{ALLOWED_GROUP_IDS_STR}'. Ограничение по группам снято.")
        ALLOWED_GROUP_IDS = set()
else:
    logger.info("ENKI_BOT_ALLOWED_GROUP_IDS не задан. Бот будет работать во всех группах.")


# --- Подключение к базе данных и инициализация ---
DB_CONNECTION_STRING = None
if SQL_SERVER_NAME and SQL_DATABASE_NAME:
    DB_CONNECTION_STRING = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={SQL_SERVER_NAME};"
        f"DATABASE={SQL_DATABASE_NAME};"
        f"Trusted_Connection=yes;"
    )
    logger.info(f"Строка подключения к БД сконфигурирована для {SQL_SERVER_NAME}/{SQL_DATABASE_NAME}")
else:
    logger.warning("Переменные окружения для SQL Server не заданы. Функционал БД отключен.")

def get_db_connection():
    if not DB_CONNECTION_STRING: return None
    try: return pyodbc.connect(DB_CONNECTION_STRING, autocommit=False)
    except pyodbc.Error as ex:
        logger.error(f"Ошибка подключения к БД: {ex.args[0]} - {ex}")
        return None

def initialize_database():
    # Эта функция проверяет наличие необходимых таблиц в БД и создает их, если они отсутствуют.
    # Вызывается один раз при старте бота.
    if not DB_CONNECTION_STRING:
        logger.warning("Невозможно инициализировать БД: строка подключения не настроена.")
        return
    # Словарь с запросами на создание таблиц и индексов к ним для ускорения выборок
    table_creation_queries = {
        "ConversationHistory": "CREATE TABLE ConversationHistory (MessageDBID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageID BIGINT NULL, Role NVARCHAR(50) NOT NULL, Content NVARCHAR(MAX) NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ConversationHistory_ChatID_Timestamp": "CREATE INDEX IX_ConversationHistory_ChatID_Timestamp ON ConversationHistory (ChatID, Timestamp DESC);",
        "ChatLog": "CREATE TABLE ChatLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, MessageID BIGINT NOT NULL, MessageText NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ChatLog_ChatID_Timestamp": "CREATE INDEX IX_ChatLog_ChatID_Timestamp ON ChatLog (ChatID, Timestamp DESC);",
        "IX_ChatLog_UserID": "CREATE INDEX IX_ChatLog_UserID ON ChatLog (UserID);",
        "ErrorLog": "CREATE TABLE ErrorLog (ErrorID INT IDENTITY(1,1) PRIMARY KEY, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, LogLevel NVARCHAR(50) NOT NULL, LoggerName NVARCHAR(255) NULL, ModuleName NVARCHAR(255) NULL, FunctionName NVARCHAR(255) NULL, LineNumber INT NULL, ErrorMessage NVARCHAR(MAX) NOT NULL, ExceptionInfo NVARCHAR(MAX) NULL);",
        "IX_ErrorLog_Timestamp": "CREATE INDEX IX_ErrorLog_Timestamp ON ErrorLog (Timestamp DESC);",
        "UserProfiles": "CREATE TABLE UserProfiles (UserID BIGINT PRIMARY KEY, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, LastName NVARCHAR(255) NULL, LastSeen DATETIME2 DEFAULT GETDATE(), MessageCount INT DEFAULT 0, PreferredLanguage NVARCHAR(10) NULL, Notes NVARCHAR(MAX) NULL, ProfileLastUpdated DATETIME2 DEFAULT GETDATE());",
        # --- НАЧАЛО НОВЫХ СТРОК ---
        "UserNameVariations": "CREATE TABLE UserNameVariations (VariationID INT IDENTITY(1,1) PRIMARY KEY, UserID BIGINT NOT NULL, NameVariation NVARCHAR(255) NOT NULL, FOREIGN KEY (UserID) REFERENCES UserProfiles(UserID) ON DELETE CASCADE);",
        "IX_UserNameVariations_NameVariation": "CREATE UNIQUE INDEX IX_UserNameVariations_NameVariation ON UserNameVariations (UserID, NameVariation);"
        # --- КОНЕЦ НОВЫХ СТРОК ---
    }
    conn = None
    try:
        conn = pyodbc.connect(DB_CONNECTION_STRING, autocommit=True)
        cursor = conn.cursor()
        logger.info("Проверка и создание таблиц БД при необходимости...")
        for item_name, query in table_creation_queries.items():
            is_index = item_name.startswith("IX_")
            table_name_for_check = item_name.split("_")[1] if is_index else item_name
            if is_index:
                try:
                    cursor.execute(query)
                    logger.info(f"Индекс {item_name} создан или подтвержден.")
                except pyodbc.ProgrammingError as pe:
                    if "already an index" in str(pe).lower() or "already exists" in str(pe).lower():
                        logger.info(f"Индекс {item_name} уже существует.")
                    else: raise
                continue
            check_sql = "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = ?"
            cursor.execute(check_sql, table_name_for_check)
            if cursor.fetchone(): logger.info(f"Таблица {table_name_for_check} уже существует.")
            else:
                logger.info(f"Таблица {table_name_for_check} не найдена. Создание..."); cursor.execute(query); logger.info(f"Таблица {table_name_for_check} создана.")
        cursor.close(); logger.info("Проверка инициализации БД завершена.")
    except pyodbc.Error as ex: logger.error(f"Ошибка инициализации БД: {ex}", exc_info=True)
    except Exception as e: logger.error(f"Неожиданная ошибка во время инициализации БД: {e}", exc_info=True)
    finally:
        if conn: conn.close()

# --- Логирование ошибок в БД ---
class SQLDBLogHandler(logging.Handler):
    # Этот класс позволяет автоматически записывать критические ошибки Python прямо в таблицу ErrorLog в БД.
    def __init__(self): super().__init__(); self.conn = None
    def _get_db_conn_for_logging(self):
        if not DB_CONNECTION_STRING: return None
        try: return pyodbc.connect(DB_CONNECTION_STRING, autocommit=True)
        except pyodbc.Error: return None
    def emit(self, record: logging.LogRecord):
        if self.conn is None: self.conn = self._get_db_conn_for_logging()
        if self.conn:
            try:
                msg, exc_info_str = self.format(record), traceback.format_exc() if record.exc_info else None
                sql = "INSERT INTO ErrorLog (LogLevel, LoggerName, ModuleName, FunctionName, LineNumber, ErrorMessage, ExceptionInfo) VALUES (?, ?, ?, ?, ?, ?, ?)"
                with self.conn.cursor() as c: c.execute(sql, record.levelname, record.name, record.module, record.funcName, record.lineno, msg, exc_info_str)
            except: self.handleError(record); self.conn = None
    def close(self):
        if self.conn:
            try: self.conn.close()
            except: pass
        super().close()

if DB_CONNECTION_STRING:
    db_log_handler = SQLDBLogHandler(); db_log_handler.setLevel(logging.ERROR)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
    db_log_handler.setFormatter(formatter); logging.getLogger().addHandler(db_log_handler)
    logger.info("Настроено логирование ошибок в SQL БД.")
else: logger.warning("Логирование ошибок в SQL БД НЕ настроено.")

# --- Инициализация клиентов API ---
openai_async_client = None
if OPENAI_API_KEY:
    try: openai_async_client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY); logger.info("Клиент OpenAI AsyncOpenAI инициализирован.")
    except Exception as e: logger.error(f"Не удалось инициализировать клиент OpenAI AsyncOpenAI: {e}")
else: logger.warning("Ключ OpenAI API не найден. Вызовы к OpenAI отключены.")

# --- Вспомогательные функции ---
# --- В разделе "Вспомогательные функции" ---

# Эта функция находится в вашем основном файле EnkiBot.py

async def populate_name_variations_with_llm(user_id: int, first_name: str, last_name: str | None, username: str | None):
    """
    Использует LLM для генерации ЛИНГВИСТИЧЕСКИХ вариантов имени пользователя,
    включая уменьшительные формы, транслитерацию и падежи.
    """
    if not openai_async_client:
        logger.warning(f"Генерация вариантов имени для user {user_id} пропущена: OpenAI клиент не настроен.")
        return

    name_parts = [part for part in [first_name, last_name, username] if part]
    name_info = ", ".join(name_parts)
    
    logger.info(f"Запрос на ЛИНГВИСТИЧЕСКУЮ генерацию вариантов для пользователя {user_id} ({name_info}).")

    # --- НОВЫЙ, СУПЕР-СФОКУСИРОВАННЫЙ ПРОМПТ ---
    system_prompt = (
        "You are a language expert specializing in Russian and English names. Your task is to generate a list of linguistic variations for a user's name. Focus ONLY on realistic, human-used variations. DO NOT generate technical usernames with numbers or suffixes like '_dev'."
        "\n\n**Goal:** Create variations for recognition in natural language text."
        "\n\n**Categories for Generation:**"
        "\n1.  **Original Forms:** The original first name, last name, and combinations."
        "\n2.  **Diminutives & Nicknames:** Common short and affectionate forms (e.g., 'Антонина' -> 'Тоня'; 'Robert' -> 'Rob')."
        "\n3.  **Transliteration (with variants):** Provide multiple common Latin spellings for all Cyrillic forms (original and diminutives). Example for 'Тоня': 'tonya', 'tonia'."
        "\n4.  **Reverse Transliteration:** If the source name is Latin, provide plausible Cyrillic versions. Example for 'Yael': 'Яэль', 'Йаэль'."
        "\n5.  **Russian Declensions (Grammatical Cases):** For all primary Russian names (full and short forms), provide their forms in different grammatical cases (genitive, dative, accusative, instrumental, prepositional). Example for 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'."
        "\n\n**Output Format:** Return a single JSON object: {\"variations\": [\"variation1\", \"variation2\", ...]}. All variations must be in lowercase."
    )
    
    user_prompt = f"Generate linguistic variations for the user with the following info: {name_info}"

    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    name_variations = set()
    try:
        completion = await openai_async_client.chat.completions.create(
            model='gpt-4o-mini',
            messages=messages_for_api,
            temperature=0.3, # Снижаем температуру для более предсказуемых, основанных на правилах результатов
            response_format={"type": "json_object"}
        )
        if completion.choices and completion.choices[0].message:
            response_str = completion.choices[0].message.content
            try:
                data = json.loads(response_str)
                variations_list = data.get('variations')
                if isinstance(variations_list, list):
                    name_variations.update([str(v).lower().strip() for v in variations_list if v and str(v).strip()])
                    logger.info(f"LLM сгенерировала {len(variations_list)} лингвистических вариантов для user {user_id}.")
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Ошибка декодирования JSON от LLM для user {user_id}: {response_str}. Ошибка: {e}")
    except Exception as e:
        logger.error(f"Ошибка OpenAI при генерации вариантов имени для {user_id}: {e}")

    # Добавляем оригинальные имена еще раз
    name_variations.update([p.lower() for p in name_parts])
    
    # Сохраняем все уникальные варианты в БД
    if name_variations:
        db_conn = get_db_connection()
        if db_conn:
            try:
                with db_conn.cursor() as cursor:
                    sql = """
                        MERGE INTO UserNameVariations AS t
                        USING (SELECT ? AS UserID, ? AS NameVariation) AS s
                        ON (t.UserID = s.UserID AND t.NameVariation = s.NameVariation)
                        WHEN NOT MATCHED THEN
                            INSERT (UserID, NameVariation) VALUES (s.UserID, s.NameVariation);
                    """
                    params_to_insert = [(user_id, var) for var in name_variations if var]
                    if params_to_insert:
                        cursor.executemany(sql, params_to_insert)
                        db_conn.commit()
                        logger.info(f"Сохранено/обновлено {len(params_to_insert)} вариантов имени для user {user_id}.")
            except pyodbc.Error as ex:
                logger.error(f"Ошибка БД при сохранении вариантов имени для {user_id}: {ex}")
                if db_conn: db_conn.rollback()
            finally:
                if db_conn: db_conn.close()
def get_translit_variations(name: str) -> set[str]:
    """
    Создает набор вариаций имени, включая оригинал и его транслитерацию.
    Работает в обе стороны: с кириллицы на латиницу и наоборот.
    """
    variations = {name.lower()}
    try:
        # Проверяем, содержит ли имя кириллические символы
        if re.search('[а-яА-Я]', name):
            translit_name = translit(name, 'ru', reversed=True) # ru -> en
            variations.add(translit_name.lower())
        else:
            translit_name = translit(name, 'ru') # en -> ru
            variations.add(translit_name.lower())
    except LanguagePackNotFound: # ИСПРАВЛЕННАЯ ОШИБКА
        logger.warning(f"Пакет для транслитерации 'ru' не найден. Пропускаем для имени: {name}")
    except Exception as e:
        logger.error(f"Неожиданная ошибка при транслитерации имени {name}: {e}")
    return variations
async def analyze_replied_message(original_text: str, user_question: str) -> str:
    """
    Анализирует исходный текст (на который ответили) в контексте вопроса пользователя.
    """
    logger.info(f"Запрос на анализ текста. Длина исходного текста: {len(original_text)}, Вопрос: '{user_question}'")
    
    # Создаем очень четкий промпт для LLM, чтобы она поняла свою задачу
    system_prompt = (
        "Ты — AI-аналитик. Твоя задача — проанализировать 'Исходный текст' и дать содержательный ответ на 'Вопрос пользователя' об этом тексте. "
        "Твой анализ должен быть объективным, кратким и по существу. Если вопрос общий (например, 'что думаешь?'), "
        "сделай краткое резюме, выделив ключевые тезисы или настроения в исходном тексте."
    )

    user_prompt = f"""
Исходный текст для анализа:
---
"{original_text}"
---

Вопрос пользователя об этом тексте:
---
"{user_question}"
---

Твой анализ:
"""

    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # Для этой задачи можно использовать любую из ваших LLM. OpenAI хорошо подходит для анализа.
    if openai_async_client:
        try:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini', # Быстрая и умная модель для таких задач
                messages=messages_for_api,
                temperature=0.5, # Чуть больше креативности для анализа
                max_tokens=1000
            )
            if completion.choices and completion.choices[0].message:
                return completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Ошибка OpenAI при анализе сообщения: {e}", exc_info=True)
            return "К сожалению, произошла ошибка во время анализа текста."
    
    return "Функция анализа не может быть выполнена, так как AI-клиент не настроен."
# --- Функции для вызова API ---
async def call_openai_llm(messages_for_api: list) -> str | None:
    if not openai_async_client: logger.warning("Клиент OpenAI недоступен."); return None
    logger.info(f"Вызов OpenAI (модель: {OPENAI_MODEL_ID}) с {len(messages_for_api)} сообщениями контекста.")
    try:
        completion = await openai_async_client.chat.completions.create(model=OPENAI_MODEL_ID, messages=messages_for_api, temperature=0.7, max_tokens=2000)
        if completion.choices and completion.choices[0].message: return completion.choices[0].message.content.strip()
    except Exception as e: logger.error(f"Ошибка при работе с OpenAI API: {e}", exc_info=True)
    return None

async def call_llm_api(p_name: str, key: str | None, url: str | None, model: str, msgs: list) -> str | None:
    # Универсальная функция для вызова LLM API, совместимых с OpenAI (Groq, OpenRouter)
    if not key: logger.warning(f"Ключ API для {p_name} недоступен."); return None
    if not url: logger.warning(f"URL для {p_name} недоступен."); return None
    logger.info(f"Вызов {p_name} ({model}) с {len(msgs)} сообщениями контекста.")
    hdrs = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    if p_name == "OpenRouter": hdrs.update({"HTTP-Referer": "YOUR_PROJECT_URL", "X-Title": "EnkiBot"})
    payload = {"model": model, "messages": msgs, "max_tokens": 2000, "temperature": 0.7}
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.post(url, json=payload, headers=hdrs, timeout=30.0); resp.raise_for_status()
            data = resp.json()
            if data.get("choices") and data["choices"][0].get("message"): return data["choices"][0]["message"].get("content","").strip()
    except Exception as e: logger.error(f"Ошибка при работе с API {p_name}: {e}", exc_info=True)
    return None

async def call_google_ai_llm_specific(messages_for_api: list) -> str | None:
    # Отдельная функция для Google AI, так как их SDK имеет другой формат
    if not GOOGLE_AI_API_KEY: logger.warning("Ключ Google AI API недоступен."); return None
    try:
        import google.generativeai as genai; genai.configure(api_key=GOOGLE_AI_API_KEY)
        sys_instr, gem_hist, final_prompt = "You are a helpful assistant.", [], ""
        # ... (логика преобразования истории в формат Gemini) ...
        # Эта часть кода требует тщательной адаптации формата сообщений
        return "Google AI call not fully implemented in this example"
    except ImportError: logger.error("Библиотека google.generativeai не установлена.")
    except Exception as e: logger.error(f"Ошибка Google AI API: {e}", exc_info=True)
    return None

# --- Основные функции бота ---
async def analyze_weather_request_with_llm(text: str) -> dict:
    """
    Анализирует запрос о погоде и определяет, нужна ли текущая погода или прогноз.
    Возвращает словарь, например: {'type': 'forecast', 'days': 7} или {'type': 'current'}.
    """
    logger.info(f"Анализ типа погодного запроса из текста: '{text}'")
    
    system_prompt = (
        "You are an expert in analyzing weather-related requests. Your task is to determine the user's intent. "
        "Does the user want the 'current' weather or a 'forecast' for several days? "
        "If it is a forecast, also determine for how many days. Your answer MUST be a valid JSON object and nothing else. "
    
        "Examples:\n"
        # --- Basic Cases ---
        "- User text: 'погода в Лондоне' -> Your response: {\"type\": \"current\"}\n"
        "- User text: 'what's the weather like?' -> Your response: {\"type\": \"current\"}\n"
    
        # --- Multi-Day Forecast Cases ---
        "- User text: 'погода в Тампе на неделю' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n"
        "- User text: 'прогноз на 5 дней в Берлине' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n"
    
        # --- NEW: Cases for specific or relative days ---
        "- User text: 'какая погода будет завтра?' -> Your response: {\"type\": \"forecast\", \"days\": 2}\n"
        "- User text: 'дай прогноз на субботу' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n"
    
        # --- NEW: Cases for generic forecast requests ---
        "- User text: 'просто дай прогноз погоды' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n"
        "- User text: 'прогноз на выходные' -> Your response: {\"type\": \"forecast\", \"days\": 3}\n"

        # --- Fallback Rule ---
        "If you are unsure, always default to 'current'."
    )
    
    messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": text}]
    
    try:
        if openai_async_client:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini', messages=messages_for_api, temperature=0, response_format={"type": "json_object"}
            )
            if completion.choices and completion.choices[0].message:
                response_str = completion.choices[0].message.content.strip()
                logger.info(f"LLM вернула для анализа погоды: {response_str}")
                return json.loads(response_str)
    except Exception as e:
        logger.error(f"Ошибка LLM при анализе запроса погоды: {e}")

    # Возвращаем значение по умолчанию в случае ошибки
    return {"type": "current"}
async def extract_location_with_llm(text: str) -> str | None:
    """
    Использует LLM для извлечения названия города из текста пользователя.
    Возвращает название города на английском языке, готовое для API, или None.
    """
    logger.info(f"Запрос на извлечение локации из текста: '{text}'")
    
    # Промпт специально разработан, чтобы LLM вернула только название или 'None'
    system_prompt = (
        "You are an expert text analysis tool. Your task is to extract a city or location name from the user's text. "
        "Analyze the following text and identify the geographical location (city, region, country) mentioned. "
        "Return ONLY the name of the location in English, suitable for a weather API query. "
        "For example, if the text is 'какая погода в Санкт-Петербурге', you must return 'Saint Petersburg'. "
        "If the text is 'покажи погоду в Астане', you must return 'Astana'. "
        "If no specific location is found, you MUST return the single word: None"
    )
    
    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": text}
    ]
    
    location = None
    
    # Пытаемся использовать OpenAI как приоритетный вариант
    if openai_async_client:
        try:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini', # Используем быструю и умную модель
                messages=messages_for_api,
                temperature=0, # Нам нужна точность, а не креативность
                max_tokens=50
            )
            if completion.choices and completion.choices[0].message:
                location = completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Ошибка OpenAI при извлечении локации: {e}")

    # Если OpenAI не сработал, пробуем Groq как резервный вариант
    if not location and GROQ_API_KEY:
         location = await call_llm_api("Groq (Location)", GROQ_API_KEY, GROQ_ENDPOINT_URL, GROQ_MODEL_ID, messages_for_api)

    # Проверяем ответ от LLM. Если она вернула 'None' или пустую строку, считаем, что город не найден.
    if location and location.lower() != 'none' and location.strip() != "":
        logger.info(f"LLM успешно извлекла локацию: '{location}'")
        return location
    
    logger.warning("LLM не смогла извлечь локацию из текста.")
    return None
async def extract_news_topic_with_llm(text: str) -> str | None:
    """
    Использует LLM для извлечения темы/ключа для поиска новостей.
    """
    logger.info(f"Запрос на извлечение темы новостей из текста: '{text}'")
    
    system_prompt = (
        "You are an expert text analysis tool. Your task is to extract the main topic, keyword, or location from a user's request for news. "
        "Analyze the text. If it contains a specific subject, you MUST return that subject in its base (nominative) case and in the original language. "
        "For example, for a request 'новости в москве', you must return 'Москва'. For 'news about cars', return 'cars'. "
        "If the request is general (e.g., 'what's the news?', 'latest headlines'), you MUST return the single word: None"
    )
    
    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": text}
    ]
    
    topic = None
    # Используем быструю модель для этой задачи
    if openai_async_client:
        try:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=messages_for_api,
                temperature=0,
                max_tokens=50
            )
            if completion.choices and completion.choices[0].message:
                topic = completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Ошибка OpenAI при извлечении темы новостей: {e}")

    if topic and topic.lower() != 'none' and topic.strip() != "":
        logger.info(f"LLM успешно извлекла тему новостей: '{topic}'")
        return topic
    
    logger.info("LLM не нашла конкретной темы, будут запрошены общие новости.")
    return None
from datetime import datetime, timedelta # Убедитесь, что timedelta импортирована

# Переименуем для ясности
async def get_weather_data(location: str, forecast_type: str = 'current', days: int = 7) -> str:
    """
    Получает данные о погоде: текущие или прогноз на несколько дней.
    """
    if not WEATHER_API_KEY:
        return "Функция погоды не настроена: отсутствует ключ API."

    # --- БЛОК ДЛЯ ТЕКУЩЕЙ ПОГОДЫ (остался почти без изменений) ---
    if forecast_type == 'current':
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {"q": location, "appid": WEATHER_API_KEY, "units": "metric", "lang": "ru"}
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
            data = response.json()
            city = data.get("name")
            desc = data["weather"][0].get("description")
            temp = data["main"].get("temp")
            feels = data["main"].get("feels_like")
            wind = data["wind"].get("speed")
            return (
                f"Погода в городе {city}:\n"
                f"  - Сейчас: {desc.capitalize()}\n"
                f"  - Температура: {temp:.1f}°C (ощущается как {feels:.1f}°C)\n"
                f"  - Ветер: {wind:.1f} м/с"
            )
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404: return f"К сожалению, я не смог найти город '{location}'."
            logger.error(f"HTTP ошибка при запросе погоды для {location}: {e}")
            return "Не удалось получить прогноз погоды из-за ошибки сервера."
        except Exception as e:
            logger.error(f"Неожиданная ошибка при запросе погоды: {e}", exc_info=True)
            return "Произошла непредвиденная ошибка при запросе прогноза."

    # --- НОВЫЙ БЛОК ДЛЯ ПРОГНОЗА НА НЕСКОЛЬКО ДНЕЙ ---
    elif forecast_type == 'forecast':
        # Используем другой endpoint для прогноза
        url = "https://api.openweathermap.org/data/2.5/forecast"
        # Запрашиваем на 5 дней, так как это стандарт для бесплатного API
        params = {"q": location, "appid": WEATHER_API_KEY, "units": "metric", "lang": "ru", "cnt": 40} # 40 записей = 5 дней * 8 записей/день
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
            
            data = response.json()
            city = data.get("city", {}).get("name")
            forecast_list = data.get("list", [])
            
            if not forecast_list:
                return f"Не удалось получить прогноз для города '{location}'."

            daily_forecasts = {}
            for forecast in forecast_list:
                # Группируем по дням, чтобы избежать дубликатов
                day_str = datetime.fromtimestamp(forecast["dt"]).strftime('%Y-%m-%d')
                if day_str not in daily_forecasts:
                    daily_forecasts[day_str] = {
                        'day_name': datetime.fromtimestamp(forecast["dt"]).strftime('%A'), # Название дня недели
                        'temp': forecast['main']['temp'],
                        'description': forecast['weather'][0]['description']
                    }

            # Форматируем красивый ответ
            report_lines = [f"Прогноз погоды в городе {city} на ближайшие дни:"]
            for day_data in list(daily_forecasts.values())[:days]: # Ограничиваем кол-вом запрошенных дней
                report_lines.append(
                    f"  - {day_data['day_name'].capitalize()}: {day_data['temp']:.0f}°C, {day_data['description']}"
                )
            return "\n".join(report_lines)

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404: return f"К сожалению, я не смог найти город '{location}' для прогноза."
            logger.error(f"HTTP ошибка при запросе прогноза для {location}: {e}")
            return "Не удалось получить прогноз погоды из-за ошибки сервера."
        except Exception as e:
            logger.error(f"Неожиданная ошибка при запросе прогноза: {e}", exc_info=True)
            return "Произошла непредвиденная ошибка при запросе прогноза."
    
    return "Неизвестный тип запроса погоды."

async def analyze_and_update_user_profile(user_id: int, message_text: str):
    """
    Создает или обновляет структурированный психологический профиль пользователя,
    анализируя его сообщения.
    """
    if not openai_async_client:
        logger.warning(f"Профилирование для user {user_id} пропущено: OpenAI клиент не настроен.")
        return

    MAX_PROFILE_SIZE = 4000  # Увеличим размер, т.к. профиль стал более детальным
    logger.info(f"Запуск создания/обновления психологического профиля для пользователя {user_id}...")

    # Шаг 1: Получаем текущий профиль из БД
    current_profile_notes = ""
    db_conn = get_db_connection()
    if db_conn:
        try:
            with db_conn.cursor() as cursor:
                cursor.execute("SELECT Notes FROM UserProfiles WHERE UserID = ?", user_id)
                row = cursor.fetchone()
                if row and row[0]:
                    current_profile_notes = row[0]
        except pyodbc.Error as ex:
            logger.error(f"Ошибка БД при чтении профиля для {user_id}: {ex}")
        finally:
            db_conn.close()

    # Шаг 2: Выбираем стратегию и промпт (создание или обновление)
    
    system_prompt = ""
    user_prompt = ""

    if not current_profile_notes:
        # СТРАТЕГИЯ 1: СОЗДАНИЕ ПЕРВОНАЧАЛЬНОГО ПРОФИЛЯ
        logger.info(f"Существующий профиль для {user_id} не найден. Создание нового...")
        system_prompt = (
            "Ты — AI-психолог и профайлер. Твоя задача — создать первоначальный психологический портрет пользователя на основе его сообщения. "
            "Проанализируй текст на предмет стиля общения, возможных черт личности (используй модель 'Большая пятерка' как ориентир: Открытость, Добросовестность, Экстраверсия, Доброжелательность), а также ключевых интересов. "
            "Твой ответ ДОЛЖЕН быть структурирован строго по предложенному формату с заголовками Markdown. Будь объективен и основывайся только на предоставленном тексте."
        )
        user_prompt = f"""
            Проанализируй следующее сообщение от нового пользователя и создай его психологический профиль.

            Сообщение пользователя:
            ---
            "{message_text}"
            ---

            Твой результат (строго в формате Markdown):
            """
    else:
        # СТРАТЕГИЯ 2: ОБНОВЛЕНИЕ И СИНТЕЗ СУЩЕСТВУЮЩЕГО ПРОФИЛЯ
        logger.info(f"Обновление существующего профиля для {user_id}...")
        system_prompt = (
            "Ты — AI-психолог, обновляющий досье на пациента. Тебе предоставлены 'Существующий психологический профиль' и 'Новое сообщение' от пользователя. "
            "Твоя задача — не просто добавить новую информацию, а **переосмыслить и синтезировать весь профиль**. "
            "Если новое сообщение подтверждает черту — усиль ее описание. Если противоречит — скорректируй или смягчи. Если открывает что-то новое — интегрируй это в существующую структуру. "
            "Цель — получить эволюционировавший, но все еще лаконичный профиль. Сохраняй исходную структуру Markdown."
        )
        user_prompt = f"""
            Существующий психологический профиль:
            ---
            {current_profile_notes}
            ---

            Новое сообщение от пользователя для анализа:
            ---
            "{message_text}"
            ---

            Твой обновленный и переосмысленный психологический профиль:
            """

    # Шаг 3: Вызов LLM для анализа
    analysis_messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    
    updated_profile_notes = None
    try:
        # Используем более мощную модель для сложных задач анализа
        completion = await openai_async_client.chat.completions.create(
            model='gpt-4o-mini',  # gpt-4o даст еще лучшие результаты
            messages=analysis_messages,
            temperature=0.5,
            max_tokens=1000 
        )
        if completion.choices and completion.choices[0].message:
            updated_profile_notes = completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Ошибка OpenAI при анализе профиля для {user_id}: {e}")

    # Шаг 4: Обновляем профиль в БД
    if updated_profile_notes and updated_profile_notes.strip():
        db_conn_update = get_db_connection()
        if db_conn_update:
            try:
                with db_conn_update.cursor() as cursor:
                    final_notes = updated_profile_notes[:MAX_PROFILE_SIZE]
                    sql = "UPDATE UserProfiles SET Notes = ?, ProfileLastUpdated = GETDATE() WHERE UserID = ?"
                    cursor.execute(sql, final_notes, user_id)
                    db_conn_update.commit()
                    logger.info(f"Успешно обновлен психологический профиль для пользователя {user_id}.")
            except pyodbc.Error as ex:
                logger.error(f"Ошибка БД при сохранении профиля для {user_id}: {ex}")
                if db_conn_update: db_conn_update.rollback()
            finally:
                if db_conn_update: db_conn_update.close()
    else:
        logger.warning(f"Анализ профиля не вернул результат для пользователя {user_id}.")
def find_search_query_in_text(text: str) -> str | None:
    """
    Анализирует текст, приводя слова к начальной форме (лемме),
    и ищет комбинацию триггерных слов и предлогов для извлечения имени.
    Возвращает имя пользователя для поиска или None.
    """
    # Словари триггеров (используем леммы - начальные формы слов)
    # Легко расширять, добавляя новые ключевые слова
    TELL_LEMMAS = {'рассказать', 'поведать', 'сообщить', 'описать'}
    INFO_LEMMAS = {'информация', 'инфо', 'справка', 'досье', 'данные'}
    WHO_LEMMAS = {'кто', 'что'}
    EXPLAIN_LEMMAS = {'пояснить', 'объяснить'}
    REMEMBER_LEMMAS = {'помнить', 'напомнить'}

    # Предлоги, которые обычно следуют за триггерами
    PREPOSITIONS = {'о', 'про', 'за', 'на', 'по'}

    words = re.findall(r"[\w'-]+", text.lower())
    
    for i, word in enumerate(words):
        try:
            # Получаем лемму слова
            lemma = morph.parse(word)[0].normal_form
            
            # Проверяем, является ли лемма одним из наших триггеров
            is_trigger = (lemma in TELL_LEMMAS or 
                          lemma in INFO_LEMMAS or 
                          lemma in WHO_LEMMAS or 
                          lemma in EXPLAIN_LEMMAS or 
                          lemma in REMEMBER_LEMMAS)

            if is_trigger:
                # Мы нашли триггерное слово. Теперь нужно найти имя, которое идет после него.
                # Индекс, с которого начинается имя
                start_index = i + 1
                
                # Если следующее слово - предлог, пропускаем его
                if start_index < len(words) and words[start_index] in PREPOSITIONS:
                    start_index += 1
                
                # Все, что идет дальше (до 3 слов), считаем именем
                if start_index < len(words):
                    # Захватываем от 1 до 3 слов после триггера/предлога
                    name_parts = words[start_index : start_index + 3]
                    return " ".join(name_parts)

        except Exception as e:
            logger.error(f"Ошибка при лемматизации слова '{word}': {e}")
            continue
            
    return None
async def get_orchestrated_llm_response(prompt_text: str, chat_id: int, user_id: int, message_id: int, context: ContextTypes.DEFAULT_TYPE) -> str:
    """
    Это "мозг" бота. Функция определяет, нужно ли искать информацию в памяти,
    собирает весь необходимый контекст и управляет вызовами к LLM.
    """
    history_from_db, keyword_context_messages, profile_context_messages = [], [], []
    conn = get_db_connection()
    
    # Сначала получаем общую историю чата
    if conn:
        try:
            with conn.cursor() as c:
                MAX_RECENT_HISTORY = 100
                sql_hist = "SELECT TOP (?) Role, Content FROM ConversationHistory WHERE ChatID = ? ORDER BY Timestamp DESC"
                c.execute(sql_hist, MAX_RECENT_HISTORY, chat_id)
                history_from_db = [{"role": row.Role.lower(), "content": row.Content} for row in reversed(c.fetchall())]
        except pyodbc.Error as ex: 
            logger.error(f"DB error during history fetch: {ex}")
    

    # --- Шаг 1: Лингвистический поиск ключевой фразы ---
    search_term_original = find_search_query_in_text(prompt_text)

    if search_term_original:
        logger.info(f"Лингвистический анализ нашел запрос о пользователе: '{search_term_original}'")
    
    # --- Шаг 2: Поиск пользователя, его профиля и последних сообщений ---
    if conn and search_term_original:
        try:
            with conn.cursor() as c:
                # --- НАЧАЛО НОВОЙ ЛОГИКИ ПОИСКА ---
                # Теперь мы ищем точное совпадение в новой таблице вариантов имен.
                # Это быстрее и точнее, чем поиск с LIKE по нескольким полям.
                logger.info(f"Ищу UserID в таблице UserNameVariations по запросу: '{search_term_original.lower()}'")
                
                # Сначала находим ID всех пользователей, у которых есть такой вариант имени.
                sql_find_user_ids = "SELECT DISTINCT UserID FROM UserNameVariations WHERE NameVariation = ?"
                c.execute(sql_find_user_ids, search_term_original.lower())
                user_ids = [row.UserID for row in c.fetchall()]

                matched_profiles = []
                if user_ids:
                    # Если ID найдены, одним запросом получаем полные профили этих пользователей.
                    id_placeholders = ','.join('?' for _ in user_ids)
                    sql_find_user = f"""
                        SELECT UserID, FirstName, LastName, Username, Notes 
                        FROM UserProfiles 
                        WHERE UserID IN ({id_placeholders})
                    """
                    c.execute(sql_find_user, *user_ids)
                    matched_profiles = c.fetchall()
                
                logger.info(f"Найдено {len(matched_profiles)} профилей по запросу '{search_term_original}'.")
                # --- КОНЕЦ НОВОЙ ЛОГИКИ ПОИСКА ---

                # <<< НАЧАЛО БЛОКА СИНТЕЗА ДАННЫХ (остался без изменений) >>>
                if len(matched_profiles) == 1:
                    profile = matched_profiles[0]
                    target_user_id = profile.UserID
                    user_identifier = profile.FirstName or profile.Username or f"User ID {target_user_id}"

                    # 1. Получаем готовое досье из профиля
                    if profile.Notes and profile.Notes.strip():
                        profile_context_messages.append({
                            "role": "system",
                            "content": f"Важнейший контекст (готовое досье) по пользователю '{user_identifier}':\n---\n{profile.Notes}\n---"
                        })
                        logger.info(f"Загружен профиль (досье) для '{user_identifier}'.")

                    # 2. Получаем последние 50 сообщений из лога
                    logger.info(f"Запрашиваю до 50 последних сообщений для пользователя {user_identifier} (ID: {target_user_id})...")
                    sql_get_messages = """
                        SELECT TOP 50 MessageText 
                        FROM ChatLog
                        WHERE UserID = ? AND ChatID = ?
                        ORDER BY Timestamp DESC
                    """
                    c.execute(sql_get_messages, target_user_id, chat_id)
                    recent_messages_rows = c.fetchall()

                    if recent_messages_rows:
                        formatted_messages = "\n".join([f'- "{row.MessageText}"' for row in recent_messages_rows if row.MessageText and row.MessageText.strip()])
                        keyword_context_messages.append({
                            "role": "system",
                            "content": f"Дополнительный контекст для анализа (сырые данные): Вот до 50 последних сообщений от '{user_identifier}'. Используй их вместе с досье для составления самого актуального ответа.\n---\n{formatted_messages}\n---"
                        })
                        logger.info(f"Загружено {len(recent_messages_rows)} последних сообщений для анализа.")
                    else:
                        logger.info(f"Последние сообщения для {user_identifier} не найдены в логах этого чата.")
                # <<< КОНЕЦ БЛОКА СИНТЕЗА ДАННЫХ >>>

                elif len(matched_profiles) > 1:
                    user_options = [f"@{p.Username}" if p.Username else f"{p.FirstName or ''} {p.LastName or ''}".strip() for p in matched_profiles]
                    user_options = [opt for opt in user_options if opt]
                    logger.info(f"Найдено несколько пользователей: {user_options}. Запрашиваю уточнение.")
                    return f"Я нашел несколько пользователей с таким именем: {', '.join(user_options)}. О ком именно вы спрашиваете? Пожалуйста, уточните (можно по @username)."
                
                else:
                    logger.info(f"Профили не найдены для '{search_term_original}'.") # Сообщение остается, но теперь оно означает, что в таблице UserNameVariations нет такого имени.

        except pyodbc.Error as ex:
            logger.error(f"Ошибка БД при поиске в памяти: {ex}")
        finally:
            if conn:
                conn.close() # Закрываем соединение здесь, так как все операции с БД в этой функции завершены
                conn = None # Устанавливаем в None, чтобы избежать двойного закрытия

    # --- Шаг 3: Оркестрация LLM (остался без изменений) ---
    sys_prompt_content = (
        "Ты EnkiBot, умный и дружелюбный ассистент-аналитик в Telegram-чате, созданный Yael Demedetskaya. "
        "Твоя задача — помогать пользователям, отвечая на их вопросы. Ты обладаешь долгосрочной памятью о разговорах и профилях участников. "
        "Когда тебя просят рассказать о ком-то, твоя главная задача — СИНТЕЗИРОВАТЬ ИНФОРМАЦИЮ. "
        "Тебе будут предоставлены два типа данных: готовое досье из профиля и набор последних 'сырых' сообщений от этого человека. "
        "Проанализируй ОБА источника и составь на их основе новый, краткий, но содержательный и актуальный ответ. Не просто пересказывай досье, а обогащай его свежей информацией из сообщений. "
        "Отвечай развернуто, естественно и по-русски. Будь вежлив, но не слишком формален."
    )
    
    messages_for_api = [{"role": "system", "content": sys_prompt_content}] + profile_context_messages + keyword_context_messages + history_from_db + [{"role": "user", "content": prompt_text}]

    MAX_MSG_CTX = 40
    if len(messages_for_api) > MAX_MSG_CTX:
        sys_p = [m for m in messages_for_api if m["role"] == "system"]
        usr_hist = [m for m in messages_for_api if m["role"] != "system"]
        messages_for_api = sys_p + usr_hist[-(MAX_MSG_CTX - len(sys_p)):]

    tasks, task_names = [], []
    if openai_async_client: 
        tasks.append(call_openai_llm(messages_for_api))
        task_names.append("OpenAI")
    if GROQ_API_KEY: 
        tasks.append(call_llm_api("Groq", GROQ_API_KEY, GROQ_ENDPOINT_URL, GROQ_MODEL_ID, messages_for_api))
        task_names.append("Groq")

    if not tasks:
        return "Извините, ни один из моих AI-ассистентов сейчас не доступен."

    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    final_reply = None
    for i, res in enumerate(results):
        if isinstance(res, str) and res.strip():
            logger.info(f"Используем успешный ответ от {task_names[i]}.")
            final_reply = res.replace('**', '')
            break
        elif isinstance(res, Exception):
            logger.error(f"Провайдер {task_names[i]} вернул ошибку: {res}")

    if not final_reply:
        final_reply = "К сожалению, я не смог получить четкий ответ. Пожалуйста, попробуйте еще раз."

    # Сохранение в историю вынесено из блока `if conn...`, так как соединение уже может быть закрыто
    db_conn_for_saving = get_db_connection()
    if db_conn_for_saving:
        try:
            with db_conn_for_saving.cursor() as c:
                sql_save = "INSERT INTO ConversationHistory (ChatID, UserID, MessageID, Role, Content) VALUES (?, ?, ?, ?, ?)"
                c.execute(sql_save, chat_id, user_id, message_id, 'user', prompt_text)
                c.execute(sql_save, chat_id, context.bot.id, None, 'assistant', final_reply)
                db_conn_for_saving.commit()
        except pyodbc.Error as ex:
            logger.error(f"Ошибка БД при сохранении истории: {ex}")
            if db_conn_for_saving: db_conn_for_saving.rollback()
        finally:
            if db_conn_for_saving: db_conn_for_saving.close()
            
    return final_reply
# --- Обработчики команд и сообщений ---

async def log_message_to_db(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # Логирует каждое сообщение в ChatLog и обновляет профиль пользователя
    if not update.message or not update.message.text: return
    chat_id, user, message = update.effective_chat.id, update.effective_user, update.message
    if ALLOWED_GROUP_IDS and chat_id not in ALLOWED_GROUP_IDS: return

    conn = get_db_connection()
    if conn:
        try:
            with conn.cursor() as c: 
                # SQL MERGE - это мощная команда "UPSERT" (UPDATE or INSERT).
                # Она проверяет, есть ли юзер с таким UserID. Если есть - обновляет его данные.
                # Если нет - вставляет новую запись. Это избавляет от лишних проверок в коде.
                upsert_user_sql = """
                    MERGE UserProfiles AS t
                    USING (VALUES(?,?,?,?,GETDATE())) AS s(UserID,Username,FirstName,LastName,LastSeen)
                    ON t.UserID = s.UserID
                    WHEN MATCHED THEN
                        UPDATE SET Username=s.Username, FirstName=s.FirstName, LastName=s.LastName, LastSeen=s.LastSeen, MessageCount=ISNULL(t.MessageCount,0)+1
                    WHEN NOT MATCHED THEN
                        INSERT(UserID,Username,FirstName,LastName,LastSeen,MessageCount,ProfileLastUpdated)
                        VALUES(s.UserID,s.Username,s.FirstName,s.LastName,s.LastSeen,1,GETDATE())
                    OUTPUT $action AS Action;
                """
                c.execute(upsert_user_sql, user.id, user.username, user.first_name, user.last_name)
                
                sql_chatlog = "INSERT INTO ChatLog (ChatID, UserID, Username, FirstName, MessageID, MessageText) VALUES (?, ?, ?, ?, ?, ?)"
                c.execute(sql_chatlog, chat_id, user.id, user.username, user.first_name, message.message_id, message.text)
                conn.commit()
            
            # Запускаем анализ профиля в фоновом режиме, чтобы не задерживать основной поток
            if message.text and len(message.text.strip()) > 10 :
                asyncio.create_task(analyze_and_update_user_profile(user_id=user.id, message_text=message.text))
        except pyodbc.Error as ex: 
            logger.error(f"Ошибка БД при логировании сообщения: {ex}")
            if conn: conn.rollback() 
        finally: 
            if conn: conn.close()

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    # Главный обработчик всех текстовых сообщений.
    if not update.message or not update.message.text:
        return

    # Шаг 1: Логируем все сообщения в БД.
    # Эта функция также запускает фоновое обновление профиля пользователя.
    await log_message_to_db(update, context)

    chat_id = update.effective_chat.id
    user_msg_txt = update.message.text

    # Проверяем, разрешена ли работа в этой группе
    if ALLOWED_GROUP_IDS and chat_id not in ALLOWED_GROUP_IDS:
        return

    # <<< НАЧАЛО БЛОКА: Анализ сообщения, на которое ответили >>>
    # Проверяем, является ли это сообщение ответом на другое сообщение с текстом
    if update.message.reply_to_message and update.message.reply_to_message.text:
        msg_lower = user_msg_txt.lower()
        bot_user_lower = context.bot.username.lower()
        
        # Проверяем, упомянули ли бота в тексте ответа, используя тот же список никнеймов
        is_bot_mentioned = (f"@{bot_user_lower}" in msg_lower or
                            any(re.search(r'\b' + re.escape(n) + r'\b', msg_lower, re.I) for n in BOT_NICKNAMES_TO_CHECK))

        # Условие срабатывает, если упомянули бота И отвечают НЕ на его собственное сообщение
        if is_bot_mentioned and update.message.reply_to_message.from_user.id != context.bot.id:
            logger.info("Сработал триггер анализа сообщения по ответу.")
            await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

            original_text = update.message.reply_to_message.text
            user_question = user_msg_txt  # Вопрос — это сам текст ответа

            # <<< НАЧАЛО ИЗМЕНЕНИЙ: ОБРАБОТКА "ПУСТОГО" ВОПРОСА >>>
            # Проверяем, содержит ли вопрос что-то кроме имени бота
            question_check = user_msg_txt.lower()
            # Удаляем все известные никнеймы бота из текста вопроса
            for nickname in BOT_NICKNAMES_TO_CHECK:
                question_check = question_check.replace(nickname, '').strip()
            # Дополнительно удаляем прямое упоминание @
            question_check = question_check.replace(f"@{bot_user_lower}", '').strip()
            
            # Если после очистки почти ничего не осталось, задаем вопрос по умолчанию
            if len(question_check) < 5: # Используем небольшое число, чтобы отсечь знаки препинания и короткий мусор
                logger.info(f"Вопрос в ответе почти пуст ('{user_msg_txt}'). Используется вопрос по умолчанию.")
                user_question = "Проанализируй этот текст, выдели главную мысль и выскажи свое мнение."
            # <<< КОНЕЦ ИЗМЕНЕНИЙ >>>

            # Вызываем нашу новую функцию-анализатор с (возможно) новым вопросом
            analysis_result = await analyze_replied_message(original_text, user_question)
            
            # Отвечаем на сообщение пользователя (которое само является ответом)
            await update.message.reply_text(analysis_result)
            return  # ВАЖНО: Завершаем дальнейшую обработку
    # <<< КОНЕЦ БЛОКА >>>

    # --- Стандартная логика определения триггера для обычных сообщений ---
    is_group = update.message.chat.type in ['group', 'supergroup']
    msg_lower = user_msg_txt.lower()
    bot_user_lower = context.bot.username.lower()
    triggered = (f"@{bot_user_lower}" in msg_lower or
                 any(re.search(r'\b' + re.escape(n) + r'\b', msg_lower, re.I) for n in BOT_NICKNAMES_TO_CHECK) or
                 (update.message.reply_to_message and update.message.reply_to_message.from_user.id == context.bot.id))

    # В группе реагируем только на прямое обращение
    if is_group and not triggered:
        return

    # --- Маршрутизация по специальным функциям (погода, новости) ---
    if re.search(r'\b(погод|прогноз|weather|forecast)\b', msg_lower, re.I):
        logger.info(f"Сработал триггер погоды. Запускаю анализ запроса: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

        intent_data = await analyze_weather_request_with_llm(user_msg_txt)
        forecast_type = intent_data.get("type", "current")
        days_to_forecast = intent_data.get("days", 7)
        city = await extract_location_with_llm(user_msg_txt)

        if city:
            weather_report = await get_weather_data(
                location=city,
                forecast_type=forecast_type,
                days=days_to_forecast
            )
            await update.message.reply_text(weather_report)
        else:
            await update.message.reply_text("Я готов показать погоду, но не смог понять, для какого города. Пожалуйста, уточните.")
        return

    if re.search(r'\b(новост|news|события|заголовки|headlines|что нового)\b', msg_lower, re.I):
        logger.info(f"Сработал триггер новостей. Запускаю извлечение темы из текста: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

        topic = await extract_news_topic_with_llm(user_msg_txt)
        news_report = await get_latest_news(query=topic)
        await update.message.reply_text(news_report, disable_web_page_preview=True)
        return

    # --- Вызов основного "мозга" для всех остальных случаев ---
    logger.info(f"Обработка сообщения через LLM в чате {chat_id}")
    await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
    reply = await get_orchestrated_llm_response(
        prompt_text=user_msg_txt,
        chat_id=chat_id,
        user_id=update.effective_user.id,
        message_id=update.message.message_id,
        context=context
    )
    if reply:
        await update.message.reply_text(reply)

# --- Остальные обработчики и главная функция ---
async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None: 
    logger.error(f'Update "{update}" caused error "{context.error}"', exc_info=context.error)

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user
    await update.message.reply_html(rf"Привет, {user.mention_html()}! Я EnkiBot, создан Yael Demedetskaya. Чем могу помочь?")

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = ("Я EnkiBot, AI-ассистент от Yael Demedetskaya.\n"
           "В группах я отвечаю, когда вы упоминаете меня по имени (@EnkiBot, Энки) или отвечаете на мои сообщения.\n" 
           "Вы можете спросить меня 'расскажи о [имя/тема]', чтобы я поискал информацию в истории чата.\n"
           "Чтобы узнать погоду, спросите 'какая погода в [город]?'\n\n"
           "**Команды:**\n/start - Начало работы\n/help - Эта справка\n/news - Последние новости")
    await update.message.reply_text(msg)

async def news_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
    report = await get_latest_news() # <<< ИЗМЕНЕНИЕ ЗДЕСЬ
    await update.message.reply_text(report, disable_web_page_preview=True)

# <<< ОБНОВЛЕННАЯ ВЕРСИЯ >>>
# <<< ОБНОВЛЕННАЯ ВЕРСИЯ, СПОСОБНАЯ ИСКАТЬ ПО ТЕМЕ >>>
async def get_latest_news(query: str | None = None, country="us", category="general", num=5) -> str:
    """
    Асинхронно получает новости.
    - Если 'query' указан, ищет по всему миру по этому ключевому слову (на всех языках).
    - Если 'query' не указан, возвращает главные новости для указанной страны.
    """
    if not NEWS_API_KEY:
        return "Ключ News API отсутствует."

    params = {"apiKey": NEWS_API_KEY, "pageSize": num}
    base_url = "https://newsapi.org/v2/"

    if query:
        logger.info(f"Выполняется поиск новостей по запросу: '{query}'")
        # Используем endpoint для поиска по ключевому слову
        endpoint = "everything"
        # <<< ИЗМЕНЕНИЕ ЗДЕСЬ: параметр 'language' полностью убран >>>
        params.update({"q": query, "sortBy": "publishedAt"})
    else:
        logger.info(f"Запрашиваются главные новости для страны '{country}'")
        # Используем endpoint для главных заголовков
        endpoint = "top-headlines"
        params.update({"country": country, "category": category})

    url = base_url + endpoint
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params)
            response.raise_for_status()

        articles = response.json().get("articles", [])
        if not articles:
            return f"Новости по запросу '{query}' не найдены." if query else "Новости не найдены."

        # Форматирование ответа
        title = f"Новости по запросу '{query}':" if query else "Последние новости:"
        headlines = [title] + [f"- {a.get('title','N/A')} ({a.get('source',{}).get('name','N/A')})\n  Читать: {a.get('url','#')}" for a in articles]
        return "\n\n".join(headlines)
        
    except httpx.HTTPStatusError as e:
        logger.error(f"Ошибка HTTP при запросе новостей: {e}")
        return f"Не удалось получить новости. Сервис вернул ошибку {e.response.status_code}."
    except Exception as e:
        logger.error(f"Неожиданная ошибка NewsAPI: {e}", exc_info=True)
        return "Произошла непредвиденная ошибка при получении новостей."
async def backfill_existing_user_name_variations():
    """
    Разовый скрипт для заполнения вариантов имен для всех пользователей,
    которые уже существуют в базе данных.
    """
    logger.info("Запуск скрипта миграции для заполнения вариантов имен существующих пользователей...")
    conn = get_db_connection()
    if not conn:
        logger.error("Миграция невозможна: нет подключения к БД.")
        return

    users_to_migrate = []
    try:
        with conn.cursor() as cursor:
            # Получаем всех пользователей из профилей
            sql = "SELECT UserID, FirstName, Username FROM UserProfiles"
            cursor.execute(sql)
            users_to_migrate = cursor.fetchall()
    except pyodbc.Error as e:
        logger.error(f"Ошибка при получении списка пользователей для миграции: {e}")
        conn.close()
        return

    logger.info(f"Найдено {len(users_to_migrate)} существующих пользователей для обработки.")

    # Для каждого пользователя запускаем уже существующую функцию генерации имен
    for user in users_to_migrate:
        logger.info(f"Обработка пользователя ID: {user.UserID}, Имя: {user.FirstName}")
        try:
            # Мы можем повторно использовать нашу функцию!
            await populate_name_variations_with_llm(user.UserID, user.FirstName, user.Username)
            # Добавим небольшую задержку, чтобы не перегружать API
            await asyncio.sleep(1) 
        except Exception as e:
            logger.error(f"Ошибка при миграции пользователя {user.UserID}: {e}")

    conn.close()
    logger.info("Миграция имен пользователей завершена.")
def main() -> None:
    """Главная функция, которая запускает бота."""
    initialize_database()
    if not TELEGRAM_BOT_TOKEN:
        logger.critical("Токен бота отсутствует. Запуск невозможен.")
        return

    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("news", news_command)) 
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_error_handler(error_handler) 

    logger.info("Запуск бота...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == '__main__':
    main()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/__init__.py ---
======================================================================

# enkibot/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file intentionally left blank.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/app.py ---
======================================================================

# enkibot/app.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
from telegram.ext import Application

from enkibot import config
from enkibot.utils.database import DatabaseManager
from enkibot.core.llm_services import LLMServices
from enkibot.core.language_service import LanguageService
from enkibot.modules.intent_recognizer import IntentRecognizer
from enkibot.modules.profile_manager import ProfileManager
from enkibot.modules.api_router import ApiRouter
from enkibot.modules.response_generator import ResponseGenerator
from enkibot.core.telegram_handlers import TelegramHandlerService

logger = logging.getLogger(__name__)

class EnkiBotApplication:
    def __init__(self, ptb_application: Application):
        logger.info("EnkiBotApplication initializing...")
        self.ptb_application = ptb_application

        # Initialize core services
        self.db_manager = DatabaseManager(config.DB_CONNECTION_STRING)
        self.llm_services = LLMServices(
            openai_api_key=config.OPENAI_API_KEY, openai_model_id=config.OPENAI_MODEL_ID,
            groq_api_key=config.GROQ_API_KEY, groq_model_id=config.GROQ_MODEL_ID, groq_endpoint_url=config.GROQ_ENDPOINT_URL,
            openrouter_api_key=config.OPENROUTER_API_KEY, openrouter_model_id=config.OPENROUTER_MODEL_ID, openrouter_endpoint_url=config.OPENROUTER_ENDPOINT_URL,
            google_ai_api_key=config.GOOGLE_AI_API_KEY, google_ai_model_id=config.GOOGLE_AI_MODEL_ID
        )
        self.language_service = LanguageService(
            llm_services=self.llm_services, 
            db_manager=self.db_manager # Pass db_manager for fetching chat history
        )
        
        # Initialize functional modules/services
        self.intent_recognizer = IntentRecognizer(self.llm_services)
        self.profile_manager = ProfileManager(self.llm_services, self.db_manager)
        self.api_router = ApiRouter(
            weather_api_key=config.WEATHER_API_KEY, 
            news_api_key=config.NEWS_API_KEY,
            llm_services=self.llm_services
        )
        self.response_generator = ResponseGenerator(
            self.llm_services, 
            self.db_manager, 
            self.intent_recognizer
        )

        # Initialize Telegram handlers, passing all necessary services
        self.handler_service = TelegramHandlerService(
            application=self.ptb_application,
            db_manager=self.db_manager,
            llm_services=self.llm_services,
            intent_recognizer=self.intent_recognizer,
            profile_manager=self.profile_manager,
            api_router=self.api_router,
            response_generator=self.response_generator,
            language_service=self.language_service,
            allowed_group_ids=config.ALLOWED_GROUP_IDS, # Pass as set
            bot_nicknames=config.BOT_NICKNAMES_TO_CHECK # Pass as list
        )
        
        logger.info("EnkiBotApplication initialized all services.")

    def register_handlers(self):
        """Registers all Telegram handlers."""
        self.handler_service.register_all_handlers()
        logger.info("EnkiBotApplication: All handlers registered with PTB Application.")

    def run(self):
        """Starts the bot polling."""
        logger.info("EnkiBotApplication: Starting PTB Application polling...")
        self.ptb_application.run_polling(allowed_updates=Update.ALL_TYPES)
        logger.info("EnkiBotApplication: Polling stopped.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/combine_files.py ---
======================================================================

# enkibot/combine_files.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
import os
import logging

# --- Configuration ---
# The root directory of your project source code.
# IMPORTANT: Please ensure this path is correct for your system.
PROJECT_ROOT = r'c:\Projects\EnkiBot\EnkiBot\EnkiBot'
# The name of the file that will contain all the combined code.
OUTPUT_FILENAME = 'combined_enkibot_python_source.txt' # Changed name to reflect content
# --- End Configuration ---

# Setup basic logging for the script itself.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_all_python_files(root_dir):
    """
    Walks through the directory structure and collects paths of .py files.
    
    Args:
        root_dir (str): The starting directory to walk.
        
    Returns:
        list: A sorted list of all .py file paths found.
    """
    file_paths = []
    for dirpath, _, filenames in os.walk(root_dir):
        # Exclude __pycache__ directories
        if '__pycache__' in dirpath:
            continue
        for filename in filenames:
            # --- MODIFICATION IS HERE ---
            # Only include files that end with the .py extension.
            if filename.endswith(('.py','.json')):
                file_paths.append(os.path.join(dirpath, filename))
                
    return sorted(file_paths)

def combine_project_files(root_dir, output_file):
    """
    Reads all .py files from a project directory and writes their contents
    into a single output file, with headers for each file.
    
    Args:
        root_dir (str): The root directory of the project to combine.
        output_file (str): The path to the single output file.
    """
    logging.info(f"Starting to combine only .py files from project root: '{root_dir}'")
    
    all_files = get_all_python_files(root_dir)
    
    if not all_files:
        logging.error(f"No .py files found in '{root_dir}'. Please check the PROJECT_ROOT path.")
        return

    try:
        # Open the output file in write mode with UTF-8 encoding
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for file_path in all_files:
                # Normalize path for consistent display
                normalized_path = file_path.replace('\\', '/')
                header = f"======================================================================\n"
                header += f"--- File: {normalized_path} ---\n"
                header += f"======================================================================\n\n"
                
                outfile.write(header)
                logging.info(f"Processing: {normalized_path}")
                
                try:
                    # Open the source file in read mode with UTF-8 encoding
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(infile.read())
                    # Add newlines for separation between files
                    outfile.write("\n\n\n")
                except Exception as e:
                    # Handle potential read errors
                    error_message = f"*** ERROR: Could not read file. Reason: {e} ***\n\n\n"
                    outfile.write(error_message)
                    logging.warning(f"Could not read {file_path}: {e}")

    except IOError as e:
        logging.error(f"Fatal error writing to output file '{output_file}': {e}")
        return

    logging.info(f"Successfully combined {len(all_files)} .py files into '{output_file}'.")

if __name__ == '__main__':
    if not os.path.isdir(PROJECT_ROOT):
        print(f"Error: The project directory '{PROJECT_ROOT}' was not found.")
        print("Please make sure the PROJECT_ROOT path is correct and you are running this script from a location that can access it.")
    else:
        combine_project_files(PROJECT_ROOT, OUTPUT_FILENAME)


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/config.py ---
======================================================================

﻿# enkibot/config.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Configuration ===
# ==================================================================================================
# Central configuration file for API keys, model IDs, database settings, and other constants.
# It is best practice to load sensitive information from environment variables.
# ==================================================================================================

import os
import logging

# --- Core Bot Settings ---
TELEGRAM_BOT_TOKEN = os.getenv('ENKI_BOT_TOKEN')
# A list of bot nicknames that trigger a response in group chats.
BOT_NICKNAMES_TO_CHECK = ["enki", "enkibot", "энки", "энкибот", "бот", "bot"]

# --- Database Configuration (MS SQL Server) ---
SQL_SERVER_NAME = os.getenv('ENKI_BOT_SQL_SERVER_NAME')
SQL_DATABASE_NAME = os.getenv('ENKI_BOT_SQL_DATABASE_NAME')
DB_CONNECTION_STRING = (
    f"DRIVER={{ODBC Driver 17 for SQL Server}};"
    f"SERVER={SQL_SERVER_NAME};"
    f"DATABASE={SQL_DATABASE_NAME};"
    f"Trusted_Connection=yes;"
) if SQL_SERVER_NAME and SQL_DATABASE_NAME else None

# --- LLM Provider API Keys & Models ---
# OpenAI
OPENAI_API_KEY = os.getenv('ENKI_BOT_OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MODEL_ID', 'gpt-4o-mini')                 # General purpose (if not overridden by task-specific models)
OPENAI_CLASSIFICATION_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_CLASSIFICATION_MODEL_ID', 'gpt-3.5-turbo') # For faster tasks like intent classification
OPENAI_TRANSLATION_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_TRANSLATION_MODEL_ID', 'gpt-4o-mini')      # For language pack creation

# Groq
GROQ_API_KEY = os.getenv('ENKI_BOT_GROQ_API_KEY')
GROQ_MODEL_ID = os.getenv('ENKI_BOT_GROQ_MODEL_ID', 'llama-3.3-70b-versatile')
GROQ_ENDPOINT_URL = "https://api.groq.com/openai/v1/chat/completions"

# OpenRouter
OPENROUTER_API_KEY = os.getenv('ENKI_BOT_OPENROUTER_API_KEY')
OPENROUTER_MODEL_ID = os.getenv('ENKI_BOT_OPENROUTER_MODEL_ID', 'mistralai/mistral-7b-instruct:free')
OPENROUTER_ENDPOINT_URL = "https://openrouter.ai/api/v1/chat/completions"

# Google AI
GOOGLE_AI_API_KEY = os.getenv('ENKI_BOT_GOOGLE_AI_API_KEY')
GOOGLE_AI_MODEL_ID = os.getenv('ENKI_BOT_GOOGLE_AI_MODEL_ID', 'gemini-1.5-flash-latest')

# --- External Service API Keys ---
NEWS_API_KEY = os.getenv('ENKI_BOT_NEWS_API_KEY')
WEATHER_API_KEY = os.getenv('ENKI_BOT_WEATHER_API_KEY')

# --- Group Chat Access Control ---
ALLOWED_GROUP_IDS_STR = os.getenv('ENKI_BOT_ALLOWED_GROUP_IDS')
ALLOWED_GROUP_IDS = set()
if ALLOWED_GROUP_IDS_STR:
    try:
        ALLOWED_GROUP_IDS = set(int(id_str.strip()) for id_str in ALLOWED_GROUP_IDS_STR.split(','))
        if ALLOWED_GROUP_IDS:
            logging.info(f"Bot access is restricted to group IDs: {ALLOWED_GROUP_IDS}")
    except ValueError:
        logging.error(f"Invalid format for ENKI_BOT_ALLOWED_GROUP_IDS: '{ALLOWED_GROUP_IDS_STR}'. No group restrictions applied.")
else:
    logging.info("ENKI_BOT_ALLOWED_GROUP_IDS is not set. The bot will operate in all groups.")

# --- Language and Prompts Configuration ---
# The default language to use if detection fails.
DEFAULT_LANGUAGE = "en"
# The directory where language-specific prompt files (e.g., en.json, ru.json) are stored.
LANGUAGE_PACKS_DIR = os.path.join(os.path.dirname(__file__), 'lang')


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/__init__.py ---
======================================================================

# enkibot/core/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# This file makes the 'core' directory a Python package.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/language_service.py ---
======================================================================

# enkibot/core/language_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# enkibot/core/language_service.py
# enkibot/core/language_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Ensure your GPLv3 header is here)

import logging
import json
import os
import re
from typing import Dict, Any, Optional, List

from telegram import Update 

from enkibot import config 
from enkibot.core.llm_services import LLMServices 
from enkibot.utils.database import DatabaseManager 

logger = logging.getLogger(__name__)

class LanguageService:
    def __init__(self, 
                 llm_services: LLMServices, 
                 db_manager: DatabaseManager,
                 lang_packs_dir: str = config.LANGUAGE_PACKS_DIR, 
                 default_lang: str = config.DEFAULT_LANGUAGE):
        
        logger.info("LanguageService __init__ STARTING")
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.lang_packs_dir = lang_packs_dir
        self.default_language = default_lang
        self.primary_fallback_lang = default_lang 
        self.secondary_fallback_lang = "ru"

        self.language_packs: Dict[str, Dict[str, Any]] = {}
        self.llm_prompt_sets: Dict[str, Dict[str, Dict[str, str]]] = {}
        self.response_strings: Dict[str, Dict[str, str]] = {}
        
        self.current_lang: str = self.default_language
        self.current_llm_prompt_sets: Dict[str, Dict[str, str]] = {}
        self.current_response_strings: Dict[str, str] = {}
        self.current_lang_pack_full: Dict[str, Any] = {}

        self._load_language_packs() 
        logger.info("LanguageService __init__ COMPLETED")

    def _load_language_packs(self):
        self.language_packs = {}
        self.llm_prompt_sets = {}
        self.response_strings = {}
        if not os.path.exists(self.lang_packs_dir):
            logger.error(f"Language packs directory not found: {self.lang_packs_dir}")
            try:
                os.makedirs(self.lang_packs_dir, exist_ok=True)
                logger.info(f"Created language packs directory: {self.lang_packs_dir}")
            except OSError as e:
                logger.error(f"Could not create language packs directory {self.lang_packs_dir}: {e}")
        
        for lang_file in os.listdir(self.lang_packs_dir):
            if lang_file.endswith(".json"):
                lang_code = lang_file[:-5]
                file_path = os.path.join(self.lang_packs_dir, lang_file)
                try:
                    with open(file_path, 'r', encoding='utf-8-sig') as f: 
                        pack_content = json.load(f)
                        self.language_packs[lang_code] = pack_content
                        self.llm_prompt_sets[lang_code] = pack_content.get("prompts", {})
                        self.response_strings[lang_code] = pack_content.get("responses", {})
                        logger.info(f"Successfully loaded language pack: {lang_code} from {file_path}")
                except json.JSONDecodeError as jde:
                    logger.error(f"Error decoding JSON from language file: {lang_file}. Error: {jde.msg} at L{jde.lineno} C{jde.colno} (char {jde.pos})", exc_info=False)
                except Exception as e:
                    logger.error(f"Error loading language file {lang_file}: {e}", exc_info=True)
        
        self._set_current_language_internals(self.default_language)

    def _set_current_language_internals(self, lang_code_to_set: str):
        chosen_lang_code = lang_code_to_set
        if chosen_lang_code not in self.language_packs:
            logger.warning(f"Language pack for initially requested '{chosen_lang_code}' not found.")
            if self.primary_fallback_lang in self.language_packs:
                logger.info(f"Falling back to primary fallback: '{self.primary_fallback_lang}'.")
                chosen_lang_code = self.primary_fallback_lang
            elif self.secondary_fallback_lang in self.language_packs:
                logger.info(f"Primary fallback '{self.primary_fallback_lang}' not found. Falling back to secondary: '{self.secondary_fallback_lang}'.")
                chosen_lang_code = self.secondary_fallback_lang
            elif self.language_packs: 
                first_available = next(iter(self.language_packs))
                logger.error(f"Fallbacks ('{self.primary_fallback_lang}', '{self.secondary_fallback_lang}') not found. Using first available: '{first_available}'.")
                chosen_lang_code = first_available
            else: 
                logger.critical("CRITICAL: No language packs loaded at all. Service may be impaired.")
                self.current_lang = "none" 
                self.current_lang_pack_full = {}
                self.current_llm_prompt_sets = {}
                self.current_response_strings = {}
                return

        self.current_lang = chosen_lang_code
        self.current_lang_pack_full = self.language_packs.get(chosen_lang_code, {})
        self.current_llm_prompt_sets = self.llm_prompt_sets.get(chosen_lang_code, {})
        self.current_response_strings = self.response_strings.get(chosen_lang_code, {})
        
        if not self.current_llm_prompt_sets and not self.current_response_strings:
             logger.error(f"Language '{self.current_lang}' pack loaded, but it seems empty (no 'prompts' or 'responses').")
        else:
            logger.info(f"LanguageService: Successfully set current language context to: '{self.current_lang}'")

    async def _create_and_load_language_pack(self, new_lang_code: str, update_context: Optional[Update] = None) -> bool:
        logger.info(f"LanguageService: Attempting to create language pack for new language: {new_lang_code}")
        english_pack_key = "en"
        if english_pack_key not in self.language_packs:
            logger.error(f"Cannot create new language pack: Source English ('{english_pack_key}') pack not found.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback", 
                                                                                           "My apologies, I'm having trouble setting up support for this language right now (missing base files)."))
            return False

        english_pack_content_str = json.dumps(self.language_packs[english_pack_key], ensure_ascii=False, indent=2)
        target_language_name = new_lang_code 

        translation_system_prompt = (
             f"You are an expert translation AI. Your task is to translate a complete JSON language pack from English to {target_language_name} (language code: {new_lang_code}).\n"
            "You MUST maintain the original JSON structure and all original keys (e.g., \"prompts\", \"responses\", \"weather_conditions_map\", \"days_of_week\", and all keys within them). Only translate the string values associated with the keys.\n"
            "The output MUST be a single, valid JSON object and nothing else. Do not add any explanatory text, comments, or markdown before or after the JSON.\n"
            "Ensure all translated strings are appropriate for a friendly AI assistant and are natural-sounding in the target language. Pay special attention to escaping characters within JSON strings if necessary (e.g. double quotes inside a string should be \\\", newlines as \\n)."
        )
        translation_user_prompt = f"Translate the following English JSON language pack to {target_language_name} ({new_lang_code}):\n\n{english_pack_content_str}"
        
        messages_for_api = [{"role": "system", "content": translation_system_prompt}, {"role": "user", "content": translation_user_prompt}]
        response_format_arg = {"response_format": {"type": "json_object"}}
        
        translated_content_str: Optional[str] = None
        try:
            translator_model_id = config.OPENAI_TRANSLATION_MODEL_ID 
            logger.info(f"Using model {translator_model_id} for language pack translation to {new_lang_code}")
            translated_content_str = await self.llm_services.call_openai_llm(
                messages_for_api, model_id=translator_model_id, 
                temperature=0.1, max_tokens=4000, **response_format_arg
            )
        except Exception as e:
            logger.error(f"LLM call itself failed during translation for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False

        if not translated_content_str:
            logger.error(f"LLM failed to provide a translation string for {new_lang_code}.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False
        
        clean_response = translated_content_str.strip() 
        try:
            match = re.search(r"```json\s*(.*?)\s*```", clean_response, re.DOTALL | re.IGNORECASE)
            if match: clean_response = match.group(1).strip()
            else:
                if clean_response.startswith("```json"): clean_response = clean_response[7:]
                if clean_response.endswith("```"): clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            logger.debug(f"Attempting to parse cleaned LLM translation for {new_lang_code}: '{clean_response[:300]}...'")
            translated_pack_content = json.loads(clean_response) 
            
            if not all(k in translated_pack_content for k in ["prompts", "responses", "weather_conditions_map", "days_of_week"]):
                logger.error(f"Translated pack for {new_lang_code} is missing core top-level keys. Aborting save.")
                raise ValueError("Translated JSON missing core keys.")

            new_pack_path = os.path.join(self.lang_packs_dir, f"{new_lang_code}.json")
            with open(new_pack_path, 'w', encoding='utf-8') as f: 
                json.dump(translated_pack_content, f, ensure_ascii=False, indent=2)
            logger.info(f"Successfully created and saved new language pack: {new_lang_code}.json")
            
            self.language_packs[new_lang_code] = translated_pack_content
            self.llm_prompt_sets[new_lang_code] = translated_pack_content.get("prompts", {})
            self.response_strings[new_lang_code] = translated_pack_content.get("responses", {})
            logger.info(f"New language pack for {new_lang_code} is now available at runtime.")
            return True
        except json.JSONDecodeError as jde:
            logger.error(
                f"Failed to decode LLM translation JSON for {new_lang_code}. Error: {jde.msg} "
                f"at L{jde.lineno} C{jde.colno} (char {jde.pos}). "
                f"Nearby: '{clean_response[max(0, jde.pos-50):jde.pos+50]}'", 
                exc_info=False 
            )
            log_limit = 3000
            full_resp_to_log = translated_content_str 
            if len(full_resp_to_log) < log_limit: logger.debug(f"Full problematic translated content for {new_lang_code}:\n{full_resp_to_log}")
            else: logger.debug(f"Problematic translated content (first {log_limit} chars) for {new_lang_code}:\n{full_resp_to_log[:log_limit]}")
            if update_context and update_context.effective_message:
                await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False
        except Exception as e:
            logger.error(f"Error processing/saving new lang pack for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message: 
                await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False

    async def determine_language_context(self, 
                                         current_message_text: Optional[str], 
                                         chat_id: Optional[int], 
                                         update_context: Optional[Update] = None) -> str:
        LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD = 0.70  
        NUM_RECENT_MESSAGES_FOR_CONTEXT = 2 
        MIN_MESSAGE_LENGTH_FOR_LLM_INPUT = 5 
        MIN_AGGREGATED_TEXT_LENGTH_FOR_LLM = 15

        final_candidate_lang_code = self.current_lang if self.current_lang != "none" else self.default_language
        
        lang_detector_prompts = self.get_llm_prompt_set("language_detector_llm")

        if not (lang_detector_prompts and "system" in lang_detector_prompts and \
                lang_detector_prompts.get("user_template_full_context") and \
                lang_detector_prompts.get("user_template_latest_only") ):
            logger.error("LLM language detector prompts are incomplete/missing. Using current/default logic for language.")
            # No LLM detection possible, just ensure current lang is set via fallbacks
            self._set_current_language_internals(final_candidate_lang_code)
            return self.current_lang

        history_context_str = ""
        latest_message_payload = current_message_text or "" 

        if chat_id and (not latest_message_payload.strip() or len(latest_message_payload.strip()) < MIN_MESSAGE_LENGTH_FOR_LLM_INPUT):
            logger.debug(f"Current msg short or absent, fetching {NUM_RECENT_MESSAGES_FOR_CONTEXT} recent msgs from chat {chat_id}.")
            try:
                if self.db_manager:
                    recent_messages = await self.db_manager.get_recent_chat_texts(chat_id, limit=NUM_RECENT_MESSAGES_FOR_CONTEXT)
                    if recent_messages:
                        history_context_str = "\n".join(recent_messages)
                        logger.debug(f"Fetched {len(recent_messages)} messages for lang detection context.")
                else: logger.warning("db_manager not available in determine_language_context for history fetch.")
            except Exception as e: logger.error(f"Error fetching recent chat texts for lang detection: {e}", exc_info=True)
        
        user_prompt_template_key = "user_template_full_context" if history_context_str else "user_template_latest_only"
        user_prompt_template = lang_detector_prompts[user_prompt_template_key] # We checked existence above
            
        user_prompt_for_llm_detector = user_prompt_template.format(
            latest_message=latest_message_payload, 
            history_context=history_context_str 
        )
            
        messages_for_llm_detector = [
            {"role": "system", "content": lang_detector_prompts["system"]},
            {"role": "user", "content": user_prompt_for_llm_detector}
        ]

        llm_detected_primary_lang: Optional[str] = None
        llm_detected_confidence: float = 0.0
        
        aggregated_text_for_llm_prompt_check = f"{history_context_str}\n{latest_message_payload}".strip()

        if aggregated_text_for_llm_prompt_check and len(aggregated_text_for_llm_prompt_check) >= MIN_AGGREGATED_TEXT_LENGTH_FOR_LLM:
            try:
                detection_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID 
                logger.info(f"Requesting LLM language detection with model {detection_model_id} for text: '{aggregated_text_for_llm_prompt_check[:70]}...'")
                
                completion_str = await self.llm_services.call_openai_llm(
                    messages_for_llm_detector, model_id=detection_model_id,
                    temperature=0.0, max_tokens=150, response_format={"type": "json_object"}
                )
                if completion_str:
                    try:
                        detection_result = json.loads(completion_str)
                        logger.info(f"LLM language detection response: {detection_result}")
                        llm_detected_primary_lang = str(detection_result.get("primary_lang", "")).lower()
                        confidence_val = detection_result.get("confidence", 0.0)
                        try: llm_detected_confidence = float(confidence_val)
                        except (ValueError, TypeError): llm_detected_confidence = 0.0
                    except json.JSONDecodeError as e:
                         logger.error(f"Failed to decode JSON from LLM lang detector: {e}. Raw: {completion_str}")
                else:
                    logger.warning("LLM language detector returned no content.")
            except Exception as e:
                logger.error(f"Error calling LLM for language detection: {e}", exc_info=True)
        else:
            logger.info(f"Not enough aggregated text ('{aggregated_text_for_llm_prompt_check[:50]}...') for LLM language detection. Using current/default logic.")

        # Logic to use LLM detection result
        if llm_detected_primary_lang and llm_detected_confidence >= LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD:
            logger.info(f"LLM confidently detected primary lang '{llm_detected_primary_lang}' (conf: {llm_detected_confidence:.2f}).")
            final_candidate_lang_code = llm_detected_primary_lang
        else:
            if llm_detected_primary_lang: 
                 logger.warning(f"LLM detected lang '{llm_detected_primary_lang}' but confidence ({llm_detected_confidence:.2f}) "
                               f"< threshold ({LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD}). Using current/default: {final_candidate_lang_code}")
            # If no LLM detection or low confidence, final_candidate_lang_code remains as initialized (current or default)
        
        # Set language context based on final_candidate_lang_code
        if final_candidate_lang_code not in self.language_packs:
            logger.warning(f"Language pack for candidate '{final_candidate_lang_code}' not found. Attempting creation.")
            if await self._create_and_load_language_pack(final_candidate_lang_code, update_context=update_context):
                self._set_current_language_internals(final_candidate_lang_code)
            else: 
                logger.warning(f"Failed to create pack for '{final_candidate_lang_code}'. Applying prioritized fallbacks.")
                self._set_current_language_internals(self.default_language) 
        else: 
            self._set_current_language_internals(final_candidate_lang_code)
            
        return self.current_lang
    
    def get_llm_prompt_set(self, key: str) -> Optional[Dict[str, str]]:
        current_prompts_to_check = self.current_llm_prompt_sets
        prompt_set = current_prompts_to_check.get(key)
        primary_fallback_lang = self.default_language
        secondary_fallback_lang = "ru"

        if not prompt_set: 
            logger.debug(f"LLM prompt set key '{key}' not in current lang '{self.current_lang}'. Trying '{primary_fallback_lang}'.")
            current_prompts_to_check = self.llm_prompt_sets.get(primary_fallback_lang, {})
            prompt_set = current_prompts_to_check.get(key)
            if not prompt_set and primary_fallback_lang != secondary_fallback_lang: 
                 logger.debug(f"LLM prompt set key '{key}' not in '{primary_fallback_lang}'. Trying '{secondary_fallback_lang}'.")
                 current_prompts_to_check = self.llm_prompt_sets.get(secondary_fallback_lang, {})
                 prompt_set = current_prompts_to_check.get(key)
        
        if not prompt_set:
            logger.error(f"LLM prompt set for key '{key}' ultimately not found.")
            return None
        if not isinstance(prompt_set, dict) or "system" not in prompt_set: 
            logger.error(f"LLM prompt set for key '{key}' (found in lang or fallback) is malformed: {prompt_set}")
            return None
        return prompt_set

    def get_response_string(self, key: str, default_value: Optional[str] = None, **kwargs) -> str:
        raw_string = self.current_response_strings.get(key)
        lang_tried = self.current_lang
        primary_fallback_lang = self.default_language
        secondary_fallback_lang = "ru"

        if raw_string is None: 
            lang_tried = primary_fallback_lang
            raw_string = self.response_strings.get(primary_fallback_lang, {}).get(key)
            if raw_string is None and primary_fallback_lang != secondary_fallback_lang: 
                lang_tried = secondary_fallback_lang
                raw_string = self.response_strings.get(secondary_fallback_lang, {}).get(key)

        if raw_string is None: 
            if default_value is not None: raw_string = default_value
            else: logger.error(f"Response string for key '{key}' ultimately not found. Using placeholder."); raw_string = f"[[Missing response: {key}]]"
        
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError as e:
            logger.error(f"Missing format key '{e}' in response string for key '{key}' (lang tried: {lang_tried}, raw: '{raw_string}')")
            english_raw = self.response_strings.get("en", {}).get(key, f"[[Format error & missing English for key: {key}]]")
            try: return english_raw.format(**kwargs) if kwargs else english_raw
            except KeyError: return f"[[Formatting error for response key: {key} - check placeholders/English pack]]"


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/llm_services.py ---
======================================================================

# enkibot/core/llm_services.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# ==================================================================================================
# === EnkiBot LLM Services ===
# ==================================================================================================

import logging
import httpx
import openai 
import asyncio
from typing import List, Dict, Optional, Any, Tuple

from enkibot import config # Import config to use its attributes

logger = logging.getLogger(__name__)

class LLMServices:
    def __init__(self, openai_api_key: Optional[str], openai_model_id: str, # This is general default
                 groq_api_key: Optional[str], groq_model_id: str, groq_endpoint_url: str,
                 openrouter_api_key: Optional[str], openrouter_model_id: str, openrouter_endpoint_url: str,
                 google_ai_api_key: Optional[str], google_ai_model_id: str):
        
        print("***** LLMServices __init__ STARTING *****") 
        logger.info("LLMServices __init__ STARTING")
        
        self.openai_api_key = openai_api_key
        # These are defaults if not overridden by task-specific model IDs from config
        self.openai_model_id = openai_model_id 
        self.openai_classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        self.openai_translation_model_id = config.OPENAI_TRANSLATION_MODEL_ID

        self.openai_async_client: Optional[openai.AsyncOpenAI] = None
        if self.openai_api_key:
            try:
                self.openai_async_client = openai.AsyncOpenAI(api_key=self.openai_api_key)
                logger.info("OpenAI AsyncClient initialized successfully.")
                print("INFO: OpenAI AsyncClient initialized successfully.")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI AsyncClient: {e}")
                print(f"ERROR: Failed to initialize OpenAI AsyncClient: {e}")
        else:
            logger.warning("OpenAI API key not provided. OpenAI calls will be disabled.")
            print("WARN: OpenAI API key not provided.")

        self.groq_api_key = groq_api_key
        self.groq_model_id = groq_model_id
        self.groq_endpoint_url = groq_endpoint_url
        if self.groq_api_key: print(f"INFO: Groq configured with key: {self.groq_api_key[:5]}...")
        else: print("WARN: Groq API key not provided.")

        self.openrouter_api_key = openrouter_api_key
        self.openrouter_model_id = openrouter_model_id
        self.openrouter_endpoint_url = openrouter_endpoint_url
        if self.openrouter_api_key: print(f"INFO: OpenRouter configured with key: {self.openrouter_api_key[:5]}...")
        else: print("WARN: OpenRouter API key not provided.")
        
        self.google_ai_api_key = google_ai_api_key
        self.google_ai_model_id = google_ai_model_id
        if self.google_ai_api_key: print(f"INFO: Google AI configured with key: {self.google_ai_api_key[:5]}...")
        else: print("WARN: Google AI API key not provided.")
        
        print("***** LLMServices __init__ COMPLETED *****") 
        logger.info("LLMServices __init__ COMPLETED")

    def is_provider_configured(self, provider_name: str) -> bool:
        provider_name_lower = provider_name.lower()
        if provider_name_lower == "openai":
            return bool(self.openai_async_client and self.openai_api_key)
        elif provider_name_lower == "groq":
            return bool(self.groq_api_key and self.groq_endpoint_url and self.groq_model_id)
        elif provider_name_lower == "openrouter":
            return bool(self.openrouter_api_key and self.openrouter_endpoint_url and self.openrouter_model_id)
        # Add other providers as needed
        return False

    async def call_openai_llm(self, messages: List[Dict[str, str]], 
                              model_id: Optional[str] = None, # Allows overriding model_id for specific tasks
                              temperature: float = 0.7, 
                              max_tokens: int = 2000, 
                              **kwargs) -> Optional[str]:
        print(f"DEBUG: Attempting call_openai_llm. Configured: {self.is_provider_configured('openai')}")
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not initialized or API key missing. Cannot make call.")
            print("WARN: OpenAI client not init or key missing in call_openai_llm.")
            return None
        
        # Use passed model_id if provided, otherwise the instance's default openai_model_id
        actual_model_id = model_id or self.openai_model_id 
        logger.info(f"Calling OpenAI (model: {actual_model_id}) with {len(messages)} messages.")
        print(f"INFO: Calling OpenAI (model: {actual_model_id}) messages_count: {len(messages)}")

        call_params = { "model": actual_model_id, "messages": messages, "temperature": temperature, "max_tokens": max_tokens, **kwargs }
        try:
            print(f"DEBUG: Before OpenAI completions.create with params: model='{call_params.get('model')}', temp: {call_params.get('temperature')}")
            completion = await self.openai_async_client.chat.completions.create(**call_params)
            print(f"DEBUG: OpenAI completion object received: {type(completion)}")
            if completion.choices and completion.choices[0].message and completion.choices[0].message.content:
                response_content = completion.choices[0].message.content.strip()
                print(f"INFO: OpenAI successful response (first 50 chars): {response_content[:50]}")
                return response_content
            logger.warning(f"OpenAI call to {actual_model_id} returned no content or unexpected structure. Choices: {completion.choices}")
            print(f"WARN: OpenAI call to {actual_model_id} no content. Choices: {completion.choices}")
            return None
        except openai.APIStatusError as e: 
            logger.error(f"OpenAI API Status Error (model: {actual_model_id}): HTTP Status {e.status_code} - {e.message}", exc_info=False)
            print(f"ERROR: OpenAI API Status Error (model: {actual_model_id}): HTTP Status {e.status_code} - {e.message}")
            logger.debug(f"OpenAI API Full Status Error Details: {e.response.text if e.response else 'No response body'}")
            return None
        except openai.APIError as e: 
            logger.error(f"OpenAI API Error (model: {actual_model_id}): {e.message}", exc_info=False)
            print(f"ERROR: OpenAI API Error (model: {actual_model_id}): {e.message}")
            logger.debug(f"OpenAI API Full Error Details: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error with OpenAI API (model: {actual_model_id}): {e}", exc_info=True)
            print(f"ERROR: Unexpected OpenAI error (model: {actual_model_id}): {e}")
            return None

    async def call_llm_api(self, provider_name: str, api_key: Optional[str], endpoint_url: Optional[str], 
                           model_id: str, messages: List[Dict[str, str]], 
                           temperature: float = 0.7, max_tokens: int = 2000,
                           **kwargs 
                           ) -> Optional[str]:
        print(f"DEBUG: Attempting call_llm_api for {provider_name}. Key: {'Set' if api_key else 'Not Set'}")
        if not api_key or not endpoint_url:
            logger.warning(f"{provider_name} not configured (key or URL missing). Skipping call.")
            print(f"WARN: {provider_name} not configured in call_llm_api.")
            return None
        
        logger.info(f"Calling {provider_name} (model: {model_id}) with {len(messages)} messages.")
        print(f"INFO: Calling {provider_name} ({model_id}) messages_count: {len(messages)}")

        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        if provider_name.lower() == "openrouter": 
            headers.update({"HTTP-Referer": "http://localhost:8000", "X-Title": "EnkiBot"}) # Consider making these configurable

        payload = {"model": model_id, "messages": messages, "max_tokens": max_tokens, "temperature": temperature, **kwargs}
        
        try:
            print(f"DEBUG: Before {provider_name} POST to {endpoint_url}. Model: {model_id}")
            async with httpx.AsyncClient() as client:
                resp = await client.post(endpoint_url, json=payload, headers=headers, timeout=30.0)
            print(f"DEBUG: {provider_name} response status: {resp.status_code}")
            resp.raise_for_status()
            data = resp.json()
            if data.get("choices") and data["choices"][0].get("message") and data["choices"][0]["message"].get("content"):
                response_content = data["choices"][0]["message"]["content"].strip()
                print(f"INFO: {provider_name} successful response (first 50 chars): {response_content[:50]}")
                return response_content
            logger.warning(f"{provider_name} call to {model_id} returned no content or unexpected structure. Data: {data.get('choices')}")
            print(f"WARN: {provider_name} call to {model_id} no content. Data: {data.get('choices')}")
            return None
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error for {provider_name} ({model_id}): {e.response.status_code} - Response: {e.response.text[:500]}...", exc_info=False)
            print(f"ERROR: HTTP Error for {provider_name} ({model_id}): {e.response.status_code} - {e.response.text[:100]}")
            logger.debug(f"{provider_name} Full Error Response Content: {e.response.content}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error with {provider_name} API ({model_id}): {e}", exc_info=True)
            print(f"ERROR: Unexpected {provider_name} error ({model_id}): {e}")
            return None

    async def race_llm_calls(self, messages: List[Dict[str, str]]) -> Optional[str]:
        task_info: List[Tuple[asyncio.Task, str]] = []

        # print("DEBUG: Preparing tasks for race_llm_calls.") 
        if self.is_provider_configured("openai"):
            task = asyncio.create_task(self.call_openai_llm(messages, model_id=self.openai_model_id)) # Uses general model for race
            task_info.append((task, "OpenAI"))
            # print("DEBUG: OpenAI task created for race.")
        if self.is_provider_configured("groq"):
            task = asyncio.create_task(self.call_llm_api("Groq", self.groq_api_key, self.groq_endpoint_url, self.groq_model_id, messages))
            task_info.append((task, "Groq"))
            # print("DEBUG: Groq task created for race.")
        if self.is_provider_configured("openrouter"):
            task = asyncio.create_task(self.call_llm_api("OpenRouter", self.openrouter_api_key, self.openrouter_endpoint_url, self.openrouter_model_id, messages))
            task_info.append((task, "OpenRouter"))
            # print("DEBUG: OpenRouter task created for race.")
        
        if not task_info:
            logger.warning("No LLM providers configured for racing calls.")
            # print("WARN: No LLM providers for race_llm_calls.")
            return None

        logger.info(f"Racing LLM calls to: {[name for _, name in task_info]}")
        # print(f"INFO: Racing LLM calls to: {[name for _, name in task_info]}")
        
        task_to_provider_map = {task_obj: name for task_obj, name in task_info}
        tasks_only = [task_obj for task_obj, _ in task_info]

        for future in asyncio.as_completed(tasks_only):
            provider_name_for_log = task_to_provider_map.get(future, "UnknownProvider") 
            try:
                result = await future
                if result and result.strip():
                    logger.info(f"Successful response from {provider_name_for_log} in race.")
                    print(f"INFO: Successful response from {provider_name_for_log} in race.") 
                    return result.strip()
                else:
                    logger.warning(f"{provider_name_for_log} returned no content in race (details should be in provider-specific logs).")
                    print(f"WARN: {provider_name_for_log} returned no content in race.")
            except Exception as e: 
                logger.warning(f"Provider {provider_name_for_log} task raised an exception during race_llm_calls: {type(e).__name__} - {e} (details should be in provider-specific logs).")
                print(f"WARN: Provider {provider_name_for_log} future failed in race: {type(e).__name__} - {e}")
        
        logger.error("All LLM providers failed or returned no content in race_llm_calls.")
        print("ERROR: All LLM providers failed in race_llm_calls.") 
        return None

    async def generate_image_openai(self, 
                                    prompt: str, 
                                    n: int = 1, 
                                    size: str = "1024x1024", 
                                    quality: str = "standard", 
                                    response_format: str = "url"
                                    ) -> Optional[List[Dict[str, str]]]:
        if not self.is_provider_configured("openai"):
            logger.error("OpenAI client not configured. Cannot generate image.")
            return None
        
        # Use DALL-E model from config
        model_to_use = config.OPENAI_DALLE_MODEL_ID 
        logger.info(f"Requesting DALL-E image generation with prompt: '{prompt[:70]}...' using model {model_to_use}")
        print(f"DEBUG: DALL-E image generation prompt: '{prompt}'")

        try:
            response = await self.openai_async_client.images.generate(
                model=model_to_use,
                prompt=prompt,
                n=n,
                size=size, # Make sure this matches DALL-E 3 supported sizes
                quality=quality, # 'standard' or 'hd'
                response_format=response_format # "url" or "b64_json"
            )
            
            image_data_list = []
            if response.data:
                for image_obj in response.data:
                    if response_format == "url" and image_obj.url:
                        image_data_list.append({"url": image_obj.url})
                    elif response_format == "b64_json" and image_obj.b64_json:
                        image_data_list.append({"b64_json": image_obj.b64_json})
            
            if image_data_list:
                logger.info(f"DALL-E generated {len(image_data_list)} image(s). First URL/data snippet: {str(image_data_list[0])[:100]}")
                return image_data_list
            else:
                logger.warning(f"DALL-E call successful but no image data in expected format. Response: {response}")
                return None

        except openai.APIError as e: # Catch specific OpenAI errors
            logger.error(f"OpenAI DALL-E API Error (model: {model_to_use}): Status {e.status_code if hasattr(e, 'status_code') else 'N/A'} - {e.message}", exc_info=False)
            logger.debug(f"OpenAI DALL-E API Full Error: {e}")
        except Exception as e:
            logger.error(f"Unexpected error during DALL-E image generation: {e}", exc_info=True)
        return None
```

This `llm_services.py` includes the new `generate_image_openai` method and retains the corrected `race_llm_calls` and task-specific model ID handli


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/telegram_handlers.py ---
======================================================================


# enkibot/core/telegram_handlers.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
import logging
import asyncio
import json
from typing import Optional, Dict, Any, List, TYPE_CHECKING

from telegram import Update, ReplyKeyboardRemove
from telegram.ext import Application, ContextTypes, ConversationHandler, CommandHandler, MessageHandler, filters
from telegram.constants import ChatAction
import re 

if TYPE_CHECKING:
    # from enkibot import config # Not directly needed if values passed via __init__
    from enkibot.core.language_service import LanguageService
    from enkibot.utils.database import DatabaseManager
    from enkibot.core.llm_services import LLMServices
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.profile_manager import ProfileManager
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator

# Import config directly for any global constants from config if truly needed,
# but prefer passed dependencies for service-specific configs.
from enkibot import config as bot_config 


logger = logging.getLogger(__name__)

# Conversation states
ASK_CITY = 1
ASK_NEWS_TOPIC = 2 

class TelegramHandlerService:
    def __init__(self, 
                 application: Application, 
                 db_manager: 'DatabaseManager', 
                 llm_services: 'LLMServices',   
                 intent_recognizer: 'IntentRecognizer', 
                 profile_manager: 'ProfileManager',     
                 api_router: 'ApiRouter',             
                 response_generator: 'ResponseGenerator', 
                 language_service: 'LanguageService',
                 allowed_group_ids: set, 
                 bot_nicknames: list
                ):
        logger.info("TelegramHandlerService __init__ STARTING")
        self.application = application
        self.db_manager = db_manager
        self.llm_services = llm_services 
        self.intent_recognizer = intent_recognizer
        self.profile_manager = profile_manager
        self.api_router = api_router
        self.response_generator = response_generator
        self.language_service = language_service
        
        self.allowed_group_ids = allowed_group_ids 
        self.bot_nicknames = bot_nicknames 
        
        # This dictionary will store temporary data for ongoing conversations by chat_id
        # e.g., {'chat_id': {'action_type': 'ask_city_weather', 'original_message_id': 123}}
        self.pending_action_data: Dict[int, Dict[str, Any]] = {}
        logger.info("TelegramHandlerService __init__ COMPLETED")

    async def log_message_and_profile_tasks(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not update.message or not update.message.text or not update.effective_user: 
            logger.debug("log_message_and_profile_tasks: Skipping due to missing message, text, or user.")
            return
        
        chat_id = update.effective_chat.id
        user = update.effective_user
        message = update.message

        # If we only want to log messages from allowed groups *if the bot is triggered to respond*,
        # this check might be better placed after _is_triggered.
        # For now, it logs all messages from allowed groups.
        if self.allowed_group_ids and chat_id not in self.allowed_group_ids:
            # logger.debug(f"log_message_and_profile_tasks: Skipping msg from chat {chat_id} (not in allowed group IDs: {self.allowed_group_ids}).")
            return # Do not log messages from unallowed groups

        current_lang_for_log = self.language_service.current_lang 

        action_taken = await self.db_manager.log_chat_message_and_upsert_user(
            chat_id=chat_id, user_id=user.id, username=user.username,
            first_name=user.first_name, last_name=user.last_name,
            message_id=message.message_id, message_text=message.text,
            preferred_language=current_lang_for_log 
        )
        logger.info(f"Message from user {user.id} logged. Profile action: {action_taken}.")

        name_var_prompts = self.language_service.get_llm_prompt_set("name_variation_generator")
        if action_taken and action_taken.lower() == "insert" and name_var_prompts and "system" in name_var_prompts:
            logger.info(f"New user {user.id} ({user.first_name}). Queuing name variation generation.")
            asyncio.create_task(self.profile_manager.populate_name_variations_with_llm(
                user_id=user.id, first_name=user.first_name, last_name=user.last_name, username=user.username,
                system_prompt=name_var_prompts["system"], 
                user_prompt_template=name_var_prompts.get("user","Generate for: {name_info}")
            ))
        elif action_taken and action_taken.lower() == "insert" and not name_var_prompts:
            logger.warning("Could not generate name variations for new user: name_variation_generator prompt missing.")

        profile_create_prompts = self.language_service.get_llm_prompt_set("profile_creator")
        profile_update_prompts = self.language_service.get_llm_prompt_set("profile_updater")
        if message.text and len(message.text.strip()) > 10:
            if profile_create_prompts and "system" in profile_create_prompts and \
               profile_update_prompts and "system" in profile_update_prompts:
                logger.info(f"Message from user {user.id} meets criteria for profile analysis. Queuing task.")
                asyncio.create_task(self.profile_manager.analyze_and_update_user_profile(
                    user_id=user.id, message_text=message.text,
                    create_system_prompt=profile_create_prompts["system"],
                    create_user_prompt_template=profile_create_prompts.get("user","Analyze: {message_text}"),
                    update_system_prompt=profile_update_prompts["system"],
                    update_user_prompt_template=profile_update_prompts.get("user","Update based on: {message_text} with existing: {current_profile_notes}")
                ))
            else: 
                logger.warning(f"Profile prompts missing/malformed for lang '{current_lang_for_log}'. Skipping profile analysis for user {user.id}.")

    async def _is_triggered(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt_lower: str) -> bool:
        if not update.message or not context.bot:
            return False
        
        current_chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ['group', 'supergroup']

        if not is_group:
            logger.debug(f"_is_triggered: True (private chat with user {update.effective_user.id if update.effective_user else 'Unknown'})")
            return True

        if self.allowed_group_ids and current_chat_id not in self.allowed_group_ids:
            logger.info(f"_is_triggered: False (group {current_chat_id} not in self.allowed_group_ids: {self.allowed_group_ids})")
            return False
        
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        is_at_mentioned = bool(bot_username_lower and f"@{bot_username_lower}" in user_msg_txt_lower)
        is_nickname_mentioned = any(re.search(r'\b' + re.escape(nick.lower()) + r'\b', user_msg_txt_lower, re.I) for nick in self.bot_nicknames)
        is_bot_mentioned = is_at_mentioned or is_nickname_mentioned
        is_reply_to_bot = (update.message.reply_to_message and 
                           update.message.reply_to_message.from_user and
                           context.bot and 
                           update.message.reply_to_message.from_user.id == context.bot.id)
        
        final_trigger_decision = is_bot_mentioned or is_reply_to_bot
        if final_trigger_decision:
            logger.info(f"_is_triggered: True (Group: {current_chat_id}): @M={is_at_mentioned}, NickM={is_nickname_mentioned}, Reply={is_reply_to_bot}")
        return final_trigger_decision

    async def _process_weather_request(self, update: Update, context: ContextTypes.DEFAULT_TYPE, city: str, original_message_id: Optional[int] = None) -> int:
        """Helper to process weather once city is known."""
        if not update.message or not update.effective_chat: return ConversationHandler.END

        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        logger.info(f"Processing weather for city: '{city}'")
        
        forecast_data = await self.api_router.get_weather_data_structured(
            location=city, 
            lang_pack_full=self.language_service.current_lang_pack_full
        )

        reply_to_id = original_message_id or update.message.message_id

        if forecast_data:
            compiler_prompts = self.language_service.get_llm_prompt_set("weather_forecast_compiler")
            if not (compiler_prompts and "system" in compiler_prompts and "user_template" in compiler_prompts):
                logger.error("Weather forecast compiler LLM prompts are missing or malformed.")
                await update.message.reply_text(self.language_service.get_response_string("weather_api_data_error", location=city), reply_to_message_id=reply_to_id)
            else:
                compiled_response = await self.response_generator.compile_weather_forecast_response(
                    forecast_data_structured=forecast_data,
                    lang_code=self.language_service.current_lang,
                    system_prompt=compiler_prompts["system"],
                    user_prompt_template=compiler_prompts["user_template"]
                )
                await update.message.reply_text(compiled_response, reply_to_message_id=reply_to_id)
        else:
            await update.message.reply_text(self.language_service.get_response_string("weather_city_not_found", location=city), reply_to_message_id=reply_to_id)
        return ConversationHandler.END

    async def _handle_weather_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        logger.info(f"TelegramHandlers: Initial WEATHER_QUERY: '{user_msg_txt}'")
        if not update.message or not update.effective_chat: return ConversationHandler.END

        location_extract_prompts = self.language_service.get_llm_prompt_set("location_extractor")
        if not (location_extract_prompts and "system" in location_extract_prompts):
            logger.error("Location extractor LLM prompts missing.")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END

        city = await self.intent_recognizer.extract_location_with_llm(
            text=user_msg_txt, lang_code=self.language_service.current_lang,
            system_prompt=location_extract_prompts["system"], 
            user_prompt_template=location_extract_prompts.get("user","{text}")
        )

        if city:
            return await self._process_weather_request(update, context, city)
        else:
            logger.info("No city identified for weather, asking user.")
            self.pending_action_data[update.effective_chat.id] = {
                "action_type": "ask_city_weather", 
                "original_message_id": update.message.message_id 
            }
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city"))
            return ASK_CITY

    async def _process_news_request(self, update: Update, context: ContextTypes.DEFAULT_TYPE, topic: Optional[str], original_message_id: Optional[int] = None) -> int:
        """Helper to process news once topic is known (or if general news)."""
        if not update.message or not update.effective_chat: return ConversationHandler.END
        
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        logger.info(f"Processing news for topic: '{topic if topic else 'General'}'")

        reply_to_id = original_message_id or update.message.message_id
        
        articles_structured = await self.api_router.get_latest_news_structured(
            query=topic, 
            lang_code=self.language_service.current_lang
        )

        if articles_structured is not None:
            if not articles_structured:
                no_articles_key = "news_api_no_articles" if topic else "news_api_no_general_articles"
                await update.message.reply_text(self.language_service.get_response_string(no_articles_key, query=topic or ""), reply_to_message_id=reply_to_id)
            else:
                compiler_prompts = self.language_service.get_llm_prompt_set("news_compiler")
                if not (compiler_prompts and "system" in compiler_prompts and "user_template" in compiler_prompts):
                    logger.error("News compiler LLM prompts are missing or malformed.")
                    await update.message.reply_text(self.language_service.get_response_string("news_api_data_error"), reply_to_message_id=reply_to_id)
                else:
                    compiled_response = await self.response_generator.compile_news_response(
                        articles_structured=articles_structured, topic=topic,
                        lang_code=self.language_service.current_lang,
                        system_prompt=compiler_prompts["system"],
                        user_prompt_template=compiler_prompts["user_template"]
                    )
                    await update.message.reply_text(compiled_response, disable_web_page_preview=True, reply_to_message_id=reply_to_id)
        else:
            await update.message.reply_text(self.language_service.get_response_string("news_api_error"), reply_to_message_id=reply_to_id)
        return ConversationHandler.END

    async def _handle_news_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        logger.info(f"TelegramHandlers: Initial NEWS_QUERY: '{user_msg_txt}'")
        if not update.message or not update.effective_chat: return ConversationHandler.END

        news_topic_prompts = self.language_service.get_llm_prompt_set("news_topic_extractor")
        if not (news_topic_prompts and "system" in news_topic_prompts):
            logger.error("News topic extractor LLM prompts missing.")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END
        
        topic = await self.intent_recognizer.extract_news_topic_with_llm(
            text=user_msg_txt, lang_code=self.language_service.current_lang, 
            system_prompt=news_topic_prompts["system"], 
            user_prompt_template=news_topic_prompts.get("user","{text}")
        )

        if topic: # Topic was extracted from the initial query
            return await self._process_news_request(update, context, topic)
        else: # No topic extracted, ask the user
            logger.info("No topic identified for news from initial query, asking user.")
            self.pending_action_data[update.effective_chat.id] = {
                "action_type": "ask_news_topic",
                "original_message_id": update.message.message_id
            }
            await update.message.reply_text(self.language_service.get_response_string("news_ask_topic"))
            return ASK_NEWS_TOPIC

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        if not update.message or not update.message.text or not update.effective_chat or not update.effective_user: 
            return None 
        
        await self.language_service.determine_language_context(
            update.message.text, 
            chat_id=update.effective_chat.id,
            update_context=update
        )
        logger.critical(f"POST-DETECTION LANGUAGE for '{update.message.text[:30]}...': {self.language_service.current_lang} (ChatID: {update.effective_chat.id})")
        
        await self.log_message_and_profile_tasks(update, context)
        
        chat_id = update.effective_chat.id
        user_msg_txt = update.message.text
        
        # Check if this message is a response to a pending action
        if chat_id in self.pending_action_data:
            pending_action_details = self.pending_action_data.pop(chat_id) # Consume the pending action
            action_type = pending_action_details.get("action_type")
            original_msg_id = pending_action_details.get("original_message_id")

            if action_type == "ask_city_weather":
                logger.info(f"Continuing 'ask_city_weather'. User provided city: '{user_msg_txt}'")
                return await self._process_weather_request(update, context, user_msg_txt, original_msg_id)
            elif action_type == "ask_news_topic":
                logger.info(f"Continuing 'ask_news_topic'. User provided topic: '{user_msg_txt}'")
                return await self._process_news_request(update, context, user_msg_txt, original_msg_id)
        
        # If no pending action, proceed with normal message handling
        user_msg_txt_lower = user_msg_txt.lower()
        if not await self._is_triggered(update, context, user_msg_txt_lower): 
            return None

        master_intent_prompts = self.language_service.get_llm_prompt_set("master_intent_classifier")
        master_intent = "UNKNOWN_INTENT"
        if master_intent_prompts and "system" in master_intent_prompts:
            user_template_for_master = master_intent_prompts.get("user","{text_to_classify}")
            master_intent = await self.intent_recognizer.classify_master_intent(
                text=user_msg_txt, lang_code=self.language_service.current_lang,
                system_prompt=master_intent_prompts["system"], user_prompt_template=user_template_for_master )
        else: 
            logger.error(f"Master intent classification prompt set missing/malformed for lang '{self.language_service.current_lang}'.")
        
        logger.info(f"Master Intent for '{user_msg_txt[:50]}...' (lang: {self.language_service.current_lang}) classified as: {master_intent}")

        if master_intent == "WEATHER_QUERY":
            return await self._handle_weather_intent(update, context, user_msg_txt)
        elif master_intent == "NEWS_QUERY": 
            return await self._handle_news_intent(update, context, user_msg_txt)
        elif master_intent == "MESSAGE_ANALYSIS_QUERY": 
            is_valid_reply_scenario = (
                update.message.reply_to_message and update.message.reply_to_message.text and 
                update.message.reply_to_message.from_user and context.bot and 
                update.message.reply_to_message.from_user.id != context.bot.id )
            if is_valid_reply_scenario:
                await self._handle_message_analysis_query(update, context, user_msg_txt)
            else:
                logger.info("MESSAGE_ANALYSIS_QUERY classified, but not valid reply. Treating as GENERAL_CHAT.")
                await self._handle_user_profile_query_or_general_chat(update, context, user_msg_txt, "GENERAL_CHAT")
        elif master_intent in ["USER_PROFILE_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]:
            await self._handle_user_profile_query_or_general_chat(update, context, user_msg_txt, master_intent)
        else: 
            logger.warning(f"Unhandled master intent type: {master_intent}. Falling back to general.")
            await self._handle_user_profile_query_or_general_chat(update, context, user_msg_txt, "UNKNOWN_INTENT")
        return None # Default return if no conversation state change

    # --- Other handlers (_handle_message_analysis_query, _handle_user_profile_query_or_general_chat,
    #      start_command, help_command, error_handler) remain largely the same as message #38 ---
    # Ensure they use self.language_service.get_response_string() etc.

    async def _handle_message_analysis_query(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> bool:
        # ... (Full implementation from message #38, using self.language_service and self.bot_nicknames) ...
        if not (update.message and update.message.reply_to_message and 
                update.message.reply_to_message.text and 
                update.message.reply_to_message.from_user and
                context.bot and update.message.reply_to_message.from_user.id != context.bot.id):
            logger.debug("_handle_message_analysis_query: Conditions not met for reply analysis.")
            return False 
        
        logger.info("TelegramHandlers: Processing MESSAGE_ANALYSIS_QUERY.")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        original_text = update.message.reply_to_message.text
        question_for_analysis = user_msg_txt
        
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        cleaned_question = user_msg_txt.lower()
        for nick in self.bot_nicknames + ([f"@{bot_username_lower}"] if bot_username_lower else []): # Use self.bot_nicknames
            cleaned_question = cleaned_question.replace(nick.lower(), "").strip()
        if len(cleaned_question) < 5: 
            question_for_analysis = self.language_service.get_response_string("replied_message_default_question")
        
        analyzer_prompts = self.language_service.get_llm_prompt_set("replied_message_analyzer")
        if not (analyzer_prompts and "system" in analyzer_prompts) : 
            logger.error("Prompt set for replied message analysis is missing or malformed."); 
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message")); 
            return True 
        
        analysis_result = await self.response_generator.analyze_replied_message(
            original_text=original_text, user_question=question_for_analysis,
            system_prompt=analyzer_prompts["system"], user_prompt_template=analyzer_prompts.get("user") )
        await update.message.reply_text(analysis_result)
        return True 

    async def _handle_user_profile_query_or_general_chat(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str, master_intent: str) -> None:
        # ... (Full implementation from message #38, including Babel for language name) ...
        logger.info(f"TelegramHandlers: Handling {master_intent}: '{user_msg_txt[:50]}...'")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        main_orchestrator_prompts = self.language_service.get_llm_prompt_set("main_orchestrator")
        system_prompt_override = "You are EnkiBot, a helpful AI assistant." 
        if main_orchestrator_prompts and "system" in main_orchestrator_prompts:
            system_prompt_template = main_orchestrator_prompts["system"]
            language_name_for_prompt = self.language_service.current_lang 
            try: 
                from babel import Locale
                locale = Locale.parse(self.language_service.current_lang)
                language_name_for_prompt = locale.get_display_name('en') 
            except ImportError: logger.debug("Babel not installed, using ISO code as lang_name in main_orchestrator prompt.")
            except Exception as e: logger.warning(f"Could not get display name for lang {self.language_service.current_lang}: {e}")
            try:
                system_prompt_override = system_prompt_template.format(language_name=language_name_for_prompt, lang_code=self.language_service.current_lang)
            except KeyError as ke:
                logger.error(f"KeyError formatting main_orchestrator system prompt: {ke}. Using template as is: '{system_prompt_template}'")
                system_prompt_override = system_prompt_template
            logger.info(f"Using system prompt for {master_intent} with language instruction for {self.language_service.current_lang}")
        
        logger.critical(f"FINAL SYSTEM PROMPT for get_orchestrated_llm_response (lang: {self.language_service.current_lang}): '{system_prompt_override}'")
        
        reply = await self.response_generator.get_orchestrated_llm_response(
            prompt_text=user_msg_txt, chat_id=update.effective_chat.id, user_id=update.effective_user.id, 
            message_id=update.message.message_id, context=context, lang_code=self.language_service.current_lang,
            system_prompt_override=system_prompt_override, 
            user_search_ambiguous_response_template=self.language_service.get_response_string("user_search_ambiguous_clarification"),
            user_search_not_found_response_template=self.language_service.get_response_string("user_search_not_found_in_db") )
        if reply: await update.message.reply_text(reply)
        else: await update.message.reply_text(self.language_service.get_response_string("llm_error_fallback"))

    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.effective_user or not update.message: return
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE) 
        await update.message.reply_html(self.language_service.get_response_string("start", user_mention=update.effective_user.mention_html()))

    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message: return
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE)
        await update.message.reply_text(self.language_service.get_response_string("help"))

    async def news_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]: 
        logger.info(f"TelegramHandlers: /news command received by user {update.effective_user.id if update.effective_user else 'Unknown'}")
        if not update.message or not update.effective_chat: return ConversationHandler.END # End if no context
        
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE)
        # Directly call _handle_news_intent. It will ask for topic if needed.
        # Pass a generic text or the command itself if _handle_news_intent expects some text for initial topic extraction
        return await self._handle_news_intent(update, context, user_msg_txt=update.message.text or "/news")

    async def error_handler(self, update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
        # ... (same as message #38)
        logger.error(f'Update "{update}" caused error "{context.error}"', exc_info=True)
        if isinstance(update, Update) and update.effective_chat:
            try:
                error_msg = self.language_service.get_response_string("generic_error_message", "Oops! Something went very wrong on my end.")
                await context.bot.send_message(chat_id=update.effective_chat.id, text=error_msg)
            except Exception as e: 
                logger.error(f"CRITICAL: Error sending error message to user: {e}", exc_info=True)
    
    async def cancel_conversation(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
        """Cancels and ends the current conversation."""
        chat_id = update.effective_chat.id
        if chat_id in self.pending_action_data:
            del self.pending_action_data[chat_id]
        logger.info(f"User {update.effective_user.id if update.effective_user else ''} cancelled conversation.")
        if update.message: # Ensure message exists to reply to
            await update.message.reply_text(
                self.language_service.get_response_string("conversation_cancelled", "Okay, current operation cancelled."), 
                reply_markup=ReplyKeyboardRemove() 
            )
        return ConversationHandler.END

    def register_all_handlers(self): 
        # Command handlers outside of conversations
        self.application.add_handler(CommandHandler("start", self.start_command))
        self.application.add_handler(CommandHandler("help", self.help_command))
        # News command is now an entry point to a conversation
        # self.application.add_handler(CommandHandler("news", self.news_command)) # Removed direct, now part of conv

        # Main Conversation Handler for messages (includes weather, news flows)
        conv_handler = ConversationHandler(
            entry_points=[
                MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message),
                CommandHandler("news", self.news_command) # /news can also start a news conversation
            ],
            states={
                ASK_CITY: [MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message)],
                ASK_NEWS_TOPIC: [MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message)],
            },
            fallbacks=[CommandHandler("cancel", self.cancel_conversation)],
            map_to_parent={ # If this handler is nested, this allows ending the parent too. Not strictly needed if it's top-level.
                ConversationHandler.END: ConversationHandler.END 
            },
            allow_reentry=True 
        )
        self.application.add_handler(conv_handler)
        self.application.add_error_handler(self.error_handler)
        logger.info("TelegramHandlerService: All handlers registered.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/__init__.py ---
======================================================================

*** ERROR: Could not read file. Reason: 'utf-8' codec can't decode byte 0xf6 in position 947: invalid start byte ***


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/module_tester.py ---
======================================================================

# enkibot/evolution/module_tester.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Module Tester (Placeholder) ===
# ==================================================================================================
# This module will provide the framework for rigorously evaluating evolved
# variants of EnkiBot's modules and prompts.
# It will be responsible for:
# - Executing tests in a secure, sandboxed environment.
# - Running benchmark tests for Python modules (e.g., unit tests, performance tests).
# - Evaluating LLM prompt effectiveness using frameworks like LLM-as-a-Judge.
# - Collecting and returning detailed performance metrics to the coordinator.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

def test_variant(parent_variant, modification):
    """
    Tests a new, modified variant of a module or prompt.

    Args:
        parent_variant: The original version of the bot component.
        modification: The proposed change to be applied.

    Returns:
        A tuple containing the new child variant and its performance data.
    """
    logger.info(f"Testing a new variant with modification: {modification} (mock).")
    # In the future, this function would:
    # 1. Apply the modification in a sandboxed environment.
    # 2. Run a suite of tests (unit, integration, performance).
    # 3. Evaluate against the multi-objective fitness function.
    # 4. Return the results.
    
    mock_performance_data = {"task_success": 0.95, "efficiency": 120, "safety_score": 1.0}
    
    # The new variant would be a representation of the modified code/prompt
    new_child_variant = {"id": "variant-002", "parent": "variant-001", "modification": modification}
    
    return new_child_variant, mock_performance_data


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/self_improvement_coordinator.py ---
======================================================================

# enkibot/evolution/self_improvement_coordinator.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Self-Improvement Coordinator (Placeholder) ===
# ==================================================================================================
# This module will serve as the central nervous system for EnkiBot's evolution.
# It will be responsible for:
# - Orchestrating the evolutionary loop: selection, modification, evaluation.
# - Managing the "Agent Variant Archive" of different EnkiBot versions.
# - Triggering the modification of Python modules and LLM prompts.
# - Recording performance metadata to guide the evolutionary process.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

class SelfImprovementCoordinator:
    def __init__(self):
        logger.info("Self-Improvement Coordinator initialized (Placeholder).")
        # In the future, this will initialize the Agent Variant Archive connection.
        self.agent_variant_archive = {}

    def run_evolutionary_cycle(self):
        """
        Executes a single cycle of selection, modification, and evaluation.
        """
        logger.info("Executing a mock evolutionary cycle...")
        # 1. Select parent variant(s) from the archive.
        # parent = self.select_parent()

        # 2. Propose modifications to code or prompts.
        # modification = self.propose_modification(parent)

        # 3. Create and test the new child variant.
        # child_variant, performance_data = module_tester.test_variant(parent, modification)

        # 4. Add the new variant and its performance data to the archive.
        # self.archive_variant(child_variant, performance_data)
        logger.info("Mock evolutionary cycle complete.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/__init__.py ---
======================================================================

# enkibot/lang/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This directory holds the language packs (JSON files) for EnkiBot.
# Each file corresponds to a language code (e.g., 'en', 'ru') and contains
# all user-facing strings and system prompts for that language.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/en.json ---
======================================================================

﻿{
  "prompts": {
    "master_intent_classifier": {
      "system": "You are an AI routing assistant. Your task is to classify the user's intent based on their message. Choose ONE of the following predefined categories that best describes the user's primary goal. Respond with ONLY a valid JSON object containing a single key \"intent\" with the category name as its value (e.g., {\"intent\": \"WEATHER_QUERY\"}).\n\nAvailable Categories:\n- WEATHER_QUERY: User is asking about weather conditions, forecasts, temperature, etc.\n- NEWS_QUERY: User is asking for news articles, headlines, or updates on current breaking events or specific news topics.\n- USER_PROFILE_QUERY: User is asking for information about a specific person (e.g., 'who is [name]?', 'tell me about [name]'), including requests for specific details, facts, lists, or analyses related to that person (e.g., 'Kiyosaki's failed predictions', 'biography of X', 'list of Y's accomplishments').\n- MESSAGE_ANALYSIS_QUERY: User is replying to another message and asking you (the bot) to analyze, summarize, or comment on that replied-to message.\n- GENERAL_CHAT: User is making a general statement, asking a general knowledge question, seeking information or analysis not fitting other specific categories (e.g. 'explain black holes', 'compare X and Y'), or engaging in casual conversation.\n- UNKNOWN_INTENT: If the intent is very unclear or doesn't fit any other category despite the broader definitions.",
      "user": "{text_to_classify}"
    },
    "name_variation_generator": {
      "system": "You are a language expert specializing in Russian and English names. Your task is to generate a list of linguistic variations for a user's name. Focus ONLY on realistic, human-used variations. DO NOT generate technical usernames with numbers or suffixes like '_dev'.\n\n**Goal:** Create variations for recognition in natural language text.\n\n**Categories for Generation:**\n1.  **Original Forms:** The original first name, last name, and combinations.\n2.  **Diminutives & Nicknames:** Common short and affectionate forms (e.g., 'Antonina' -> 'Tonya'; 'Robert' -> 'Rob').\n3.  **Transliteration (with variants):** Provide multiple common Latin spellings for all Cyrillic forms (original and diminutives). Example for 'Тоня': 'tonya', 'tonia'.\n4.  **Reverse Transliteration:** If the source name is Latin, provide plausible Cyrillic versions. Example for 'Yael': 'Яэль', 'Йаэль'.\n5.  **Russian Declensions (Grammatical Cases):** For all primary Russian names (full and short forms), provide their forms in different grammatical cases (genitive, dative, accusative, instrumental, prepositional). Example for 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Output Format:** Return a single JSON object: {\"variations\": [\"variation1\", \"variation2\", ...]}. All variations must be in lowercase.",
      "user": "Generate linguistic variations for the user with the following info: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "You are an AI analyst. Your task is to analyze the 'Original Text' and provide a meaningful response to the 'User's Question' about that text. Your analysis should be objective, concise, and to the point. If the question is generic (e.g., 'what do you think?'), provide a brief summary, highlighting the key points or sentiment of the original text.",
      "user": "Original Text for analysis:\n---\n\"{original_text}\"\n---\n\nUser's question about this text:\n---\n\"{user_question}\"\n---\n\nYour analysis:"
    },
    "weather_intent_analyzer": {
      "system": "You are an expert in analyzing weather-related requests. Your task is to determine the user's intent. Does the user want the 'current' weather or a 'forecast' for several days? If it is a forecast, also determine for how many days. Your answer MUST be a valid JSON object and nothing else.\n\nExamples:\n- User text: 'weather in London' -> Your response: {\"type\": \"current\"}\n- User text: 'what's the weather like?' -> Your response: {\"type\": \"current\"}\n- User text: 'weather in Tampa for the week' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'forecast for 5 days in Berlin' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'what will the weather be like tomorrow?' -> Your response: {\"type\": \"forecast\", \"days\": 2}\n- User text: 'give me the forecast for Saturday' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'just give me a weather forecast' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'forecast for the weekend' -> Your response: {\"type\": \"forecast\", \"days\": 3}\n\nFallback Rule: If you are unsure, always default to 'current'.",
      "user": "{text}"
    },
    "location_extractor": {
      "system": "You are an expert text analysis tool. Your task is to extract a city or location name from the user's text. Analyze the following text and identify the geographical location (city, region, country) mentioned. Return ONLY the name of the location in English, suitable for a weather API query. For example, if the text is 'what's the weather in Saint Petersburg', you must return 'Saint Petersburg'. If no specific location is found, you MUST return the single word: None",
      "user": "{text}"
    },
    "news_topic_extractor": {
      "system": "You are an expert text analysis tool. Your task is to extract the main topic, keyword, or location from a user's request for news. Analyze the text. If it contains a specific subject, you MUST return that subject in its base (nominative) case and in the original language of the request. For example, for a request 'news in Moscow', you must return 'Moscow'. For 'news about cars', return 'cars'. If the request is general (e.g., 'what's the news?', 'latest headlines'), you MUST return the single word: None",
      "user": "{text}"
    },
    "profile_creator": {
      "system": "You are an AI psychologist and profiler. Your task is to create an initial psychological profile of a user based on their message. Analyze the text for communication style, possible personality traits (using the 'Big Five' model as a guide: Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism), and key interests. Your response MUST be strictly structured in the proposed Markdown format with headers. Be objective and base your analysis only on the provided text.",
      "user": "Analyze the following message from a new user and create their psychological profile.\n\nUser's message:\n---\n\"{message_text}\"\n---\n\nYour result (strictly in Markdown format):"
    },
    "profile_updater": {
      "system": "You are an AI psychologist updating a patient's file. You are provided with an 'Existing psychological profile' and a 'New message' from the user. Your task is not just to add new information, but to **re-evaluate and synthesize the entire profile**. If the new message confirms a trait, strengthen its description. If it contradicts, correct or soften it. If it reveals something new, integrate it into the existing structure. The goal is to obtain an evolved, yet still concise, profile. Maintain the original Markdown structure.",
      "user": "Existing psychological profile:\n---\n{current_profile_notes}\n---\n\nNew message from the user for analysis:\n---\n\"{message_text}\"\n---\n\nYour updated and re-evaluated psychological profile:"
    },
    "main_orchestrator": {
      "system": "You are EnkiBot, an intelligent and friendly AI assistant in a Telegram chat, created by Yael Demedetskaya. Your primary goal is to be helpful, engaging, and informative. You have access to long-term memory about conversations and user profiles. When asked about someone, synthesize information from their profile (dossier) and recent messages. **It is CRITICAL that your entire response be in {language_name} (language code: {lang_code}). Do not switch languages.** Respond naturally. Be polite but not overly formal."
    },
    "language_detector_llm": {
      "system": "Ты — эксперт по определению языка. Проанализируй предоставленный текст, который включает 'Последнее сообщение пользователя' и, возможно, 'Недавнюю историю чата'. 'Последнее сообщение пользователя' наиболее важно для определения его основного языка. Ответь ТОЛЬКО валидным JSON-объектом следующего вида: {\"primary_lang\": \"ru\", \"confidence\": 0.95, \"other_detected_langs\": [\"en\", \"de\"]}. 'primary_lang' должен быть ISO 639-1 кодом языка последнего сообщения. 'confidence' — это твоя уверенность (от 0.0 до 1.0) в определении primary_lang *последнего сообщения*. 'other_detected_langs' — это необязательный список других ISO 639-1 кодов языков, значительно представленных во всем тексте, если таковые имеются.",
      "user_template_full_context": "Пожалуйста, определи язык 'Последнего сообщения пользователя', учитывая 'Недавнюю историю чата'.\n\nПоследнее сообщение пользователя:\n```text\n{latest_message}\n```\n\nНедавняя история чата (старые сообщения сначала):\n```text\n{history_context}\n```",
      "user_template_latest_only": "Пожалуйста, определи язык следующего 'Последнего сообщения пользователя':\n\nПоследнее сообщение пользователя:\n```text\n{latest_message}\n```"
    },
    "weather_forecast_compiler": {
      "system": "You are a helpful and friendly weather reporter. Based on the provided JSON data for a multi-day weather forecast for {location}, create a concise, easy-to-read, and engaging natural language summary for the user. Respond in {language_name}. Highlight any significant weather events, temperature trends, and overall conditions. For each day, you can mention the day name, expected condition, and temperature range (min/max or average). Aim for a conversational tone. Do not just list the data; interpret it into a nice forecast summary. The user has already been told you are generating the forecast.",
      "user_template": "Here is the weather data for {location}:\n```json\n{forecast_data_json}\n```\nPlease provide the forecast summary."
    },
    "news_compiler": {
      "system": "You are an expert news summarizer AI. You will be given a list of news articles as JSON data, potentially related to the topic: '{topic}'. Your task is to generate a concise, informative, and engaging news digest in {language_name}. Summarize the overall situation if a common theme emerges. Highlight 2-3 of the most important or interesting headlines, briefly stating their core point and source. Do not just list all articles. Provide a synthesized overview. If the topic is 'None' or general, summarize general top news. Ensure the output is well-formatted for a chat message.",
      "user_template": "Here are the latest news articles (topic: '{topic}'):\n```json\n{articles_json}\n```\nPlease provide a news digest."
    }
  },
  "responses": {
    "news_ask_topic": "Sure, I can fetch the news for you! What topic are you interested in today?",
    "weather_api_data_error": "I received some weather data for {location}, but I'm having a bit of trouble interpreting it right now. You might want to check a standard weather app.",
    "news_api_data_error": "I found some news articles, but I'm having a little trouble summarizing them for you at the moment. You can try checking a news website directly.",
    "start": "Hello, {user_mention}! I am EnkiBot, created by Yael Demedetskaya. How can I help you?",
    "help": "I am EnkiBot, an AI assistant by Yael Demedetskaya.\nIn group chats, I respond when you mention me by name (@EnkiBot, Enki) or reply to my messages.\nYou can ask me 'tell me about [name/topic]' for me to search for information in the chat history.\nTo get the weather, ask 'what's the weather in [city]?'.\nTo get news, ask 'what's the news?' or 'news about [topic]?'.\n\n**Commands:**\n/start - Start interaction\n/help - This help message\n/news - Get the latest news",
    "weather_ask_city": "I can get the weather for you, but for which city?",
    "llm_error_fallback": "Sorry, I couldn't process that request right now. Please try again later.",
    "generic_error_message": "Oops! Something went wrong on my end. I've logged the issue and my developers will look into it.",
    "language_pack_creation_failed_fallback": "I'm having a little trouble understanding that language fully right now, but I'll try my best in English. How can I help?",
    "user_search_ambiguous_clarification": "I found multiple users matching that name: {user_options}. Who are you asking about? Please clarify (e.g., by @username).",
    "user_search_not_found_in_db": "I couldn't find any information about '{search_term}' in my records for this chat.",
    "api_lang_code_openweathermap": "en",
    "weather_api_key_missing": "Weather service: API key missing.",
    "weather_report_intro_current": "Current weather in {city}:",
    "weather_condition_label": "Condition",
    "weather_temp_label": "Temperature",
    "weather_feels_like_label": "feels like",
    "weather_wind_label": "Wind",
    "weather_city_not_found": "Sorry, I couldn't find the city '{location}'.",
    "weather_server_error": "Could not get weather data due to a server error.",
    "weather_unexpected_error": "An unexpected error occurred while fetching weather.",
    "weather_forecast_unavailable": "Forecast data is unavailable for '{location}'.",
    "weather_report_intro_forecast": "Weather forecast for {city}:",
    "weather_city_not_found_forecast": "Sorry, I couldn't find '{location}' for the forecast.",
    "weather_server_error_forecast": "Could not get forecast data due to a server error.",
    "weather_unexpected_error_forecast": "An unexpected error occurred while fetching the forecast.",
    "weather_unknown_type": "Unknown weather request type.",
    "news_api_key_missing": "News service: API key missing.",
    "news_api_error": "Could not fetch news at this time. The news service might be temporarily unavailable.",
    "news_api_no_articles": "I couldn't find any news articles for your query '{query}'.",
    "news_api_no_general_articles": "I couldn't find any general news articles right now.",
    "news_report_title_topic": "News on '{topic}':",
    "news_report_title_general": "Latest News:",
    "news_unexpected_error": "An unexpected error occurred while fetching news.",
    "news_read_more": "Read: {url}",
    "replied_message_default_question": "Analyze this text, identify the main idea, and share your opinion.",
    "llm_no_assistants": "Sorry, none of my AI assistants are available right now.",
    "analysis_error": "Sorry, an error occurred during the text analysis.",
    "analysis_client_not_configured": "The analysis function cannot be performed as the AI client is not configured.",
    "conversation_cancelled": "Okay, the current operation has been cancelled."
  },
  "weather_conditions_map": {
    "clear_sky": "Clear sky",
    "few_clouds": "Few clouds",
    "scattered_clouds": "Scattered clouds",
    "broken_clouds": "Broken clouds",
    "overcast_clouds": "Overcast clouds",
    "shower_rain": "Shower rain",
    "light_intensity_shower_rain": "Light intensity shower rain",
    "rain": "Rain",
    "light_rain": "Light rain",
    "moderate_rain": "Moderate rain",
    "heavy_intensity_rain": "Heavy intensity rain",
    "thunderstorm": "Thunderstorm",
    "snow": "Snow",
    "light_snow": "Light snow",
    "mist": "Mist",
    "fog": "Fog",
    "smoke": "Smoke",
    "haze": "Haze",
    "sand_dust_whirls": "Sand/Dust Whirls",
    "squalls": "Squalls",
    "tornado": "Tornado",
    "unknown_condition": "Condition unknown"
  },
  "days_of_week": {
    "Monday": "Monday",
    "Tuesday": "Tuesday",
    "Wednesday": "Wednesday",
    "Thursday": "Thursday",
    "Friday": "Friday",
    "Saturday": "Saturday",
    "Sunday": "Sunday"
  }
}


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/ru.json ---
======================================================================

﻿{
  "prompts": {
    "master_intent_classifier": {
      "system": "Вы — AI-ассистент по маршрутизации запросов. Ваша задача — классифицировать намерение пользователя на основе его сообщения. Выберите ОДНУ из следующих предопределенных категорий, которая наилучшим образом описывает основную цель пользователя. Ответьте ТОЛЬКО валидным JSON-объектом, содержащим один ключ \"intent\" с названием категории в качестве значения (например, {\"intent\": \"WEATHER_QUERY\"}).\n\nДоступные категории:\n- WEATHER_QUERY: Пользователь спрашивает о погодных условиях, прогнозах, температуре и т.д.\n- NEWS_QUERY: Пользователь запрашивает новостные статьи, заголовки или обновления по текущим (срочным) событиям или конкретным новостным темам.\n- USER_PROFILE_QUERY: Пользователь запрашивает информацию о конкретном человеке (например, 'кто такой [имя]?', 'расскажи о [имя]'), включая запросы на конкретные детали, факты, списки или анализ, связанные с этим человеком (например, 'несбывшиеся прогнозы Кийосаки', 'биография X', 'список достижений Y').\n- MESSAGE_ANALYSIS_QUERY: Пользователь отвечает на другое сообщение и просит вас (бота) проанализировать, резюмировать или прокомментировать это сообщение, на которое был дан ответ.\n- GENERAL_CHAT: Пользователь делает общее утверждение, задает общепознавательный вопрос, ищет информацию или анализ, не подходящие под другие специфические категории (например, 'объясни черные дыры', 'сравни X и Y'), или ведет непринужденную беседу.\n- UNKNOWN_INTENT: Если намерение очень неясно или не соответствует ни одной другой категории, несмотря на более широкие определения.",
      "user": "{text_to_classify}"
    },
    "name_variation_generator": {
      "system": "Ты — эксперт по языкам, специализирующийся на русских и английских именах. Твоя задача — сгенерировать список лингвистических вариантов имени пользователя. Сосредоточься ТОЛЬКО на реалистичных, используемых людьми вариантах. НЕ генерируй технические имена пользователей с цифрами или суффиксами вроде '_dev'.\n\n**Цель:** Создать варианты для распознавания в тексте на естественном языке.\n\n**Категории для генерации:**\n1.  **Оригинальные формы:** Исходное имя, фамилия и их комбинации.\n2.  **Уменьшительно-ласкательные формы и прозвища:** Распространенные короткие и ласковые формы (например, 'Антонина' -> 'Тоня'; 'Роберт' -> 'Роб').\n3.  **Транслитерация (с вариантами):** Предоставь несколько распространенных латинских написаний для всех кириллических форм (оригинальных и уменьшительных). Пример для 'Тоня': 'tonya', 'tonia'.\n4.  **Обратная транслитерация:** Если исходное имя на латинице, предоставь правдоподобные кириллические версии. Пример для 'Yael': 'Яэль', 'Йаэль'.\n5.  **Русские склонения (падежи):** Для всех основных русских имен (полных и кратких форм) предоставь их формы в различных падежах (родительный, дательный, винительный, творительный, предложный). Пример для 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Формат вывода:** Верни единый JSON-объект: {\"variations\": [\"вариант1\", \"вариант2\", ...]}. Все варианты должны быть в нижнем регистре.",
      "user": "Сгенерируй лингвистические варианты для пользователя с информацией: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "Ты — AI-аналитик. Твоя задача — проанализировать 'Исходный текст' и дать содержательный ответ на 'Вопрос пользователя' об этом тексте. Твой анализ должен быть объективным, кратким и по существу. Если вопрос общий (например, 'что думаешь?'), сделай краткое резюме, выделив ключевые тезисы или настроения в исходном тексте.",
      "user": "Исходный текст для анализа:\n---\n\"{original_text}\"\n---\n\nВопрос пользователя об этом тексте:\n---\n\"{user_question}\"\n---\n\nТвой анализ:"
    },
    "weather_intent_analyzer": {
      "system": "Вы эксперт по анализу запросов, связанных с погодой. Ваша задача — определить намерение пользователя. Хочет ли пользователь узнать 'текущую' погоду или 'прогноз' на несколько дней? Если это прогноз, также определите, на сколько дней. Ваш ответ ДОЛЖЕН быть действительным объектом JSON и ничем другим.\nПримеры:\n- Текст пользователя: 'погода в Лондоне' -> Ваш ответ: {\"type\": \"current\"}\n- Текст пользователя: 'какая сейчас погода?' -> Ваш ответ: {\"type\": \"current\"}\n- Текст пользователя: 'погода в Тампе на неделю' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 7}\n- Текст пользователя: 'прогноз на 5 дней в Берлине' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 5}\n- Текст пользователя: 'какая погода будет завтра?' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 2}\n- Текст пользователя: 'дай прогноз на субботу' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 7}\n- Текст пользователя: 'просто дай прогноз погоды' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 5}\n- Текст пользователя: 'прогноз на выходные' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 3}\nЕсли вы не уверены, всегда выбирайте 'current'.",
      "user": "{text}"
    },
    "location_extractor": {
      "system": "Вы — экспертный инструмент анализа текста. Ваша задача — извлечь название города или местоположения из текста пользователя. Проанализируйте следующий текст и определите упомянутое географическое местоположение (город, регион, страну). Верните ТОЛЬКО название местоположения на английском языке, подходящее для запроса к API погоды. Например, если текст 'какая погода в Санкт-Петербурге', вы должны вернуть 'Saint Petersburg'. Если текст 'покажи погоду в Астане', вы должны вернуть 'Astana'. Если конкретное местоположение не найдено, вы ДОЛЖНЫ вернуть одно слово: None",
      "user": "{text}"
    },
    "news_topic_extractor": {
      "system": "Вы — экспертный инструмент анализа текста. Ваша задача — извлечь основную тему, ключевое слово или местоположение из запроса пользователя на получение новостей. Проанализируйте текст. Если он содержит конкретную тему, вы ДОЛЖНЫ вернуть эту тему в ее основной (именительном) падеже и на языке оригинала запроса. Например, для запроса 'новости в москве', вы должны вернуть 'Москва'. Для 'news about cars', верните 'cars'. Если запрос общий (например, 'какие новости?', 'последние заголовки'), вы ДОЛЖНЫ вернуть одно слово: None",
      "user": "{text}"
    },
    "profile_creator": {
      "system": "Ты — AI-психолог и профайлер. Твоя задача — создать первоначальный психологический портрет пользователя на основе его сообщения. Проанализируй текст на предмет стиля общения, возможных черт личности (используй модель 'Большая пятерка' как ориентир: Открытость, Добросовестность, Экстраверсия, Доброжелательность, Нейротизм), а также ключевых интересов. Твой ответ ДОЛЖЕН быть структурирован строго по предложенному формату Markdown с заголовками. Будь объективен и основывайся только на предоставленном тексте.",
      "user": "Проанализируй следующее сообщение от нового пользователя и создай его психологический профиль.\n\nСообщение пользователя:\n---\n\"{message_text}\"\n---\n\nТвой результат (строго в формате Markdown):"
    },
    "profile_updater": {
      "system": "Ты — AI-психолог, обновляющий досье на пациента. Тебе предоставлены 'Существующий психологический профиль' и 'Новое сообщение' от пользователя. Твоя задача — не просто добавить новую информацию, а **переосмыслить и синтезировать весь профиль**. Если новое сообщение подтверждает черту — усиль ее описание. Если противоречит — скорректируй или смягчи. Если открывает что-то новое — интегрируй это в существующую структуру. Цель — получить эволюционировавший, но все еще лаконичный профиль. Сохраняй исходную структуру Markdown.",
      "user": "Существующий психологический профиль:\n---\n{current_profile_notes}\n---\n\nНовое сообщение от пользователя для анализа:\n---\n\"{message_text}\"\n---\n\nТвой обновленный и переосмысленный психологический профиль:"
    },
    "main_orchestrator": {
      "system": "Ты EnkiBot... Твоя задача — ... **ОТВЕЧАЙ ТОЛЬКО НА РУССКОМ ЯЗЫКЕ (код языка: {lang_code}, название: {language_name}). ВЕСЬ ТВОЙ ОТВЕТ ДОЛЖЕН БЫТЬ НА РУССКОM ОБЯЗАТЕЛЬНО. Не используй другие языки.** Отвечай развернуто, естественно. Будь вежлив, но не слишком формален."
    },
    "weather_forecast_compiler": {
      "system": "Ты — отзывчивый и дружелюбный ведущий прогноза погоды. На основе предоставленных JSON-данных для многодневного прогноза погоды для {location}, создай краткое, легко читаемое и увлекательное описание на естественном языке для пользователя. Отвечай на {language_name}. Выдели любые значительные погодные явления, температурные тренды и общие условия. Для каждого дня можешь упомянуть название дня, ожидаемое состояние погоды и диапазон температур (мин/макс или среднюю). Старайся поддерживать разговорный тон. Не просто перечисляй данные, а интерпретируй их в приятный итоговый прогноз. Пользователю уже сообщили, что ты генерируешь прогноз.",
      "user_template": "Вот данные о погоде для {location}:\n```json\n{forecast_data_json}\n```\nПожалуйста, предоставь итоговый прогноз."
    },
    "news_compiler": {
      "system": "Ты — экспертный AI для составления сводок новостей. Тебе будет предоставлен список новостных статей в формате JSON, возможно, связанных с темой: '{topic}'. Твоя задача — сгенерировать краткую, информативную и увлекательную новостную сводку на {language_name}. Если прослеживается общая тема, обобщи ситуацию. Выдели 2-3 наиболее важные или интересные новости, кратко изложив их суть и источник. Не просто перечисляй все статьи, а предоставь синтезированный обзор. Если тема 'None' или общая, составь сводку по главным новостям в целом. Убедись, что вывод хорошо отформатирован для сообщения в чате.",
      "user_template": "Вот последние новостные статьи (тема: '{topic}'):\n```json\n{articles_json}\n```\nПожалуйста, предоставь новостную сводку."
    }
  },
  "responses": {
    "start": "Привет, {user_mention}! Я EnkiBot, создан Yael Demedetskaya. Чем могу помочь?",
    "help": "Я EnkiBot, AI-ассистент от Yael Demedetskaya.\nВ группах я отвечаю, когда вы упоминаете меня по имени (@EnkiBot, Энки) или отвечаете на мои сообщения.\nВы можете спросить меня 'расскажи о [имя/тема]', чтобы я поискал информацию в истории чата.\nЧтобы узнать погоду, спросите 'какая погода в [город]?'.\n\n**Команды:**\n/start - Начало работы\n/help - Эта справка\n/news - Последние новости",
    "weather_ask_city": "Я могу узнать погоду, но для какого города?",
    "llm_error_fallback": "Извините, не могу обработать ваш запрос прямо сейчас. Пожалуйста, попробуйте позже.",
    "generic_error_message": "Ой! Что-то пошло не так на моей стороне. Я уже записал ошибку, и мои разработчики её изучат.",
    "language_pack_creation_failed_fallback": "У меня небольшие трудности с полным пониманием этого языка прямо сейчас, но я постараюсь помочь на русском. Чем могу быть полезен?",
    "user_search_ambiguous_clarification": "Я нашел нескольких пользователей с таким именем: {user_options}. О ком именно вы спрашиваете? Пожалуйста, уточните (например, через @username).",
    "user_search_not_found_in_db": "Я не смог найти информацию о '{search_term}' в своих записях для этого чата.",
    "api_lang_code_openweathermap": "ru",
    "weather_api_key_missing": "Сервис погоды: отсутствует ключ API.",
    "weather_report_intro_current": "Текущая погода в г. {city}:",
    "weather_condition_label": "Состояние",
    "weather_temp_label": "Температура",
    "weather_feels_like_label": "ощущается как",
    "weather_wind_label": "Ветер",
    "weather_city_not_found": "Извините, я не смог найти город '{location}'.",
    "weather_server_error": "Не удалось получить данные о погоде из-за ошибки сервера.",
    "weather_unexpected_error": "Произошла непредвиденная ошибка при запросе погоды.",
    "weather_forecast_unavailable": "Данные прогноза для '{location}' недоступны.",
    "weather_report_intro_forecast": "Прогноз погоды для г. {city}:",
    "weather_city_not_found_forecast": "Извините, я не смог найти '{location}' для прогноза.",
    "weather_server_error_forecast": "Не удалось получить данные прогноза из-за ошибки сервера.",
    "weather_unexpected_error_forecast": "Произошла непредвиденная ошибка при запросе прогноза.",
    "weather_unknown_type": "Неизвестный тип запроса погоды.",
    "news_api_key_missing": "Новостной сервис: отсутствует ключ API.",
    "news_api_error": "Не удалось получить новости в данный момент. Новостной сервис может быть временно недоступен ({status_code}).",
    "news_api_no_articles": "Я не смог найти новости по вашему запросу '{query}'.",
    "news_api_no_general_articles": "Я не смог найти общие новости прямо сейчас.",
    "news_report_title_topic": "Новости по теме '{topic}':",
    "news_report_title_general": "Последние новости:",
    "news_unexpected_error": "Произошла непредвиденная ошибка при получении новостей.",
    "news_read_more": "Читать: {url}",
    "replied_message_default_question": "Проанализируй этот текст, выдели главную мысль и выскажи свое мнение.",
    "llm_no_assistants": "Извините, ни один из моих AI-помощников сейчас не доступен.",
    "analysis_error": "К сожалению, произошла ошибка во время анализа текста.",
    "analysis_client_not_configured": "Функция анализа не может быть выполнена, так как AI-клиент не настроен.",
    "news_ask_topic": "Конечно, могу подобрать для вас новости! Какая тема вас сегодня интересует?",
    "weather_api_data_error": "Я получил данные о погоде для {location}, но сейчас мне немного сложно их интерпретировать. Возможно, вам стоит проверить стандартное погодное приложение.",
    "news_api_data_error": "Я нашел несколько новостных статей, но сейчас мне немного сложно составить из них сводку. Вы можете попробовать посмотреть новости напрямую на новостном сайте.",
    "conversation_cancelled": "Хорошо, текущая операция отменена."
  },
  "weather_conditions_map": {
    "clear_sky": "Ясно",
    "few_clouds": "Малооблачно",
    "scattered_clouds": "Рассеянная облачность",
    "broken_clouds": "Переменная облачность",
    "overcast_clouds": "Пасмурно",
    "shower_rain": "Ливень",
    "light_intensity_shower_rain": "Небольшой ливень",
    "rain": "Дождь",
    "light_rain": "Небольшой дождь",
    "moderate_rain": "Умеренный дождь",
    "heavy_intensity_rain": "Сильный дождь",
    "thunderstorm": "Гроза",
    "snow": "Снег",
    "light_snow": "Небольшой снег",
    "mist": "Дымка",
    "fog": "Туман",
    "smoke": "Смог",
    "haze": "Мгла",
    "sand_dust_whirls": "Песчаные/пыльные вихри",
    "squalls": "Шквалы",
    "tornado": "Торнадо",
    "unknown_condition": "Состояние неизвестно"
  },
  "days_of_week": {
    "Monday": "Понедельник",
    "Tuesday": "Вторник",
    "Wednesday": "Среда",
    "Thursday": "Четверг",
    "Friday": "Пятница",
    "Saturday": "Суббота",
    "Sunday": "Воскресенье"
  }
}


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/main.py ---
======================================================================

# enkibot/main.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import os
import logging
import asyncio
from typing import Optional 
from telegram import Update 
from telegram.ext import Application 

from enkibot import config
from enkibot.utils.logging_config import setup_logging
from enkibot.utils.database import initialize_database
# --- MODIFIED IMPORT ---
from enkibot.app import EnkiBotApplication 
# --- END MODIFICATION ---

logger: Optional[logging.Logger] = None 

def clear_terminal():
    os.system('cls' if os.name == 'nt' else 'clear')

# Commented out backfill for now, as it needs careful setup if run from here
# async def run_backfill_async(): ...

def main() -> None:
    global logger 
    clear_terminal()
    setup_logging() 
    logger = logging.getLogger(__name__) 

    logger.info("Initializing database schema...")
    initialize_database()

    if not config.TELEGRAM_BOT_TOKEN:
        logger.critical("FATAL: TELEGRAM_BOT_TOKEN missing. Bot cannot start.")
        return

    try:
        logger.info("Initializing Telegram PTB Application...")
        ptb_app = Application.builder().token(config.TELEGRAM_BOT_TOKEN).build()
        
        logger.info("Initializing EnkiBotApplication...")
        # --- MODIFIED BOT INSTANTIATION ---
        enkibot_app_instance = EnkiBotApplication(ptb_app) 
        enkibot_app_instance.register_handlers() # Call the method to register handlers
        # --- END MODIFICATION ---

        logger.info("Starting EnkiBot polling...")
        # The run method is now part of EnkiBotApplication, or keep polling here
        # For simplicity, keeping polling here:
        ptb_app.run_polling(allowed_updates=Update.ALL_TYPES) 
        # Alternatively, if you add a run() method to EnkiBotApplication:
        # enkibot_app_instance.run() 
        
        logger.info("EnkiBot has stopped.")
    except Exception as e:
        logger.critical(f"Unrecoverable error during bot setup or run: {e}", exc_info=True)

if __name__ == '__main__':
    main()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/__init__.py ---
======================================================================

# enkibot/modules/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file intentionally left blank.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/api_router.py ---
======================================================================

﻿# enkibot/modules/api_router.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot API Router ===
# ==================================================================================================

# (GPLv3 Header as in your files)
import logging
import httpx 
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List

from enkibot import config # For API Keys
# Removed LLMServices import as it's not directly used here anymore for extraction

logger = logging.getLogger(__name__)

class ApiRouter:
    def __init__(self, weather_api_key: str | None, news_api_key: str | None, llm_services: Any = None): # llm_services no longer strictly needed here
        self.weather_api_key = weather_api_key
        self.news_api_key = news_api_key
        # self.llm_services = llm_services # Not used directly in this version of ApiRouter

        self.lang_to_country_map = {
            "en": "us", "ru": "ru", "de": "de", "fr": "fr", "es": "es", 
            "it": "it", "ja": "jp", "ko": "kr", "zh": "cn", "bg": "bg",
            "ua": "ua", "pl": "pl", "tr": "tr", "pt": "pt", 
        }
        self.default_news_country = "us"

    def _get_localized_response_string_from_pack(self, lang_pack_full: Optional[Dict[str, Any]], key: str, default_value: str, **kwargs) -> str:
        """ Helper to get response strings directly from a full language pack. """
        if lang_pack_full and "responses" in lang_pack_full:
            raw_string = lang_pack_full["responses"].get(key, default_value)
        else: # Fallback if pack or responses section is missing
            raw_string = default_value
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError: return default_value


    async def get_weather_data_structured(self, location: str, lang_pack_full: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """
        Fetches 5-day weather forecast data and returns it in a structured format
        suitable for LLM processing. Includes city name and a list of daily forecasts.
        """
        if not self.weather_api_key:
            logger.warning("Weather API key is not configured.")
            return None

        api_lang = "en" # Default to English for weather data city name, LLM can localize description
        if lang_pack_full and "responses" in lang_pack_full:
            api_lang = lang_pack_full["responses"].get("api_lang_code_openweathermap", "en")

        # OpenWeatherMap 5 day / 3 hour forecast endpoint
        url = "https://api.openweathermap.org/data/2.5/forecast"
        # Requesting enough data points for 5 days (8 records per day * 5 days = 40)
        params = {"q": location, "appid": self.weather_api_key, "units": "metric", "lang": api_lang, "cnt": 40}
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
            data = response.json()

            city_name = data.get("city", {}).get("name", location)
            forecast_items = data.get("list", [])

            if not forecast_items:
                logger.warning(f"No forecast items received for {location}")
                return None

            daily_forecasts_processed = []
            temp_daily_data = {} # date_str -> {'temps': [], 'conditions': set(), 'icons': set()}

            for item in forecast_items:
                dt_object = datetime.fromtimestamp(item["dt"])
                date_str = dt_object.strftime("%Y-%m-%d")
                
                if date_str not in temp_daily_data:
                    temp_daily_data[date_str] = {'temps': [], 'conditions': set(), 'icons': set()}
                
                temp_daily_data[date_str]['temps'].append(item["main"]["temp"])
                if item.get("weather") and item["weather"][0]:
                    temp_daily_data[date_str]['conditions'].add(item["weather"][0].get("description", "N/A"))
                    temp_daily_data[date_str]['icons'].add(item["weather"][0].get("icon", "N/A"))

            for date_str, daily_data in sorted(temp_daily_data.items()):
                if not daily_data['temps']: continue
                
                # For simplicity, take the condition that appears most or first unique for the day.
                # LLM can make sense of multiple conditions if provided as a list.
                day_condition = list(daily_data['conditions'])[0] if daily_data['conditions'] else "N/A"
                
                daily_forecasts_processed.append({
                    "date": date_str,
                    "day_name": datetime.strptime(date_str, "%Y-%m-%d").strftime('%A'), # English day name
                    "temp_min": min(daily_data['temps']),
                    "temp_max": max(daily_data['temps']),
                    "avg_temp": sum(daily_data['temps']) / len(daily_data['temps']),
                    "condition_descriptions": list(daily_data['conditions']), # Provide all conditions
                    "primary_condition": day_condition # LLM can choose or summarize
                })
            
            if not daily_forecasts_processed:
                 logger.warning(f"Could not process daily forecast data for {location}")
                 return None

            return {
                "location": city_name,
                "forecast_days": daily_forecasts_processed[:7] # Return up to 7 days of processed data
            }

        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error fetching weather forecast for {location}: {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e:
            logger.error(f"Unexpected error fetching structured weather data for {location}: {e}", exc_info=True)
        return None

    async def get_latest_news_structured(self, query: Optional[str] = None, lang_code: str = "en", num: int = 5) -> Optional[List[Dict[str, Any]]]:
        """
        Fetches latest news and returns a list of article dictionaries.
        """
        if not self.news_api_key:
            logger.warning("News API key is not configured.")
            return None

        params: Dict[str, Any] = {"apiKey": self.news_api_key, "pageSize": num}
        base_url = "https://newsapi.org/v2/"
        endpoint: str

        if query:
            logger.info(f"Fetching news for query: '{query}', language: {lang_code}")
            endpoint = "everything"
            params.update({"q": query, "language": lang_code, "sortBy": "relevancy"})
        else:
            country = self.lang_to_country_map.get(lang_code, self.default_news_country)
            logger.info(f"Fetching top headlines for country: '{country}' (derived from lang: {lang_code})")
            endpoint = "top-headlines"
            params.update({"country": country, "category": "general"})
        
        url = base_url + endpoint
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(url, params=params)
                logger.debug(f"NewsAPI request URL: {resp.url}")
                resp.raise_for_status()
            data = resp.json()
            articles_raw = data.get("articles", [])
            
            logger.info(f"NewsAPI returned {len(articles_raw)} articles (totalResults: {data.get('totalResults')}) for params: {params}")

            if not articles_raw:
                return [] # Return empty list if no articles

            # Process articles into a cleaner structure for the LLM
            processed_articles = []
            for article in articles_raw:
                processed_articles.append({
                    "title": article.get("title"),
                    "source": article.get("source", {}).get("name"),
                    "description": article.get("description"), # Short description or snippet
                    "url": article.get("url"),
                    "published_at": article.get("publishedAt")
                })
            return processed_articles
            
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error fetching news ({url}): {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e:
            logger.error(f"Unexpected error fetching structured news: {e}", exc_info=True)
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/base_module.py ---
======================================================================

# enkibot/modules/base_module.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Base Module ===
# ==================================================================================================
# This file is intended to hold an Abstract Base Class (ABC) for all functional modules.
# In a future evolution, all modules (e.g., IntentRecognizer, FactExtractor) would inherit
# from this class to ensure a consistent interface, for example, requiring an `execute` method.
# For now, it serves as a structural placeholder.
# ==================================================================================================

class BaseModule:
    """
    Abstract Base Class for all EnkiBot modules.
    """
    def __init__(self, name: str):
        self.name = name

    def execute(self, *args, **kwargs):
        """
        The main method to be implemented by all subclasses.
        """
        raise NotImplementedError("Each module must implement the 'execute' method.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/fact_extractor.py ---
======================================================================

﻿# enkibot/modules/fact_extractor.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Fact Extractor ===
# ==================================================================================================
# This module uses linguistic rules and morphological analysis to extract specific
# pieces of information from user text, such as a person's name mentioned in a query.
# This is faster and cheaper than using an LLM for simple, well-defined extraction tasks.
# ==================================================================================================

import logging
import re
import pymorphy3

logger = logging.getLogger(__name__)

# Initialize the morphological analyzer for Russian.
try:
    morph = pymorphy3.MorphAnalyzer()
except Exception as e:
    logger.error(f"Could not initialize pymorphy3 MorphAnalyzer: {e}. Fact extraction might fail.")
    morph = None

def find_user_search_query_in_text(text: str) -> str | None:
    """
    Analyzes text by lemmatizing words to their base form and looks for a combination
    of trigger words and prepositions to extract a user name.

    This is a classic Natural Language Processing (NLP) technique for entity extraction.

    Args:
        text: The user's message text.

    Returns:
        The extracted name as a string, or None if no name is found.
    """
    if not morph:
        logger.warning("Morphological analyzer not available. Skipping user search query extraction.")
        return None
        
    # Dictionaries of trigger word lemmas (base forms).
    # This makes the system robust to different word forms (e.g., 'tell', 'tells', 'told').
    TELL_LEMMAS = {'рассказать', 'поведать', 'сообщить', 'описать'}
    INFO_LEMMAS = {'информация', 'инфо', 'справка', 'досье', 'данные'}
    WHO_LEMMAS = {'кто', 'что'}
    EXPLAIN_LEMMAS = {'пояснить', 'объяснить'}
    REMEMBER_LEMMAS = {'помнить', 'напомнить'}

    # Prepositions that typically follow trigger words before a name.
    PREPOSITIONS = {'о', 'про', 'за', 'на', 'по'}

    # Split the text into words
    words = re.findall(r"[\w'-]+", text.lower())
    
    for i, word in enumerate(words):
        try:
            # Get the lemma (normal form) of the word
            lemma = morph.parse(word)[0].normal_form
            
            # Check if the lemma is one of our triggers
            is_trigger = (lemma in TELL_LEMMAS or
                          lemma in INFO_LEMMAS or
                          lemma in WHO_LEMMAS or
                          lemma in EXPLAIN_LEMMAS or
                          lemma in REMEMBER_LEMMAS)

            if is_trigger:
                # We found a trigger word. The name should follow it.
                start_index = i + 1
                
                # If the next word is a preposition, skip it.
                if start_index < len(words) and words[start_index] in PREPOSITIONS:
                    start_index += 1
                
                # Everything that follows (up to 3 words) is considered the name.
                if start_index < len(words):
                    # Capture 1 to 3 words after the trigger/preposition.
                    name_parts = words[start_index : start_index + 3]
                    extracted_name = " ".join(name_parts)
                    logger.info(f"Extracted potential user search query: '{extracted_name}'")
                    return extracted_name

        except Exception as e:
            logger.error(f"Error during lemmatization of word '{word}': {e}")
            continue
            
    return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/intent_recognizer.py ---
======================================================================

# enkibot/modules/intent_recognizer.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>


import logging
import json
import re 
from typing import Dict, Any, Optional, TYPE_CHECKING

from enkibot import config # For model IDs

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices 

logger = logging.getLogger(__name__)

class IntentRecognizer:
    def __init__(self, llm_services: 'LLMServices'): 
        logger.info("IntentRecognizer __init__ STARTING")
        self.llm_services = llm_services
        logger.info("IntentRecognizer __init__ COMPLETED")

    async def classify_master_intent(self, text: str, lang_code: str, 
                                     system_prompt: str, user_prompt_template: str) -> str:
        logger.info(f"Classifying master intent (lang: {lang_code}): '{text[:100]}...'")
        user_prompt = user_prompt_template.format(text_to_classify=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        # Check if the classification_model_id is one known to support JSON object mode
        if classification_model_id and \
           any(model_prefix in classification_model_id for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}

        classified_intent_value = "UNKNOWN_INTENT" 
        completion_str_for_log = "N/A"

        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=classification_model_id, 
                temperature=0.0, 
                max_tokens=100,
                **response_format_arg 
            )
            completion_str_for_log = completion_str if completion_str is not None else "None"

            if completion_str:
                try:
                    clean_comp_str = completion_str.strip()
                    # More robustly remove markdown json block using re
                    match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                    if match:
                        clean_comp_str = match.group(1).strip()
                    elif clean_comp_str.startswith("```"): # Handle simple ``` case
                        clean_comp_str = clean_comp_str.strip("` \t\n\r")
                        if clean_comp_str.lower().startswith("json"): # check if 'json' follows ```
                            clean_comp_str = clean_comp_str[4:].strip() # remove 'json' and strip
                    
                    data = json.loads(clean_comp_str)
                    intent_from_json = data.get("intent", data.get("INTENT")) 

                    if intent_from_json and isinstance(intent_from_json, str):
                        processed_intent = intent_from_json.strip().strip('_').upper().replace(" ", "_")
                        known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]
                        if processed_intent in known_categories:
                            classified_intent_value = processed_intent
                            logger.info(f"Master intent classified as: {classified_intent_value} via JSON.")
                        else:
                            logger.warning(f"LLM JSON with unknown category '{processed_intent}'. Raw value: '{intent_from_json}'. Full data: {data}. Defaulting UNKNOWN.")
                    else:
                        logger.warning(f"LLM JSON for master intent missing 'intent' key or not string. Data: {data}. Defaulting UNKNOWN.")
                except json.JSONDecodeError:
                    logger.warning(f"Failed to decode JSON from master_intent_classifier. LLM raw: '{completion_str_for_log}'. Attempting direct parse.")
                    raw_intent = completion_str_for_log.strip().strip('_').upper().replace(" ", "_")
                    if raw_intent.startswith('{"INTENT":') and raw_intent.endswith('"}'):
                        try:
                            raw_intent_data = json.loads(raw_intent) # Try to parse this specific format
                            raw_intent = raw_intent_data.get("INTENT", raw_intent).strip().strip('_').upper().replace(" ", "_")
                        except: pass 
                    
                    known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                    if raw_intent in known_categories:
                        classified_intent_value = raw_intent
                        logger.info(f"Master intent classified as: {classified_intent_value} via direct string parse fallback.")
                    else:
                         logger.warning(f"Direct string parse fallback failed for intent: '{raw_intent}'. Defaulting UNKNOWN.")
            else:
                logger.warning("Master intent classification LLM call returned no content.")
        except Exception as e: 
            logger.error(f"Error during master intent classification LLM call: {e}", exc_info=True)
        
        return classified_intent_value

    async def analyze_weather_request_with_llm(self, text: str, lang_code: str, 
                                               system_prompt: str, user_prompt_template: Optional[str]) -> Dict[str, Any]:
        logger.info(f"Analyzing weather request type (lang: {lang_code}): '{text}' with LLM.")
        user_prompt = (user_prompt_template or "{text}").format(text=text) 
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        if model_to_use and any(model_prefix in model_to_use for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}
        
        default_response = {"type": "current"} 
        completion_str_for_error_log = "N/A"
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=model_to_use,
                temperature=0, 
                **response_format_arg
            )
            completion_str_for_error_log = completion_str if completion_str is not None else "None"
            if completion_str:
                logger.info(f"LLM response for weather analysis: {completion_str}")
                clean_comp_str = completion_str.strip()
                match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                if match: clean_comp_str = match.group(1).strip()
                else:
                    if clean_comp_str.startswith("```json"): clean_comp_str = clean_comp_str[7:]
                    if clean_comp_str.endswith("```"): clean_comp_str = clean_comp_str[:-3]                
                return json.loads(clean_comp_str.strip())
            else:
                logger.warning("LLM returned no content for weather analysis.")
        except json.JSONDecodeError as e:
            logger.error(f"Error decoding JSON from LLM for weather analysis: '{completion_str_for_error_log}'. Error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error in LLM call during weather request analysis: {e}", exc_info=True)
        
        return default_response

    async def extract_location_with_llm(self, text: str, lang_code: str, 
                                        system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM location extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"):
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
            
            if not location and self.llm_services.is_provider_configured("groq"): 
                logger.info("OpenAI location extraction failed or not configured, trying Groq.")
                completion = await self.llm_services.call_llm_api(
                    "Groq", self.llm_services.groq_api_key, self.llm_services.groq_endpoint_url, 
                    self.llm_services.groq_model_id, messages_for_api,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM location extraction: {e}", exc_info=True)

        if location and location.lower() != 'none' and location.strip() != "":
            logger.info(f"LLM successfully extracted location: '{location}'")
            return location
        logger.warning(f"LLM couldn't extract location from: '{text}'.")
        return None

    async def extract_news_topic_with_llm(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM news topic extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        topic = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: topic = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM news topic extraction: {e}", exc_info=True)

        if topic and topic.lower() != 'none' and topic.strip() != "":
            logger.info(f"LLM successfully extracted news topic: '{topic}'")
            return topic
        logger.info(f"LLM found no specific news topic in '{text}'.")
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/profile_manager.py ---
======================================================================

# enkibot/modules/profile_manager.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Profile Manager ===
# ==================================================================================================
# Manages user psychological profiles and name variations using LLMs and database interaction.
# ==================================================================================================
import logging
import json
from typing import Optional, Dict, TYPE_CHECKING

from enkibot.utils.database import DatabaseManager 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices # For type hinting

logger = logging.getLogger(__name__)

class ProfileManager:
    def __init__(self, llm_services: 'LLMServices', db_manager: DatabaseManager):
        self.llm_services = llm_services # Type hint is string literal
        self.db_manager = db_manager
        self.MAX_PROFILE_SIZE = 4000

    async def populate_name_variations_with_llm(self, user_id: int, first_name: Optional[str], 
                                                last_name: Optional[str], username: Optional[str],
                                                system_prompt: str, user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"): # Check specific provider
            logger.warning(f"Name variation for user {user_id} skipped: OpenAI not configured in LLMServices.")
            return

        name_parts = [part for part in [first_name, last_name, username] if part and str(part).strip()]
        if not name_parts:
            logger.info(f"No valid name parts for user {user_id}, skipping name variation.")
            return
            
        name_info = ", ".join(name_parts)
        logger.info(f"Requesting name variations for user {user_id} ({name_info}).")
        user_prompt = user_prompt_template.format(name_info=name_info)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        name_variations = set([p.lower() for p in name_parts])
        
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages, temperature=0.3, response_format={"type": "json_object"}
            ) # Removed model_id, should be handled by call_openai_llm default
            if completion_str:
                data = json.loads(completion_str)
                variations_list = data.get('variations')
                if isinstance(variations_list, list):
                    name_variations.update([str(v).lower().strip() for v in variations_list if v and str(v).strip()])
                    logger.info(f"LLM got {len(variations_list)} raw variations. Total unique: {len(name_variations)} for user {user_id}.")
                else:
                    logger.warning(f"LLM name variations for {user_id} no 'variations' list. Resp: {completion_str[:200]}")
            else:
                logger.warning(f"LLM no content for name variations for user {user_id}.")
        except Exception as e:
            logger.error(f"LLM name variation error (user {user_id}): {e}", exc_info=True)

        if name_variations:
            await self.db_manager.save_user_name_variations(user_id, list(name_variations))
        else:
            logger.info(f"No name variations to save for user {user_id}.")

    async def analyze_and_update_user_profile(self, user_id: int, message_text: str,
                                              create_system_prompt: str, create_user_prompt_template: str,
                                              update_system_prompt: str, update_user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"):
            logger.warning(f"Profiling for {user_id} skipped: OpenAI not configured.")
            return
        logger.info(f"Starting/Updating profile analysis for user {user_id}...")
        current_notes = await self.db_manager.get_user_profile_notes(user_id)
        sys_prompt, user_prompt = "", ""

        if not current_notes:
            logger.info(f"No profile for {user_id}. Creating new.")
            sys_prompt, user_prompt = create_system_prompt, create_user_prompt_template.format(message_text=message_text)
        else:
            logger.info(f"Existing profile for {user_id}. Updating.")
            sys_prompt, user_prompt = update_system_prompt, update_user_prompt_template.format(
                current_profile_notes=current_notes, message_text=message_text)
        
        messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}]
        updated_notes_str: Optional[str] = None
        try:
            updated_notes_str = await self.llm_services.call_openai_llm(messages, temperature=0.5, max_tokens=1000)
        except Exception as e:
            logger.error(f"LLM profile analysis error for {user_id}: {e}", exc_info=True)

        if updated_notes_str and updated_notes_str.strip():
            await self.db_manager.update_user_profile_notes(user_id, updated_notes_str.strip()[:self.MAX_PROFILE_SIZE])
            logger.info(f"Profile updated for {user_id}.")
        else:
            logger.warning(f"Profile analysis no content for {user_id}.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/response_generator.py ---
======================================================================

# enkibot/modules/response_generator.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Response Generator ===
# ==================================================================================================
# enkibot/modules/intent_recognizer.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Your GPLv3 Header)

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
# print(f"%%%%% EXECUTING INTENT_RECOGNIZER.PY - VERSION FROM: {__file__} %%%%%") # You can uncomment this if you still face import issues

import logging
import json
import re # <--- CORRECT: 're' module is now imported
from typing import Dict, Any, Optional, TYPE_CHECKING

from enkibot import config # For model IDs

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices 

logger = logging.getLogger(__name__)

class IntentRecognizer:
    # <<<--- DIAGNOSTIC PRINT IR-2: INSIDE CLASS DEFINITION --- >>>
    # print("%%%%% IntentRecognizer CLASS DEFINITION BEING PARSED %%%%%") # Keep if needed

    def __init__(self, llm_services: 'LLMServices'): 
        # <<<--- DIAGNOSTIC PRINT IR-3: START OF IntentRecognizer __INIT__ --- >>>
        # print("%%%%% IntentRecognizer __init__ STARTING %%%%%") # Keep if needed
        logger.info("IntentRecognizer __init__ STARTING")
        self.llm_services = llm_services
        
        # <<< --- DIAGNOSTIC PRINT IR-4: END OF IntentRecognizer __INIT__ --- >>>
        # (These hasattr checks are good for ensuring methods are defined)
        if hasattr(self, 'analyze_weather_request_with_llm') and callable(getattr(self, 'analyze_weather_request_with_llm')):
            logger.info("DIAGNOSTIC (IntentRecognizer.__init__): self.analyze_weather_request_with_llm IS a callable attribute.")
        else:
            logger.error("DIAGNOSTIC (IntentRecognizer.__init__): self.analyze_weather_request_with_llm IS MISSING or NOT CALLABLE!")
        if hasattr(self, 'classify_master_intent') and callable(getattr(self, 'classify_master_intent')):
            logger.info("DIAGNOSTIC (IntentRecognizer.__init__): self.classify_master_intent IS a callable attribute.")
        else:
            logger.error("DIAGNOSTIC (IntentRecognizer.__init__): self.classify_master_intent IS MISSING or NOT CALLABLE!")
        # print("%%%%% IntentRecognizer __init__ COMPLETED %%%%%") # Keep if needed
        logger.info("IntentRecognizer __init__ COMPLETED")


    async def classify_master_intent(self, text: str, lang_code: str, 
                                     system_prompt: str, user_prompt_template: str) -> str:
        logger.info(f"Classifying master intent (lang: {lang_code}): '{text[:100]}...'")
        user_prompt = user_prompt_template.format(text_to_classify=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        # Use the specific classification model ID from config
        classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        if classification_model_id and \
           any(model_prefix in classification_model_id for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}

        classified_intent_value = "UNKNOWN_INTENT" 
        completion_str_for_log = "N/A" # Initialize for error logging

        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=classification_model_id, 
                temperature=0.0, 
                max_tokens=100, 
                **response_format_arg 
            )
            completion_str_for_log = completion_str if completion_str is not None else "None"

            if completion_str:
                try:
                    clean_comp_str = completion_str.strip()
                    # More robustly remove markdown json block using re
                    match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                    if match:
                        clean_comp_str = match.group(1).strip()
                    # Fallback for simple prefix/suffix if regex fails or not present
                    # also handle if LLM just wraps in ``` without json identifier
                    elif clean_comp_str.startswith("```"): 
                        clean_comp_str = clean_comp_str.strip("` \t\n\r") # General stripping of backticks and whitespace
                        if clean_comp_str.lower().startswith("json"): # check if 'json' follows ```
                            clean_comp_str = clean_comp_str[4:].strip() # remove 'json' and strip
                    
                    data = json.loads(clean_comp_str) # Attempt to parse what's left
                    intent_from_json = data.get("intent", data.get("INTENT")) 

                    if intent_from_json and isinstance(intent_from_json, str):
                        processed_intent = intent_from_json.strip().strip('_').upper().replace(" ", "_")
                        known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]
                        if processed_intent in known_categories:
                            classified_intent_value = processed_intent
                            logger.info(f"Master intent classified as: {classified_intent_value} via JSON.")
                        else:
                            logger.warning(f"LLM JSON with unknown category '{processed_intent}'. Raw value: '{intent_from_json}'. Full data: {data}. Defaulting UNKNOWN.")
                    else:
                        logger.warning(f"LLM JSON for master intent missing 'intent' key or not string. Data: {data}. Defaulting UNKNOWN.")
                except json.JSONDecodeError:
                    logger.warning(f"Failed to decode JSON from master_intent_classifier. LLM raw: '{completion_str_for_log}'. Attempting direct parse.")
                    raw_intent = completion_str_for_log.strip().strip('_').upper().replace(" ", "_")
                    # Attempt to extract from simple {"INTENT":"VALUE"} like structures if direct string is still JSONish
                    if raw_intent.startswith('{') and raw_intent.endswith('}'):
                        try:
                            temp_data = json.loads(raw_intent)
                            extracted_val = temp_data.get("intent", temp_data.get("INTENT", raw_intent))
                            raw_intent = str(extracted_val).strip().strip('_').upper().replace(" ", "_")
                        except json.JSONDecodeError: # It wasn't actually valid JSON
                            pass # Keep raw_intent as is

                    known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                    if raw_intent in known_categories:
                        classified_intent_value = raw_intent
                        logger.info(f"Master intent classified as: {classified_intent_value} via direct string parse fallback.")
                    else:
                         logger.warning(f"Direct string parse fallback failed for intent: '{raw_intent}'. Defaulting UNKNOWN.")
            else:
                logger.warning("Master intent classification LLM call returned no content.")
        except Exception as e: 
            logger.error(f"Error during master intent classification LLM call: {e}", exc_info=True)
        
        return classified_intent_value

    async def analyze_weather_request_with_llm(self, text: str, lang_code: str, 
                                               system_prompt: str, user_prompt_template: Optional[str]) -> Dict[str, Any]:
        # print(f"%%%%% IntentRecognizer.analyze_weather_request_with_llm CALLED with text: {text[:30]}... %%%%%") # Diagnostic
        logger.info(f"Analyzing weather request type (lang: {lang_code}): '{text}' with LLM.")
        user_prompt = (user_prompt_template or "{text}").format(text=text) 
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        if model_to_use and any(model_prefix in model_to_use for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}
        
        default_response = {"type": "current"} 
        completion_str_for_error_log = "N/A"
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=model_to_use,
                temperature=0, 
                **response_format_arg
            )
            completion_str_for_error_log = completion_str if completion_str is not None else "None"
            if completion_str:
                logger.info(f"LLM response for weather analysis: {completion_str}")
                clean_comp_str = completion_str.strip()
                match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                if match: clean_comp_str = match.group(1).strip()
                elif clean_comp_str.startswith("```"): # Handle simple ``` case
                    clean_comp_str = clean_comp_str.strip("` \t\n\r") 
                    if clean_comp_str.lower().startswith("json"): 
                        clean_comp_str = clean_comp_str[4:].strip()              
                return json.loads(clean_comp_str.strip())
            else:
                logger.warning("LLM returned no content for weather analysis.")
        except json.JSONDecodeError as e:
            logger.error(f"Error decoding JSON from LLM for weather analysis: '{completion_str_for_error_log}'. Error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error in LLM call during weather request analysis: {e}", exc_info=True)
        
        return default_response

    async def extract_location_with_llm(self, text: str, lang_code: str, 
                                        system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM location extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"):
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
            
            if not location and self.llm_services.is_provider_configured("groq"): 
                logger.info("OpenAI location extraction failed or not configured, trying Groq.")
                completion = await self.llm_services.call_llm_api(
                    "Groq", self.llm_services.groq_api_key, self.llm_services.groq_endpoint_url, 
                    self.llm_services.groq_model_id, messages_for_api,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM location extraction: {e}", exc_info=True)

        if location and location.lower() != 'none' and location.strip() != "":
            logger.info(f"LLM successfully extracted location: '{location}'")
            return location
        logger.warning(f"LLM couldn't extract location from: '{text}'.")
        return None

    async def extract_news_topic_with_llm(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM news topic extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        topic = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: topic = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM news topic extraction: {e}", exc_info=True)

        if topic and topic.lower() != 'none' and topic.strip() != "":
            logger.info(f"LLM successfully extracted news topic: '{topic}'")
            return topic
        logger.info(f"LLM found no specific news topic in '{text}'.")
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/__init__.py ---
======================================================================

# enkibot/utils/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file makes the 'utils' directory a Python package.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/database.py ---
======================================================================

# enkibot/utils/database.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by

# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Ensure your GPLv3 header is here)

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Your GPLv3 Header)
# ==================================================================================================
# === EnkiBot Database Utilities ===
# ==================================================================================================
import logging
import pyodbc
from typing import List, Dict, Any, Optional

from enkibot import config # For DB_CONNECTION_STRING

logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self, connection_string: Optional[str]):
        self.connection_string = connection_string
        if not self.connection_string:
            logger.warning("Database connection string is not configured. Database operations will be disabled.")

    def get_db_connection(self) -> Optional[pyodbc.Connection]:
        if not self.connection_string: return None
        try:
            conn = pyodbc.connect(self.connection_string, autocommit=False)
            logger.debug("Database connection established.")
            return conn
        except pyodbc.Error as ex:
            logger.error(f"Database connection error: {ex.args[0] if ex.args else ''} - {ex}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"Unexpected error establishing database connection: {e}", exc_info=True)
            return None

    async def execute_query(self, query: str, params: Optional[tuple] = None, fetch_one: bool = False, fetch_all: bool = False, commit: bool = False):
        if not self.connection_string:
            logger.warning("Query execution skipped: Database not configured.")
            return None if fetch_one or fetch_all else (True if commit else False)
        conn = self.get_db_connection()
        if not conn:
            return None if fetch_one or fetch_all else (True if commit else False)
        try:
            with conn.cursor() as cursor:
                logger.debug(f"Executing query: {query[:150]}... with params: {params}")
                cursor.execute(query, params) if params else cursor.execute(query)
                if commit: conn.commit(); logger.debug("Query committed."); return True
                if fetch_one: row = cursor.fetchone(); logger.debug(f"Query fetch_one: {row}"); return row
                if fetch_all: rows = cursor.fetchall(); logger.debug(f"Query fetch_all count: {len(rows)}"); return rows
            return True
        except pyodbc.Error as ex:
            logger.error(f"DB query error on '{query[:100]}...': {ex}", exc_info=True)
            try: conn.rollback(); logger.info("Transaction rolled back.")
            except pyodbc.Error as rb_ex: logger.error(f"Error during rollback: {rb_ex}", exc_info=True)
            return None if fetch_one or fetch_all else False
        except Exception as e:
            logger.error(f"Unexpected error query execution '{query[:100]}...': {e}", exc_info=True)
            return None if fetch_one or fetch_all else False
        finally:
            if conn: conn.close(); logger.debug("DB connection closed post-exec.")

    async def get_recent_chat_texts(self, chat_id: int, limit: int = 3) -> List[str]:
        if not self.connection_string: return []
        query = """
            SELECT TOP (?) MessageText FROM ChatLog
            WHERE ChatID = ? AND MessageText IS NOT NULL AND RTRIM(LTRIM(MessageText)) != ''
            ORDER BY Timestamp DESC
        """
        actual_limit = max(1, limit) 
        rows = await self.execute_query(query, (actual_limit, chat_id), fetch_all=True)
        return [row.MessageText for row in reversed(rows) if row.MessageText] if rows else []

    async def log_chat_message_and_upsert_user(
        self, chat_id: int, user_id: int, username: Optional[str],
        first_name: Optional[str], last_name: Optional[str],
        message_id: int, message_text: str, preferred_language: Optional[str] = None
    ) -> Optional[str]:
        if not self.connection_string: return None
        upsert_user_sql = """
            MERGE UserProfiles AS t
            USING (VALUES(?,?,?,?,GETDATE(),?)) AS s(UserID,Username,FirstName,LastName,LastSeen,PreferredLanguage)
            ON t.UserID = s.UserID
            WHEN MATCHED THEN
                UPDATE SET Username=s.Username, FirstName=s.FirstName, LastName=s.LastName, LastSeen=s.LastSeen, MessageCount=ISNULL(t.MessageCount,0)+1, PreferredLanguage=ISNULL(s.PreferredLanguage, t.PreferredLanguage)
            WHEN NOT MATCHED THEN
                INSERT(UserID,Username,FirstName,LastName,LastSeen,MessageCount,ProfileLastUpdated, PreferredLanguage)
                VALUES(s.UserID,s.Username,s.FirstName,s.LastName,s.LastSeen,1,GETDATE(),s.PreferredLanguage)
            OUTPUT $action AS Action;
        """
        user_params = (user_id, username, first_name, last_name, preferred_language)
        chat_log_sql = "INSERT INTO ChatLog (ChatID, UserID, Username, FirstName, MessageID, MessageText, Timestamp) VALUES (?, ?, ?, ?, ?, ?, GETDATE())"
        chat_log_params = (chat_id, user_id, username, first_name, message_id, message_text)
        conn = self.get_db_connection()
        if not conn: return None
        action_taken = None
        try:
            with conn.cursor() as cursor:
                cursor.execute(upsert_user_sql, user_params)
                row = cursor.fetchone();
                if row: action_taken = row.Action
                cursor.execute(chat_log_sql, chat_log_params)
            conn.commit()
            logger.info(f"Logged message for user {user_id}. Profile action: {action_taken}")
            return action_taken
        except pyodbc.Error as ex:
            logger.error(f"DB error logging/upserting user {user_id}: {ex}", exc_info=True); conn.rollback()
        finally:
            if conn: conn.close()
        return None # Ensure a return path if try fails before commit

    async def get_conversation_history(self, chat_id: int, limit: int = 20) -> List[Dict[str, str]]:
        query = "SELECT TOP (?) Role, Content FROM ConversationHistory WHERE ChatID = ? ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, chat_id), fetch_all=True)
        return [{"role": row.Role.lower(), "content": row.Content} for row in reversed(rows)] if rows else []

    async def save_to_conversation_history(self, chat_id: int, entity_id: int, message_id_telegram: Optional[int], role: str, content: str):
        query = "INSERT INTO ConversationHistory (ChatID, UserID, MessageID, Role, Content, Timestamp) VALUES (?, ?, ?, ?, ?, GETDATE())"
        await self.execute_query(query, (chat_id, entity_id, message_id_telegram, role, content), commit=True)

    async def get_user_profile_notes(self, user_id: int) -> Optional[str]:
        row = await self.execute_query("SELECT Notes FROM UserProfiles WHERE UserID = ?", (user_id,), fetch_one=True)
        return row.Notes if row and row.Notes else None

    async def update_user_profile_notes(self, user_id: int, notes: str):
        await self.execute_query("UPDATE UserProfiles SET Notes = ?, ProfileLastUpdated = GETDATE() WHERE UserID = ?", (notes, user_id), commit=True)
        logger.info(f"Profile notes updated for user {user_id}.")

    async def save_user_name_variations(self, user_id: int, variations: List[str]):
        if not self.connection_string or not variations: return
        sql_merge = """
            MERGE INTO UserNameVariations AS t USING (SELECT ? AS UserID, ? AS NameVariation) AS s
            ON (t.UserID = s.UserID AND t.NameVariation = s.NameVariation)
            WHEN NOT MATCHED THEN INSERT (UserID, NameVariation) VALUES (s.UserID, s.NameVariation);
        """
        conn = self.get_db_connection()
        if not conn: return
        try:
            with conn.cursor() as cursor:
                params_to_insert = [(user_id, var) for var in variations if var and str(var).strip()]
                if params_to_insert: cursor.executemany(sql_merge, params_to_insert)
            conn.commit()
            logger.info(f"Saved/updated {len(params_to_insert)} name vars for user {user_id}.")
        except pyodbc.Error as ex:
            logger.error(f"DB error saving name vars for user {user_id}: {ex}", exc_info=True); conn.rollback()
        finally:
            if conn: conn.close()
            
    async def find_user_profiles_by_name_variation(self, name_variation_query: str) -> List[Dict[str, Any]]:
        query = """
            SELECT DISTINCT up.UserID, up.FirstName, up.LastName, up.Username, up.Notes
            FROM UserProfiles up JOIN UserNameVariations unv ON up.UserID = unv.UserID
            WHERE unv.NameVariation = ?
        """
        rows = await self.execute_query(query, (name_variation_query.lower(),), fetch_all=True)
        return [{"UserID": r.UserID, "FirstName": r.FirstName, "LastName": r.LastName, "Username": r.Username, "Notes": r.Notes} for r in rows] if rows else []

    async def get_user_messages_from_chat_log(self, user_id: int, chat_id: int, limit: int = 10) -> List[str]: # Kept from previous version
        query = "SELECT TOP (?) MessageText FROM ChatLog WHERE UserID = ? AND ChatID = ? AND MessageText IS NOT NULL AND RTRIM(LTRIM(MessageText)) != '' ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, user_id, chat_id), fetch_all=True)
        return [row.MessageText for row in rows] if rows else []

def initialize_database(): # This function defines and uses DatabaseManager locally
    if not config.DB_CONNECTION_STRING:
        logger.warning("Cannot initialize database: Connection string not configured.")
        return
    
    db_mngr = DatabaseManager(config.DB_CONNECTION_STRING) 
    conn = db_mngr.get_db_connection() 
    if not conn: 
        logger.error("Failed to connect to database for initialization (initialize_database).")
        return
    
    conn.autocommit = True 
    table_queries = {
        "UserProfiles": "CREATE TABLE UserProfiles (UserID BIGINT PRIMARY KEY, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, LastName NVARCHAR(255) NULL, LastSeen DATETIME2 DEFAULT GETDATE(), MessageCount INT DEFAULT 0, PreferredLanguage NVARCHAR(10) NULL, Notes NVARCHAR(MAX) NULL, ProfileLastUpdated DATETIME2 DEFAULT GETDATE());",
        "UserNameVariations": "CREATE TABLE UserNameVariations (VariationID INT IDENTITY(1,1) PRIMARY KEY, UserID BIGINT NOT NULL, NameVariation NVARCHAR(255) NOT NULL, FOREIGN KEY (UserID) REFERENCES UserProfiles(UserID) ON DELETE CASCADE);",
        "IX_UserNameVariations_UserID_NameVariation": "CREATE UNIQUE INDEX IX_UserNameVariations_UserID_NameVariation ON UserNameVariations (UserID, NameVariation);",
        "ConversationHistory": "CREATE TABLE ConversationHistory (MessageDBID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageID BIGINT NULL, Role NVARCHAR(50) NOT NULL, Content NVARCHAR(MAX) NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ConversationHistory_ChatID_Timestamp": "CREATE INDEX IX_ConversationHistory_ChatID_Timestamp ON ConversationHistory (ChatID, Timestamp DESC);",
        "ChatLog": "CREATE TABLE ChatLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, MessageID BIGINT NOT NULL, MessageText NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ChatLog_ChatID_Timestamp": "CREATE INDEX IX_ChatLog_ChatID_Timestamp ON ChatLog (ChatID, Timestamp DESC);",
        "IX_ChatLog_UserID": "CREATE INDEX IX_ChatLog_UserID ON ChatLog (UserID);",
        "ErrorLog": "CREATE TABLE ErrorLog (ErrorID INT IDENTITY(1,1) PRIMARY KEY, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, LogLevel NVARCHAR(50) NOT NULL, LoggerName NVARCHAR(255) NULL, ModuleName NVARCHAR(255) NULL, FunctionName NVARCHAR(255) NULL, LineNumber INT NULL, ErrorMessage NVARCHAR(MAX) NOT NULL, ExceptionInfo NVARCHAR(MAX) NULL);",
        "IX_ErrorLog_Timestamp": "CREATE INDEX IX_ErrorLog_Timestamp ON ErrorLog (Timestamp DESC);",
    }
    try:
        with conn.cursor() as cursor:
            logger.info("Initializing database tables...")
            for name, query in table_queries.items():
                is_idx = name.startswith("IX_")
                obj_type = "INDEX" if is_idx else "TABLE"
                obj_name_to_check = name # For tables, this is the table name. For indexes, this is the index name.
                table_for_index = ""
                if is_idx:
                    # Attempt to parse table name from index name, e.g., IX_TableName_Column -> TableName
                    parts = name.split('_')
                    if len(parts) > 1: table_for_index = parts[1] # This is a heuristic
                    else: logger.warning(f"Could not determine table for index {name}"); continue 
                
                check_q = "SELECT name FROM sys.indexes WHERE name = ? AND object_id = OBJECT_ID(?)" if is_idx else "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = ?"
                params_check = (obj_name_to_check, table_for_index) if is_idx else (obj_name_to_check,)
                
                cursor.execute(check_q, params_check)
                if cursor.fetchone(): logger.info(f"{obj_type} '{obj_name_to_check}' already exists.")
                else: logger.info(f"{obj_type} '{obj_name_to_check}' not found. Creating..."); cursor.execute(query); logger.info(f"{obj_type} '{obj_name_to_check}' created.")
            logger.info("Database initialization check complete.")
    except Exception as e: logger.error(f"DB init error: {e}", exc_info=True)
    finally:
        if conn: conn.autocommit = False; conn.close()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/logging_config.py ---
======================================================================

# enkibot/utils/logging_config.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Logging Configuration ===
# ==================================================================================================
# Sets up application-wide structured logging to both file and console.
# Includes a custom logging handler to write critical errors to the SQL database
# and clears the previous log file on startup.
# ==================================================================================================

import logging
import traceback
import pyodbc
import os # <--- IMPORT OS MODULE
from enkibot import config

def setup_logging():
    """Initializes the logging configuration for the entire application."""

    # --- START: CLEAR PREVIOUS LOG FILE ---
    log_file_name = "bot_activity.log"
    try:
        if os.path.exists(log_file_name):
            os.remove(log_file_name)
            # This print goes to console before logging is fully set up for the bot's logger.
            print(f"INFO: Previous log file '{log_file_name}' removed successfully.")
    except OSError as e:
        # This print also goes to console.
        print(f"WARNING: Error removing previous log file '{log_file_name}': {e}")
    # --- END: CLEAR PREVIOUS LOG FILE ---
    
    # Define a custom handler for logging errors to the database
    class SQLDBLogHandler(logging.Handler):
        """
        A logging handler that writes log records with level ERROR or higher
        to a dedicated table in the SQL Server database.
        """
        def __init__(self):
            super().__init__()
            self.conn = None

        def _get_db_conn_for_logging(self):
            """Establishes a database connection specifically for logging."""
            if not config.DB_CONNECTION_STRING:
                return None
            try:
                # Use autocommit=True for logging to ensure errors are written immediately.
                return pyodbc.connect(config.DB_CONNECTION_STRING, autocommit=True)
            except pyodbc.Error:
                # If the DB is down, we can't log to it. Silently fail for now.
                # A print statement could be added here for immediate feedback if needed.
                # print("WARNING: SQLDBLogHandler could not connect to the database for logging.")
                return None

        def emit(self, record: logging.LogRecord):
            """
            Writes the log record to the ErrorLog table in the database.
            """
            if self.conn is None:
                self.conn = self._get_db_conn_for_logging()

            if self.conn:
                try:
                    msg = self.format(record)
                    exc_info_str = traceback.format_exc() if record.exc_info else None
                    sql = "INSERT INTO ErrorLog (LogLevel, LoggerName, ModuleName, FunctionName, LineNumber, ErrorMessage, ExceptionInfo) VALUES (?, ?, ?, ?, ?, ?, ?)"
                    with self.conn.cursor() as cursor:
                        cursor.execute(sql, record.levelname, record.name, record.module, record.funcName, record.lineno, msg, exc_info_str)
                except pyodbc.Error:
                    # If an error occurs during logging, handle it and sever the connection.
                    self.handleError(record)
                    self.conn = None # Reset connection to be re-established on next emit.
        
        def close(self):
            """Closes the database connection if it's open."""
            if self.conn:
                try:
                    self.conn.close()
                except pyodbc.Error:
                    pass
            super().close()

    # --- Main Logging Configuration ---
    log_level = logging.INFO
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s'
    
    # Configure root logger
    # By using basicConfig with force=True (Python 3.8+), we can reconfigure if needed.
    # For older Python, ensure basicConfig is called only once application-wide.
    # Given setup_logging() is called once from main.py, this should be fine.
    logging.basicConfig(
        format=log_format,
        level=log_level,
        handlers=[
            logging.FileHandler(log_file_name, encoding='utf-8'), # Use variable
            logging.StreamHandler()
        ]
        # force=True # Add this if using Python 3.8+ and re-running setup_logging,
                   # but it should not be necessary with current structure.
    )
    
    module_logger = logging.getLogger(__name__) # Logger for this specific module (logging_config.py)

    # Add the custom DB handler if the database is configured
    if config.DB_CONNECTION_STRING:
        db_log_handler = SQLDBLogHandler()
        db_log_handler.setLevel(logging.ERROR) # Only log ERROR and CRITICAL to DB
        formatter = logging.Formatter(log_format)
        db_log_handler.setFormatter(formatter)
        logging.getLogger().addHandler(db_log_handler) # Add to the root logger
        module_logger.info("Configured logging of ERROR-level messages to the SQL database.")
    else:
        module_logger.warning("Logging to SQL database is NOT configured (DB_CONNECTION_STRING is missing).")


