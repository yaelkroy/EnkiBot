======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/EnkiBot V1.py ---
======================================================================

﻿# ==================================================================================================
# === EnkiBot - Умный ассистент для Telegram с долгосрочной памятью и AI-функциями ===
# ==================================================================================================
#
# Ключевые особенности:
# - Оркестрация нескольких LLM: Одновременные запросы к разным моделям (OpenAI, Groq и др.)
#   с выбором самого быстрого ответа для скорости и надежности.
# - Долгосрочная память: Интеграция с базой данных MS SQL Server для хранения истории чатов,
#   профилей пользователей и логов.
# - Контекстный поиск по памяти: Возможность спрашивать о прошлых событиях или людях в чате.
#   Бот динамически находит релевантную информацию в логах.
# - Динамическое распознавание имен: Понимает имена пользователей (включая транслит) и уточняет,
#   если запрос неоднозначен.
# - Автоматическое профилирование пользователей: Бот анализирует сообщения пользователя для
#   создания и обновления профиля его интересов.
# - Встроенные функции: Получение новостей и актуального прогноза погоды.
#
# ==================================================================================================

import logging
import json
import os
import openai # Для OpenAI API
import pyodbc # Для MS SQL Server
import requests # Для синхронных запросов к News API
import httpx # Для асинхронных HTTP-запросов к API
import traceback # Для детального логирования исключений
import re # Для регулярных выражений
from telegram import Update, ForceReply
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.constants import ChatAction
import asyncio
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
import pymorphy3 # Для морфологии русского языка
from datetime import datetime # Для работы с датой и временем
from transliterate import translit
from transliterate.exceptions import LanguagePackNotFound # ИСПРАВЛЕННЫЙ ИМПОРТ

# --- Инициализация библиотек ---
DetectorFactory.seed = 0
morph = pymorphy3.MorphAnalyzer()

# --- Конфигурация: Загрузка ключей и настроек из переменных окружения ---
# Важно: Никогда не храните ключи API прямо в коде.
TELEGRAM_BOT_TOKEN = os.getenv('ENKI_BOT_TOKEN')
NEWS_API_KEY = os.getenv('ENKI_BOT_NEWS_API_KEY')
WEATHER_API_KEY = os.getenv('ENKI_BOT_WEATHER_API_KEY') # Ключ для OpenWeatherMap

# Настройки подключения к базе данных
SQL_SERVER_NAME = os.getenv('ENKI_BOT_SQL_SERVER_NAME')
SQL_DATABASE_NAME = os.getenv('ENKI_BOT_SQL_DATABASE_NAME')

# Ключи и модели для различных LLM-провайдеров
OPENAI_API_KEY = os.getenv('ENKI_BOT_OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MODEL_ID', 'gpt-4o-mini')
GROQ_API_KEY = os.getenv('ENKI_BOT_GROQ_API_KEY')
GROQ_MODEL_ID = os.getenv('ENKI_BOT_GROQ_MODEL_ID', 'llama3-8b-8192')
GROQ_ENDPOINT_URL = "https://api.groq.com/openai/v1/chat/completions"
OPENROUTER_API_KEY = os.getenv('ENKI_BOT_OPENROUTER_API_KEY')
OPENROUTER_MODEL_ID = os.getenv('ENKI_BOT_OPENROUTER_MODEL_ID', 'mistralai/mistral-7b-instruct:free')
OPENROUTER_ENDPOINT_URL = "https://openrouter.ai/api/v1/chat/completions"
GOOGLE_AI_API_KEY = os.getenv('ENKI_BOT_GOOGLE_AI_API_KEY')
GOOGLE_AI_MODEL_ID = os.getenv('ENKI_BOT_GOOGLE_AI_MODEL_ID', 'gemini-1.5-flash-latest')

# --- Константы ---
# Имена и никнеймы, на которые бот будет реагировать в групповых чатах
BOT_NICKNAMES_TO_CHECK = [ "enki", "enkibot", "энки", "энкибот", "бот", "bot" ]

# --- Настройка логирования ---
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler("bot_activity.log", encoding='utf-8'), # Логи в файл
        logging.StreamHandler() # Логи в консоль
    ]
)
logger = logging.getLogger(__name__)

# --- Конфигурация разрешенных групп ---
ALLOWED_GROUP_IDS_STR = os.getenv('ENKI_BOT_ALLOWED_GROUP_IDS') 
ALLOWED_GROUP_IDS = set()
if ALLOWED_GROUP_IDS_STR:
    try:
        # Эта логика для разбора строки у вас правильная
        ALLOWED_GROUP_IDS = set(int(id_str.strip()) for id_str in ALLOWED_GROUP_IDS_STR.split(','))
        if ALLOWED_GROUP_IDS:
            logger.info(f"Бот ограничен группами с ID: {ALLOWED_GROUP_IDS}")
    except ValueError:
        # Эта логика для обработки ошибок у вас тоже правильная
        logger.error(f"Неверный формат ENKI_BOT_ALLOWED_GROUP_IDS: '{ALLOWED_GROUP_IDS_STR}'. Ограничение по группам снято.")
        ALLOWED_GROUP_IDS = set()
else:
    logger.info("ENKI_BOT_ALLOWED_GROUP_IDS не задан. Бот будет работать во всех группах.")


# --- Подключение к базе данных и инициализация ---
DB_CONNECTION_STRING = None
if SQL_SERVER_NAME and SQL_DATABASE_NAME:
    DB_CONNECTION_STRING = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={SQL_SERVER_NAME};"
        f"DATABASE={SQL_DATABASE_NAME};"
        f"Trusted_Connection=yes;"
    )
    logger.info(f"Строка подключения к БД сконфигурирована для {SQL_SERVER_NAME}/{SQL_DATABASE_NAME}")
else:
    logger.warning("Переменные окружения для SQL Server не заданы. Функционал БД отключен.")

def get_db_connection():
    if not DB_CONNECTION_STRING: return None
    try: return pyodbc.connect(DB_CONNECTION_STRING, autocommit=False)
    except pyodbc.Error as ex:
        logger.error(f"Ошибка подключения к БД: {ex.args[0]} - {ex}")
        return None

def initialize_database():
    # Эта функция проверяет наличие необходимых таблиц в БД и создает их, если они отсутствуют.
    # Вызывается один раз при старте бота.
    if not DB_CONNECTION_STRING:
        logger.warning("Невозможно инициализировать БД: строка подключения не настроена.")
        return
    # Словарь с запросами на создание таблиц и индексов к ним для ускорения выборок
    table_creation_queries = {
        "ConversationHistory": "CREATE TABLE ConversationHistory (MessageDBID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageID BIGINT NULL, Role NVARCHAR(50) NOT NULL, Content NVARCHAR(MAX) NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ConversationHistory_ChatID_Timestamp": "CREATE INDEX IX_ConversationHistory_ChatID_Timestamp ON ConversationHistory (ChatID, Timestamp DESC);",
        "ChatLog": "CREATE TABLE ChatLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, MessageID BIGINT NOT NULL, MessageText NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ChatLog_ChatID_Timestamp": "CREATE INDEX IX_ChatLog_ChatID_Timestamp ON ChatLog (ChatID, Timestamp DESC);",
        "IX_ChatLog_UserID": "CREATE INDEX IX_ChatLog_UserID ON ChatLog (UserID);",
        "ErrorLog": "CREATE TABLE ErrorLog (ErrorID INT IDENTITY(1,1) PRIMARY KEY, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, LogLevel NVARCHAR(50) NOT NULL, LoggerName NVARCHAR(255) NULL, ModuleName NVARCHAR(255) NULL, FunctionName NVARCHAR(255) NULL, LineNumber INT NULL, ErrorMessage NVARCHAR(MAX) NOT NULL, ExceptionInfo NVARCHAR(MAX) NULL);",
        "IX_ErrorLog_Timestamp": "CREATE INDEX IX_ErrorLog_Timestamp ON ErrorLog (Timestamp DESC);",
        "UserProfiles": "CREATE TABLE UserProfiles (UserID BIGINT PRIMARY KEY, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, LastName NVARCHAR(255) NULL, LastSeen DATETIME2 DEFAULT GETDATE(), MessageCount INT DEFAULT 0, PreferredLanguage NVARCHAR(10) NULL, Notes NVARCHAR(MAX) NULL, ProfileLastUpdated DATETIME2 DEFAULT GETDATE());",
        # --- НАЧАЛО НОВЫХ СТРОК ---
        "UserNameVariations": "CREATE TABLE UserNameVariations (VariationID INT IDENTITY(1,1) PRIMARY KEY, UserID BIGINT NOT NULL, NameVariation NVARCHAR(255) NOT NULL, FOREIGN KEY (UserID) REFERENCES UserProfiles(UserID) ON DELETE CASCADE);",
        "IX_UserNameVariations_NameVariation": "CREATE UNIQUE INDEX IX_UserNameVariations_NameVariation ON UserNameVariations (UserID, NameVariation);"
        # --- КОНЕЦ НОВЫХ СТРОК ---
    }
    conn = None
    try:
        conn = pyodbc.connect(DB_CONNECTION_STRING, autocommit=True)
        cursor = conn.cursor()
        logger.info("Проверка и создание таблиц БД при необходимости...")
        for item_name, query in table_creation_queries.items():
            is_index = item_name.startswith("IX_")
            table_name_for_check = item_name.split("_")[1] if is_index else item_name
            if is_index:
                try:
                    cursor.execute(query)
                    logger.info(f"Индекс {item_name} создан или подтвержден.")
                except pyodbc.ProgrammingError as pe:
                    if "already an index" in str(pe).lower() or "already exists" in str(pe).lower():
                        logger.info(f"Индекс {item_name} уже существует.")
                    else: raise
                continue
            check_sql = "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = ?"
            cursor.execute(check_sql, table_name_for_check)
            if cursor.fetchone(): logger.info(f"Таблица {table_name_for_check} уже существует.")
            else:
                logger.info(f"Таблица {table_name_for_check} не найдена. Создание..."); cursor.execute(query); logger.info(f"Таблица {table_name_for_check} создана.")
        cursor.close(); logger.info("Проверка инициализации БД завершена.")
    except pyodbc.Error as ex: logger.error(f"Ошибка инициализации БД: {ex}", exc_info=True)
    except Exception as e: logger.error(f"Неожиданная ошибка во время инициализации БД: {e}", exc_info=True)
    finally:
        if conn: conn.close()

# --- Логирование ошибок в БД ---
class SQLDBLogHandler(logging.Handler):
    # Этот класс позволяет автоматически записывать критические ошибки Python прямо в таблицу ErrorLog в БД.
    def __init__(self): super().__init__(); self.conn = None
    def _get_db_conn_for_logging(self):
        if not DB_CONNECTION_STRING: return None
        try: return pyodbc.connect(DB_CONNECTION_STRING, autocommit=True)
        except pyodbc.Error: return None
    def emit(self, record: logging.LogRecord):
        if self.conn is None: self.conn = self._get_db_conn_for_logging()
        if self.conn:
            try:
                msg, exc_info_str = self.format(record), traceback.format_exc() if record.exc_info else None
                sql = "INSERT INTO ErrorLog (LogLevel, LoggerName, ModuleName, FunctionName, LineNumber, ErrorMessage, ExceptionInfo) VALUES (?, ?, ?, ?, ?, ?, ?)"
                with self.conn.cursor() as c: c.execute(sql, record.levelname, record.name, record.module, record.funcName, record.lineno, msg, exc_info_str)
            except: self.handleError(record); self.conn = None
    def close(self):
        if self.conn:
            try: self.conn.close()
            except: pass
        super().close()

if DB_CONNECTION_STRING:
    db_log_handler = SQLDBLogHandler(); db_log_handler.setLevel(logging.ERROR)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
    db_log_handler.setFormatter(formatter); logging.getLogger().addHandler(db_log_handler)
    logger.info("Настроено логирование ошибок в SQL БД.")
else: logger.warning("Логирование ошибок в SQL БД НЕ настроено.")

# --- Инициализация клиентов API ---
openai_async_client = None
if OPENAI_API_KEY:
    try: openai_async_client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY); logger.info("Клиент OpenAI AsyncOpenAI инициализирован.")
    except Exception as e: logger.error(f"Не удалось инициализировать клиент OpenAI AsyncOpenAI: {e}")
else: logger.warning("Ключ OpenAI API не найден. Вызовы к OpenAI отключены.")

# --- Вспомогательные функции ---
# --- В разделе "Вспомогательные функции" ---

# Эта функция находится в вашем основном файле EnkiBot.py

async def populate_name_variations_with_llm(user_id: int, first_name: str, last_name: str | None, username: str | None):
    """
    Использует LLM для генерации ЛИНГВИСТИЧЕСКИХ вариантов имени пользователя,
    включая уменьшительные формы, транслитерацию и падежи.
    """
    if not openai_async_client:
        logger.warning(f"Генерация вариантов имени для user {user_id} пропущена: OpenAI клиент не настроен.")
        return

    name_parts = [part for part in [first_name, last_name, username] if part]
    name_info = ", ".join(name_parts)
    
    logger.info(f"Запрос на ЛИНГВИСТИЧЕСКУЮ генерацию вариантов для пользователя {user_id} ({name_info}).")

    # --- НОВЫЙ, СУПЕР-СФОКУСИРОВАННЫЙ ПРОМПТ ---
    system_prompt = (
        "You are a language expert specializing in Russian and English names. Your task is to generate a list of linguistic variations for a user's name. Focus ONLY on realistic, human-used variations. DO NOT generate technical usernames with numbers or suffixes like '_dev'."
        "\n\n**Goal:** Create variations for recognition in natural language text."
        "\n\n**Categories for Generation:**"
        "\n1.  **Original Forms:** The original first name, last name, and combinations."
        "\n2.  **Diminutives & Nicknames:** Common short and affectionate forms (e.g., 'Антонина' -> 'Тоня'; 'Robert' -> 'Rob')."
        "\n3.  **Transliteration (with variants):** Provide multiple common Latin spellings for all Cyrillic forms (original and diminutives). Example for 'Тоня': 'tonya', 'tonia'."
        "\n4.  **Reverse Transliteration:** If the source name is Latin, provide plausible Cyrillic versions. Example for 'Yael': 'Яэль', 'Йаэль'."
        "\n5.  **Russian Declensions (Grammatical Cases):** For all primary Russian names (full and short forms), provide their forms in different grammatical cases (genitive, dative, accusative, instrumental, prepositional). Example for 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'."
        "\n\n**Output Format:** Return a single JSON object: {\"variations\": [\"variation1\", \"variation2\", ...]}. All variations must be in lowercase."
    )
    
    user_prompt = f"Generate linguistic variations for the user with the following info: {name_info}"

    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    name_variations = set()
    try:
        completion = await openai_async_client.chat.completions.create(
            model='gpt-4o-mini',
            messages=messages_for_api,
            temperature=0.3, # Снижаем температуру для более предсказуемых, основанных на правилах результатов
            response_format={"type": "json_object"}
        )
        if completion.choices and completion.choices[0].message:
            response_str = completion.choices[0].message.content
            try:
                data = json.loads(response_str)
                variations_list = data.get('variations')
                if isinstance(variations_list, list):
                    name_variations.update([str(v).lower().strip() for v in variations_list if v and str(v).strip()])
                    logger.info(f"LLM сгенерировала {len(variations_list)} лингвистических вариантов для user {user_id}.")
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Ошибка декодирования JSON от LLM для user {user_id}: {response_str}. Ошибка: {e}")
    except Exception as e:
        logger.error(f"Ошибка OpenAI при генерации вариантов имени для {user_id}: {e}")

    # Добавляем оригинальные имена еще раз
    name_variations.update([p.lower() for p in name_parts])
    
    # Сохраняем все уникальные варианты в БД
    if name_variations:
        db_conn = get_db_connection()
        if db_conn:
            try:
                with db_conn.cursor() as cursor:
                    sql = """
                        MERGE INTO UserNameVariations AS t
                        USING (SELECT ? AS UserID, ? AS NameVariation) AS s
                        ON (t.UserID = s.UserID AND t.NameVariation = s.NameVariation)
                        WHEN NOT MATCHED THEN
                            INSERT (UserID, NameVariation) VALUES (s.UserID, s.NameVariation);
                    """
                    params_to_insert = [(user_id, var) for var in name_variations if var]
                    if params_to_insert:
                        cursor.executemany(sql, params_to_insert)
                        db_conn.commit()
                        logger.info(f"Сохранено/обновлено {len(params_to_insert)} вариантов имени для user {user_id}.")
            except pyodbc.Error as ex:
                logger.error(f"Ошибка БД при сохранении вариантов имени для {user_id}: {ex}")
                if db_conn: db_conn.rollback()
            finally:
                if db_conn: db_conn.close()
def get_translit_variations(name: str) -> set[str]:
    """
    Создает набор вариаций имени, включая оригинал и его транслитерацию.
    Работает в обе стороны: с кириллицы на латиницу и наоборот.
    """
    variations = {name.lower()}
    try:
        # Проверяем, содержит ли имя кириллические символы
        if re.search('[а-яА-Я]', name):
            translit_name = translit(name, 'ru', reversed=True) # ru -> en
            variations.add(translit_name.lower())
        else:
            translit_name = translit(name, 'ru') # en -> ru
            variations.add(translit_name.lower())
    except LanguagePackNotFound: # ИСПРАВЛЕННАЯ ОШИБКА
        logger.warning(f"Пакет для транслитерации 'ru' не найден. Пропускаем для имени: {name}")
    except Exception as e:
        logger.error(f"Неожиданная ошибка при транслитерации имени {name}: {e}")
    return variations
async def analyze_replied_message(original_text: str, user_question: str) -> str:
    """
    Анализирует исходный текст (на который ответили) в контексте вопроса пользователя.
    """
    logger.info(f"Запрос на анализ текста. Длина исходного текста: {len(original_text)}, Вопрос: '{user_question}'")
    
    # Создаем очень четкий промпт для LLM, чтобы она поняла свою задачу
    system_prompt = (
        "Ты — AI-аналитик. Твоя задача — проанализировать 'Исходный текст' и дать содержательный ответ на 'Вопрос пользователя' об этом тексте. "
        "Твой анализ должен быть объективным, кратким и по существу. Если вопрос общий (например, 'что думаешь?'), "
        "сделай краткое резюме, выделив ключевые тезисы или настроения в исходном тексте."
    )

    user_prompt = f"""
Исходный текст для анализа:
---
"{original_text}"
---

Вопрос пользователя об этом тексте:
---
"{user_question}"
---

Твой анализ:
"""

    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # Для этой задачи можно использовать любую из ваших LLM. OpenAI хорошо подходит для анализа.
    if openai_async_client:
        try:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini', # Быстрая и умная модель для таких задач
                messages=messages_for_api,
                temperature=0.5, # Чуть больше креативности для анализа
                max_tokens=1000
            )
            if completion.choices and completion.choices[0].message:
                return completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Ошибка OpenAI при анализе сообщения: {e}", exc_info=True)
            return "К сожалению, произошла ошибка во время анализа текста."
    
    return "Функция анализа не может быть выполнена, так как AI-клиент не настроен."
# --- Функции для вызова API ---
async def call_openai_llm(messages_for_api: list) -> str | None:
    if not openai_async_client: logger.warning("Клиент OpenAI недоступен."); return None
    logger.info(f"Вызов OpenAI (модель: {OPENAI_MODEL_ID}) с {len(messages_for_api)} сообщениями контекста.")
    try:
        completion = await openai_async_client.chat.completions.create(model=OPENAI_MODEL_ID, messages=messages_for_api, temperature=0.7, max_tokens=2000)
        if completion.choices and completion.choices[0].message: return completion.choices[0].message.content.strip()
    except Exception as e: logger.error(f"Ошибка при работе с OpenAI API: {e}", exc_info=True)
    return None

async def call_llm_api(p_name: str, key: str | None, url: str | None, model: str, msgs: list) -> str | None:
    # Универсальная функция для вызова LLM API, совместимых с OpenAI (Groq, OpenRouter)
    if not key: logger.warning(f"Ключ API для {p_name} недоступен."); return None
    if not url: logger.warning(f"URL для {p_name} недоступен."); return None
    logger.info(f"Вызов {p_name} ({model}) с {len(msgs)} сообщениями контекста.")
    hdrs = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    if p_name == "OpenRouter": hdrs.update({"HTTP-Referer": "YOUR_PROJECT_URL", "X-Title": "EnkiBot"})
    payload = {"model": model, "messages": msgs, "max_tokens": 2000, "temperature": 0.7}
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.post(url, json=payload, headers=hdrs, timeout=30.0); resp.raise_for_status()
            data = resp.json()
            if data.get("choices") and data["choices"][0].get("message"): return data["choices"][0]["message"].get("content","").strip()
    except Exception as e: logger.error(f"Ошибка при работе с API {p_name}: {e}", exc_info=True)
    return None

async def call_google_ai_llm_specific(messages_for_api: list) -> str | None:
    # Отдельная функция для Google AI, так как их SDK имеет другой формат
    if not GOOGLE_AI_API_KEY: logger.warning("Ключ Google AI API недоступен."); return None
    try:
        import google.generativeai as genai; genai.configure(api_key=GOOGLE_AI_API_KEY)
        sys_instr, gem_hist, final_prompt = "You are a helpful assistant.", [], ""
        # ... (логика преобразования истории в формат Gemini) ...
        # Эта часть кода требует тщательной адаптации формата сообщений
        return "Google AI call not fully implemented in this example"
    except ImportError: logger.error("Библиотека google.generativeai не установлена.")
    except Exception as e: logger.error(f"Ошибка Google AI API: {e}", exc_info=True)
    return None

# --- Основные функции бота ---
async def analyze_weather_request_with_llm(text: str) -> dict:
    """
    Анализирует запрос о погоде и определяет, нужна ли текущая погода или прогноз.
    Возвращает словарь, например: {'type': 'forecast', 'days': 7} или {'type': 'current'}.
    """
    logger.info(f"Анализ типа погодного запроса из текста: '{text}'")
    
    system_prompt = (
        "You are an expert in analyzing weather-related requests. Your task is to determine the user's intent. "
        "Does the user want the 'current' weather or a 'forecast' for several days? "
        "If it is a forecast, also determine for how many days. Your answer MUST be a valid JSON object and nothing else. "
    
        "Examples:\n"
        # --- Basic Cases ---
        "- User text: 'погода в Лондоне' -> Your response: {\"type\": \"current\"}\n"
        "- User text: 'what's the weather like?' -> Your response: {\"type\": \"current\"}\n"
    
        # --- Multi-Day Forecast Cases ---
        "- User text: 'погода в Тампе на неделю' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n"
        "- User text: 'прогноз на 5 дней в Берлине' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n"
    
        # --- NEW: Cases for specific or relative days ---
        "- User text: 'какая погода будет завтра?' -> Your response: {\"type\": \"forecast\", \"days\": 2}\n"
        "- User text: 'дай прогноз на субботу' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n"
    
        # --- NEW: Cases for generic forecast requests ---
        "- User text: 'просто дай прогноз погоды' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n"
        "- User text: 'прогноз на выходные' -> Your response: {\"type\": \"forecast\", \"days\": 3}\n"

        # --- Fallback Rule ---
        "If you are unsure, always default to 'current'."
    )
    
    messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": text}]
    
    try:
        if openai_async_client:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini', messages=messages_for_api, temperature=0, response_format={"type": "json_object"}
            )
            if completion.choices and completion.choices[0].message:
                response_str = completion.choices[0].message.content.strip()
                logger.info(f"LLM вернула для анализа погоды: {response_str}")
                return json.loads(response_str)
    except Exception as e:
        logger.error(f"Ошибка LLM при анализе запроса погоды: {e}")

    # Возвращаем значение по умолчанию в случае ошибки
    return {"type": "current"}
async def extract_location_with_llm(text: str) -> str | None:
    """
    Использует LLM для извлечения названия города из текста пользователя.
    Возвращает название города на английском языке, готовое для API, или None.
    """
    logger.info(f"Запрос на извлечение локации из текста: '{text}'")
    
    # Промпт специально разработан, чтобы LLM вернула только название или 'None'
    system_prompt = (
        "You are an expert text analysis tool. Your task is to extract a city or location name from the user's text. "
        "Analyze the following text and identify the geographical location (city, region, country) mentioned. "
        "Return ONLY the name of the location in English, suitable for a weather API query. "
        "For example, if the text is 'какая погода в Санкт-Петербурге', you must return 'Saint Petersburg'. "
        "If the text is 'покажи погоду в Астане', you must return 'Astana'. "
        "If no specific location is found, you MUST return the single word: None"
    )
    
    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": text}
    ]
    
    location = None
    
    # Пытаемся использовать OpenAI как приоритетный вариант
    if openai_async_client:
        try:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini', # Используем быструю и умную модель
                messages=messages_for_api,
                temperature=0, # Нам нужна точность, а не креативность
                max_tokens=50
            )
            if completion.choices and completion.choices[0].message:
                location = completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Ошибка OpenAI при извлечении локации: {e}")

    # Если OpenAI не сработал, пробуем Groq как резервный вариант
    if not location and GROQ_API_KEY:
         location = await call_llm_api("Groq (Location)", GROQ_API_KEY, GROQ_ENDPOINT_URL, GROQ_MODEL_ID, messages_for_api)

    # Проверяем ответ от LLM. Если она вернула 'None' или пустую строку, считаем, что город не найден.
    if location and location.lower() != 'none' and location.strip() != "":
        logger.info(f"LLM успешно извлекла локацию: '{location}'")
        return location
    
    logger.warning("LLM не смогла извлечь локацию из текста.")
    return None
async def extract_news_topic_with_llm(text: str) -> str | None:
    """
    Использует LLM для извлечения темы/ключа для поиска новостей.
    """
    logger.info(f"Запрос на извлечение темы новостей из текста: '{text}'")
    
    system_prompt = (
        "You are an expert text analysis tool. Your task is to extract the main topic, keyword, or location from a user's request for news. "
        "Analyze the text. If it contains a specific subject, you MUST return that subject in its base (nominative) case and in the original language. "
        "For example, for a request 'новости в москве', you must return 'Москва'. For 'news about cars', return 'cars'. "
        "If the request is general (e.g., 'what's the news?', 'latest headlines'), you MUST return the single word: None"
    )
    
    messages_for_api = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": text}
    ]
    
    topic = None
    # Используем быструю модель для этой задачи
    if openai_async_client:
        try:
            completion = await openai_async_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=messages_for_api,
                temperature=0,
                max_tokens=50
            )
            if completion.choices and completion.choices[0].message:
                topic = completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Ошибка OpenAI при извлечении темы новостей: {e}")

    if topic and topic.lower() != 'none' and topic.strip() != "":
        logger.info(f"LLM успешно извлекла тему новостей: '{topic}'")
        return topic
    
    logger.info("LLM не нашла конкретной темы, будут запрошены общие новости.")
    return None
from datetime import datetime, timedelta # Убедитесь, что timedelta импортирована

# Переименуем для ясности
async def get_weather_data(location: str, forecast_type: str = 'current', days: int = 7) -> str:
    """
    Получает данные о погоде: текущие или прогноз на несколько дней.
    """
    if not WEATHER_API_KEY:
        return "Функция погоды не настроена: отсутствует ключ API."

    # --- БЛОК ДЛЯ ТЕКУЩЕЙ ПОГОДЫ (остался почти без изменений) ---
    if forecast_type == 'current':
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {"q": location, "appid": WEATHER_API_KEY, "units": "metric", "lang": "ru"}
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
            data = response.json()
            city = data.get("name")
            desc = data["weather"][0].get("description")
            temp = data["main"].get("temp")
            feels = data["main"].get("feels_like")
            wind = data["wind"].get("speed")
            return (
                f"Погода в городе {city}:\n"
                f"  - Сейчас: {desc.capitalize()}\n"
                f"  - Температура: {temp:.1f}°C (ощущается как {feels:.1f}°C)\n"
                f"  - Ветер: {wind:.1f} м/с"
            )
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404: return f"К сожалению, я не смог найти город '{location}'."
            logger.error(f"HTTP ошибка при запросе погоды для {location}: {e}")
            return "Не удалось получить прогноз погоды из-за ошибки сервера."
        except Exception as e:
            logger.error(f"Неожиданная ошибка при запросе погоды: {e}", exc_info=True)
            return "Произошла непредвиденная ошибка при запросе прогноза."

    # --- НОВЫЙ БЛОК ДЛЯ ПРОГНОЗА НА НЕСКОЛЬКО ДНЕЙ ---
    elif forecast_type == 'forecast':
        # Используем другой endpoint для прогноза
        url = "https://api.openweathermap.org/data/2.5/forecast"
        # Запрашиваем на 5 дней, так как это стандарт для бесплатного API
        params = {"q": location, "appid": WEATHER_API_KEY, "units": "metric", "lang": "ru", "cnt": 40} # 40 записей = 5 дней * 8 записей/день
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
            
            data = response.json()
            city = data.get("city", {}).get("name")
            forecast_list = data.get("list", [])
            
            if not forecast_list:
                return f"Не удалось получить прогноз для города '{location}'."

            daily_forecasts = {}
            for forecast in forecast_list:
                # Группируем по дням, чтобы избежать дубликатов
                day_str = datetime.fromtimestamp(forecast["dt"]).strftime('%Y-%m-%d')
                if day_str not in daily_forecasts:
                    daily_forecasts[day_str] = {
                        'day_name': datetime.fromtimestamp(forecast["dt"]).strftime('%A'), # Название дня недели
                        'temp': forecast['main']['temp'],
                        'description': forecast['weather'][0]['description']
                    }

            # Форматируем красивый ответ
            report_lines = [f"Прогноз погоды в городе {city} на ближайшие дни:"]
            for day_data in list(daily_forecasts.values())[:days]: # Ограничиваем кол-вом запрошенных дней
                report_lines.append(
                    f"  - {day_data['day_name'].capitalize()}: {day_data['temp']:.0f}°C, {day_data['description']}"
                )
            return "\n".join(report_lines)

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404: return f"К сожалению, я не смог найти город '{location}' для прогноза."
            logger.error(f"HTTP ошибка при запросе прогноза для {location}: {e}")
            return "Не удалось получить прогноз погоды из-за ошибки сервера."
        except Exception as e:
            logger.error(f"Неожиданная ошибка при запросе прогноза: {e}", exc_info=True)
            return "Произошла непредвиденная ошибка при запросе прогноза."
    
    return "Неизвестный тип запроса погоды."

async def analyze_and_update_user_profile(user_id: int, message_text: str):
    """
    Создает или обновляет структурированный психологический профиль пользователя,
    анализируя его сообщения.
    """
    if not openai_async_client:
        logger.warning(f"Профилирование для user {user_id} пропущено: OpenAI клиент не настроен.")
        return

    MAX_PROFILE_SIZE = 4000  # Увеличим размер, т.к. профиль стал более детальным
    logger.info(f"Запуск создания/обновления психологического профиля для пользователя {user_id}...")

    # Шаг 1: Получаем текущий профиль из БД
    current_profile_notes = ""
    db_conn = get_db_connection()
    if db_conn:
        try:
            with db_conn.cursor() as cursor:
                cursor.execute("SELECT Notes FROM UserProfiles WHERE UserID = ?", user_id)
                row = cursor.fetchone()
                if row and row[0]:
                    current_profile_notes = row[0]
        except pyodbc.Error as ex:
            logger.error(f"Ошибка БД при чтении профиля для {user_id}: {ex}")
        finally:
            db_conn.close()

    # Шаг 2: Выбираем стратегию и промпт (создание или обновление)
    
    system_prompt = ""
    user_prompt = ""

    if not current_profile_notes:
        # СТРАТЕГИЯ 1: СОЗДАНИЕ ПЕРВОНАЧАЛЬНОГО ПРОФИЛЯ
        logger.info(f"Существующий профиль для {user_id} не найден. Создание нового...")
        system_prompt = (
            "Ты — AI-психолог и профайлер. Твоя задача — создать первоначальный психологический портрет пользователя на основе его сообщения. "
            "Проанализируй текст на предмет стиля общения, возможных черт личности (используй модель 'Большая пятерка' как ориентир: Открытость, Добросовестность, Экстраверсия, Доброжелательность), а также ключевых интересов. "
            "Твой ответ ДОЛЖЕН быть структурирован строго по предложенному формату с заголовками Markdown. Будь объективен и основывайся только на предоставленном тексте."
        )
        user_prompt = f"""
            Проанализируй следующее сообщение от нового пользователя и создай его психологический профиль.

            Сообщение пользователя:
            ---
            "{message_text}"
            ---

            Твой результат (строго в формате Markdown):
            """
    else:
        # СТРАТЕГИЯ 2: ОБНОВЛЕНИЕ И СИНТЕЗ СУЩЕСТВУЮЩЕГО ПРОФИЛЯ
        logger.info(f"Обновление существующего профиля для {user_id}...")
        system_prompt = (
            "Ты — AI-психолог, обновляющий досье на пациента. Тебе предоставлены 'Существующий психологический профиль' и 'Новое сообщение' от пользователя. "
            "Твоя задача — не просто добавить новую информацию, а **переосмыслить и синтезировать весь профиль**. "
            "Если новое сообщение подтверждает черту — усиль ее описание. Если противоречит — скорректируй или смягчи. Если открывает что-то новое — интегрируй это в существующую структуру. "
            "Цель — получить эволюционировавший, но все еще лаконичный профиль. Сохраняй исходную структуру Markdown."
        )
        user_prompt = f"""
            Существующий психологический профиль:
            ---
            {current_profile_notes}
            ---

            Новое сообщение от пользователя для анализа:
            ---
            "{message_text}"
            ---

            Твой обновленный и переосмысленный психологический профиль:
            """

    # Шаг 3: Вызов LLM для анализа
    analysis_messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    
    updated_profile_notes = None
    try:
        # Используем более мощную модель для сложных задач анализа
        completion = await openai_async_client.chat.completions.create(
            model='gpt-4o-mini',  # gpt-4o даст еще лучшие результаты
            messages=analysis_messages,
            temperature=0.5,
            max_tokens=1000 
        )
        if completion.choices and completion.choices[0].message:
            updated_profile_notes = completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Ошибка OpenAI при анализе профиля для {user_id}: {e}")

    # Шаг 4: Обновляем профиль в БД
    if updated_profile_notes and updated_profile_notes.strip():
        db_conn_update = get_db_connection()
        if db_conn_update:
            try:
                with db_conn_update.cursor() as cursor:
                    final_notes = updated_profile_notes[:MAX_PROFILE_SIZE]
                    sql = "UPDATE UserProfiles SET Notes = ?, ProfileLastUpdated = GETDATE() WHERE UserID = ?"
                    cursor.execute(sql, final_notes, user_id)
                    db_conn_update.commit()
                    logger.info(f"Успешно обновлен психологический профиль для пользователя {user_id}.")
            except pyodbc.Error as ex:
                logger.error(f"Ошибка БД при сохранении профиля для {user_id}: {ex}")
                if db_conn_update: db_conn_update.rollback()
            finally:
                if db_conn_update: db_conn_update.close()
    else:
        logger.warning(f"Анализ профиля не вернул результат для пользователя {user_id}.")
def find_search_query_in_text(text: str) -> str | None:
    """
    Анализирует текст, приводя слова к начальной форме (лемме),
    и ищет комбинацию триггерных слов и предлогов для извлечения имени.
    Возвращает имя пользователя для поиска или None.
    """
    # Словари триггеров (используем леммы - начальные формы слов)
    # Легко расширять, добавляя новые ключевые слова
    TELL_LEMMAS = {'рассказать', 'поведать', 'сообщить', 'описать'}
    INFO_LEMMAS = {'информация', 'инфо', 'справка', 'досье', 'данные'}
    WHO_LEMMAS = {'кто', 'что'}
    EXPLAIN_LEMMAS = {'пояснить', 'объяснить'}
    REMEMBER_LEMMAS = {'помнить', 'напомнить'}

    # Предлоги, которые обычно следуют за триггерами
    PREPOSITIONS = {'о', 'про', 'за', 'на', 'по'}

    words = re.findall(r"[\w'-]+", text.lower())
    
    for i, word in enumerate(words):
        try:
            # Получаем лемму слова
            lemma = morph.parse(word)[0].normal_form
            
            # Проверяем, является ли лемма одним из наших триггеров
            is_trigger = (lemma in TELL_LEMMAS or 
                          lemma in INFO_LEMMAS or 
                          lemma in WHO_LEMMAS or 
                          lemma in EXPLAIN_LEMMAS or 
                          lemma in REMEMBER_LEMMAS)

            if is_trigger:
                # Мы нашли триггерное слово. Теперь нужно найти имя, которое идет после него.
                # Индекс, с которого начинается имя
                start_index = i + 1
                
                # Если следующее слово - предлог, пропускаем его
                if start_index < len(words) and words[start_index] in PREPOSITIONS:
                    start_index += 1
                
                # Все, что идет дальше (до 3 слов), считаем именем
                if start_index < len(words):
                    # Захватываем от 1 до 3 слов после триггера/предлога
                    name_parts = words[start_index : start_index + 3]
                    return " ".join(name_parts)

        except Exception as e:
            logger.error(f"Ошибка при лемматизации слова '{word}': {e}")
            continue
            
    return None
async def get_orchestrated_llm_response(prompt_text: str, chat_id: int, user_id: int, message_id: int, context: ContextTypes.DEFAULT_TYPE) -> str:
    """
    Это "мозг" бота. Функция определяет, нужно ли искать информацию в памяти,
    собирает весь необходимый контекст и управляет вызовами к LLM.
    """
    history_from_db, keyword_context_messages, profile_context_messages = [], [], []
    conn = get_db_connection()
    
    # Сначала получаем общую историю чата
    if conn:
        try:
            with conn.cursor() as c:
                MAX_RECENT_HISTORY = 100
                sql_hist = "SELECT TOP (?) Role, Content FROM ConversationHistory WHERE ChatID = ? ORDER BY Timestamp DESC"
                c.execute(sql_hist, MAX_RECENT_HISTORY, chat_id)
                history_from_db = [{"role": row.Role.lower(), "content": row.Content} for row in reversed(c.fetchall())]
        except pyodbc.Error as ex: 
            logger.error(f"DB error during history fetch: {ex}")
    

    # --- Шаг 1: Лингвистический поиск ключевой фразы ---
    search_term_original = find_search_query_in_text(prompt_text)

    if search_term_original:
        logger.info(f"Лингвистический анализ нашел запрос о пользователе: '{search_term_original}'")
    
    # --- Шаг 2: Поиск пользователя, его профиля и последних сообщений ---
    if conn and search_term_original:
        try:
            with conn.cursor() as c:
                # --- НАЧАЛО НОВОЙ ЛОГИКИ ПОИСКА ---
                # Теперь мы ищем точное совпадение в новой таблице вариантов имен.
                # Это быстрее и точнее, чем поиск с LIKE по нескольким полям.
                logger.info(f"Ищу UserID в таблице UserNameVariations по запросу: '{search_term_original.lower()}'")
                
                # Сначала находим ID всех пользователей, у которых есть такой вариант имени.
                sql_find_user_ids = "SELECT DISTINCT UserID FROM UserNameVariations WHERE NameVariation = ?"
                c.execute(sql_find_user_ids, search_term_original.lower())
                user_ids = [row.UserID for row in c.fetchall()]

                matched_profiles = []
                if user_ids:
                    # Если ID найдены, одним запросом получаем полные профили этих пользователей.
                    id_placeholders = ','.join('?' for _ in user_ids)
                    sql_find_user = f"""
                        SELECT UserID, FirstName, LastName, Username, Notes 
                        FROM UserProfiles 
                        WHERE UserID IN ({id_placeholders})
                    """
                    c.execute(sql_find_user, *user_ids)
                    matched_profiles = c.fetchall()
                
                logger.info(f"Найдено {len(matched_profiles)} профилей по запросу '{search_term_original}'.")
                # --- КОНЕЦ НОВОЙ ЛОГИКИ ПОИСКА ---

                # <<< НАЧАЛО БЛОКА СИНТЕЗА ДАННЫХ (остался без изменений) >>>
                if len(matched_profiles) == 1:
                    profile = matched_profiles[0]
                    target_user_id = profile.UserID
                    user_identifier = profile.FirstName or profile.Username or f"User ID {target_user_id}"

                    # 1. Получаем готовое досье из профиля
                    if profile.Notes and profile.Notes.strip():
                        profile_context_messages.append({
                            "role": "system",
                            "content": f"Важнейший контекст (готовое досье) по пользователю '{user_identifier}':\n---\n{profile.Notes}\n---"
                        })
                        logger.info(f"Загружен профиль (досье) для '{user_identifier}'.")

                    # 2. Получаем последние 50 сообщений из лога
                    logger.info(f"Запрашиваю до 50 последних сообщений для пользователя {user_identifier} (ID: {target_user_id})...")
                    sql_get_messages = """
                        SELECT TOP 50 MessageText 
                        FROM ChatLog
                        WHERE UserID = ? AND ChatID = ?
                        ORDER BY Timestamp DESC
                    """
                    c.execute(sql_get_messages, target_user_id, chat_id)
                    recent_messages_rows = c.fetchall()

                    if recent_messages_rows:
                        formatted_messages = "\n".join([f'- "{row.MessageText}"' for row in recent_messages_rows if row.MessageText and row.MessageText.strip()])
                        keyword_context_messages.append({
                            "role": "system",
                            "content": f"Дополнительный контекст для анализа (сырые данные): Вот до 50 последних сообщений от '{user_identifier}'. Используй их вместе с досье для составления самого актуального ответа.\n---\n{formatted_messages}\n---"
                        })
                        logger.info(f"Загружено {len(recent_messages_rows)} последних сообщений для анализа.")
                    else:
                        logger.info(f"Последние сообщения для {user_identifier} не найдены в логах этого чата.")
                # <<< КОНЕЦ БЛОКА СИНТЕЗА ДАННЫХ >>>

                elif len(matched_profiles) > 1:
                    user_options = [f"@{p.Username}" if p.Username else f"{p.FirstName or ''} {p.LastName or ''}".strip() for p in matched_profiles]
                    user_options = [opt for opt in user_options if opt]
                    logger.info(f"Найдено несколько пользователей: {user_options}. Запрашиваю уточнение.")
                    return f"Я нашел несколько пользователей с таким именем: {', '.join(user_options)}. О ком именно вы спрашиваете? Пожалуйста, уточните (можно по @username)."
                
                else:
                    logger.info(f"Профили не найдены для '{search_term_original}'.") # Сообщение остается, но теперь оно означает, что в таблице UserNameVariations нет такого имени.

        except pyodbc.Error as ex:
            logger.error(f"Ошибка БД при поиске в памяти: {ex}")
        finally:
            if conn:
                conn.close() # Закрываем соединение здесь, так как все операции с БД в этой функции завершены
                conn = None # Устанавливаем в None, чтобы избежать двойного закрытия

    # --- Шаг 3: Оркестрация LLM (остался без изменений) ---
    sys_prompt_content = (
        "Ты EnkiBot, умный и дружелюбный ассистент-аналитик в Telegram-чате, созданный Yael Demedetskaya. "
        "Твоя задача — помогать пользователям, отвечая на их вопросы. Ты обладаешь долгосрочной памятью о разговорах и профилях участников. "
        "Когда тебя просят рассказать о ком-то, твоя главная задача — СИНТЕЗИРОВАТЬ ИНФОРМАЦИЮ. "
        "Тебе будут предоставлены два типа данных: готовое досье из профиля и набор последних 'сырых' сообщений от этого человека. "
        "Проанализируй ОБА источника и составь на их основе новый, краткий, но содержательный и актуальный ответ. Не просто пересказывай досье, а обогащай его свежей информацией из сообщений. "
        "Отвечай развернуто, естественно и по-русски. Будь вежлив, но не слишком формален."
    )
    
    messages_for_api = [{"role": "system", "content": sys_prompt_content}] + profile_context_messages + keyword_context_messages + history_from_db + [{"role": "user", "content": prompt_text}]

    MAX_MSG_CTX = 40
    if len(messages_for_api) > MAX_MSG_CTX:
        sys_p = [m for m in messages_for_api if m["role"] == "system"]
        usr_hist = [m for m in messages_for_api if m["role"] != "system"]
        messages_for_api = sys_p + usr_hist[-(MAX_MSG_CTX - len(sys_p)):]

    tasks, task_names = [], []
    if openai_async_client: 
        tasks.append(call_openai_llm(messages_for_api))
        task_names.append("OpenAI")
    if GROQ_API_KEY: 
        tasks.append(call_llm_api("Groq", GROQ_API_KEY, GROQ_ENDPOINT_URL, GROQ_MODEL_ID, messages_for_api))
        task_names.append("Groq")

    if not tasks:
        return "Извините, ни один из моих AI-ассистентов сейчас не доступен."

    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    final_reply = None
    for i, res in enumerate(results):
        if isinstance(res, str) and res.strip():
            logger.info(f"Используем успешный ответ от {task_names[i]}.")
            final_reply = res.replace('**', '')
            break
        elif isinstance(res, Exception):
            logger.error(f"Провайдер {task_names[i]} вернул ошибку: {res}")

    if not final_reply:
        final_reply = "К сожалению, я не смог получить четкий ответ. Пожалуйста, попробуйте еще раз."

    # Сохранение в историю вынесено из блока `if conn...`, так как соединение уже может быть закрыто
    db_conn_for_saving = get_db_connection()
    if db_conn_for_saving:
        try:
            with db_conn_for_saving.cursor() as c:
                sql_save = "INSERT INTO ConversationHistory (ChatID, UserID, MessageID, Role, Content) VALUES (?, ?, ?, ?, ?)"
                c.execute(sql_save, chat_id, user_id, message_id, 'user', prompt_text)
                c.execute(sql_save, chat_id, context.bot.id, None, 'assistant', final_reply)
                db_conn_for_saving.commit()
        except pyodbc.Error as ex:
            logger.error(f"Ошибка БД при сохранении истории: {ex}")
            if db_conn_for_saving: db_conn_for_saving.rollback()
        finally:
            if db_conn_for_saving: db_conn_for_saving.close()
            
    return final_reply
# --- Обработчики команд и сообщений ---

async def log_message_to_db(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # Логирует каждое сообщение в ChatLog и обновляет профиль пользователя
    if not update.message or not update.message.text: return
    chat_id, user, message = update.effective_chat.id, update.effective_user, update.message
    if ALLOWED_GROUP_IDS and chat_id not in ALLOWED_GROUP_IDS: return

    conn = get_db_connection()
    if conn:
        try:
            with conn.cursor() as c: 
                # SQL MERGE - это мощная команда "UPSERT" (UPDATE or INSERT).
                # Она проверяет, есть ли юзер с таким UserID. Если есть - обновляет его данные.
                # Если нет - вставляет новую запись. Это избавляет от лишних проверок в коде.
                upsert_user_sql = """
                    MERGE UserProfiles AS t
                    USING (VALUES(?,?,?,?,GETDATE())) AS s(UserID,Username,FirstName,LastName,LastSeen)
                    ON t.UserID = s.UserID
                    WHEN MATCHED THEN
                        UPDATE SET Username=s.Username, FirstName=s.FirstName, LastName=s.LastName, LastSeen=s.LastSeen, MessageCount=ISNULL(t.MessageCount,0)+1
                    WHEN NOT MATCHED THEN
                        INSERT(UserID,Username,FirstName,LastName,LastSeen,MessageCount,ProfileLastUpdated)
                        VALUES(s.UserID,s.Username,s.FirstName,s.LastName,s.LastSeen,1,GETDATE())
                    OUTPUT $action AS Action;
                """
                c.execute(upsert_user_sql, user.id, user.username, user.first_name, user.last_name)
                
                sql_chatlog = "INSERT INTO ChatLog (ChatID, UserID, Username, FirstName, MessageID, MessageText) VALUES (?, ?, ?, ?, ?, ?)"
                c.execute(sql_chatlog, chat_id, user.id, user.username, user.first_name, message.message_id, message.text)
                conn.commit()
            
            # Запускаем анализ профиля в фоновом режиме, чтобы не задерживать основной поток
            if message.text and len(message.text.strip()) > 10 :
                asyncio.create_task(analyze_and_update_user_profile(user_id=user.id, message_text=message.text))
        except pyodbc.Error as ex: 
            logger.error(f"Ошибка БД при логировании сообщения: {ex}")
            if conn: conn.rollback() 
        finally: 
            if conn: conn.close()

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    # Главный обработчик всех текстовых сообщений.
    if not update.message or not update.message.text:
        return

    # Шаг 1: Логируем все сообщения в БД.
    # Эта функция также запускает фоновое обновление профиля пользователя.
    await log_message_to_db(update, context)

    chat_id = update.effective_chat.id
    user_msg_txt = update.message.text

    # Проверяем, разрешена ли работа в этой группе
    if ALLOWED_GROUP_IDS and chat_id not in ALLOWED_GROUP_IDS:
        return

    # <<< НАЧАЛО БЛОКА: Анализ сообщения, на которое ответили >>>
    # Проверяем, является ли это сообщение ответом на другое сообщение с текстом
    if update.message.reply_to_message and update.message.reply_to_message.text:
        msg_lower = user_msg_txt.lower()
        bot_user_lower = context.bot.username.lower()
        
        # Проверяем, упомянули ли бота в тексте ответа, используя тот же список никнеймов
        is_bot_mentioned = (f"@{bot_user_lower}" in msg_lower or
                            any(re.search(r'\b' + re.escape(n) + r'\b', msg_lower, re.I) for n in BOT_NICKNAMES_TO_CHECK))

        # Условие срабатывает, если упомянули бота И отвечают НЕ на его собственное сообщение
        if is_bot_mentioned and update.message.reply_to_message.from_user.id != context.bot.id:
            logger.info("Сработал триггер анализа сообщения по ответу.")
            await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

            original_text = update.message.reply_to_message.text
            user_question = user_msg_txt  # Вопрос — это сам текст ответа

            # <<< НАЧАЛО ИЗМЕНЕНИЙ: ОБРАБОТКА "ПУСТОГО" ВОПРОСА >>>
            # Проверяем, содержит ли вопрос что-то кроме имени бота
            question_check = user_msg_txt.lower()
            # Удаляем все известные никнеймы бота из текста вопроса
            for nickname in BOT_NICKNAMES_TO_CHECK:
                question_check = question_check.replace(nickname, '').strip()
            # Дополнительно удаляем прямое упоминание @
            question_check = question_check.replace(f"@{bot_user_lower}", '').strip()
            
            # Если после очистки почти ничего не осталось, задаем вопрос по умолчанию
            if len(question_check) < 5: # Используем небольшое число, чтобы отсечь знаки препинания и короткий мусор
                logger.info(f"Вопрос в ответе почти пуст ('{user_msg_txt}'). Используется вопрос по умолчанию.")
                user_question = "Проанализируй этот текст, выдели главную мысль и выскажи свое мнение."
            # <<< КОНЕЦ ИЗМЕНЕНИЙ >>>

            # Вызываем нашу новую функцию-анализатор с (возможно) новым вопросом
            analysis_result = await analyze_replied_message(original_text, user_question)
            
            # Отвечаем на сообщение пользователя (которое само является ответом)
            await update.message.reply_text(analysis_result)
            return  # ВАЖНО: Завершаем дальнейшую обработку
    # <<< КОНЕЦ БЛОКА >>>

    # --- Стандартная логика определения триггера для обычных сообщений ---
    is_group = update.message.chat.type in ['group', 'supergroup']
    msg_lower = user_msg_txt.lower()
    bot_user_lower = context.bot.username.lower()
    triggered = (f"@{bot_user_lower}" in msg_lower or
                 any(re.search(r'\b' + re.escape(n) + r'\b', msg_lower, re.I) for n in BOT_NICKNAMES_TO_CHECK) or
                 (update.message.reply_to_message and update.message.reply_to_message.from_user.id == context.bot.id))

    # В группе реагируем только на прямое обращение
    if is_group and not triggered:
        return

    # --- Маршрутизация по специальным функциям (погода, новости) ---
    if re.search(r'\b(погод|прогноз|weather|forecast)\b', msg_lower, re.I):
        logger.info(f"Сработал триггер погоды. Запускаю анализ запроса: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

        intent_data = await analyze_weather_request_with_llm(user_msg_txt)
        forecast_type = intent_data.get("type", "current")
        days_to_forecast = intent_data.get("days", 7)
        city = await extract_location_with_llm(user_msg_txt)

        if city:
            weather_report = await get_weather_data(
                location=city,
                forecast_type=forecast_type,
                days=days_to_forecast
            )
            await update.message.reply_text(weather_report)
        else:
            await update.message.reply_text("Я готов показать погоду, но не смог понять, для какого города. Пожалуйста, уточните.")
        return

    if re.search(r'\b(новост|news|события|заголовки|headlines|что нового)\b', msg_lower, re.I):
        logger.info(f"Сработал триггер новостей. Запускаю извлечение темы из текста: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

        topic = await extract_news_topic_with_llm(user_msg_txt)
        news_report = await get_latest_news(query=topic)
        await update.message.reply_text(news_report, disable_web_page_preview=True)
        return

    # --- Вызов основного "мозга" для всех остальных случаев ---
    logger.info(f"Обработка сообщения через LLM в чате {chat_id}")
    await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
    reply = await get_orchestrated_llm_response(
        prompt_text=user_msg_txt,
        chat_id=chat_id,
        user_id=update.effective_user.id,
        message_id=update.message.message_id,
        context=context
    )
    if reply:
        await update.message.reply_text(reply)

# --- Остальные обработчики и главная функция ---
async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None: 
    logger.error(f'Update "{update}" caused error "{context.error}"', exc_info=context.error)

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user
    await update.message.reply_html(rf"Привет, {user.mention_html()}! Я EnkiBot, создан Yael Demedetskaya. Чем могу помочь?")

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = ("Я EnkiBot, AI-ассистент от Yael Demedetskaya.\n"
           "В группах я отвечаю, когда вы упоминаете меня по имени (@EnkiBot, Энки) или отвечаете на мои сообщения.\n" 
           "Вы можете спросить меня 'расскажи о [имя/тема]', чтобы я поискал информацию в истории чата.\n"
           "Чтобы узнать погоду, спросите 'какая погода в [город]?'\n\n"
           "**Команды:**\n/start - Начало работы\n/help - Эта справка\n/news - Последние новости")
    await update.message.reply_text(msg)

async def news_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
    report = await get_latest_news() # <<< ИЗМЕНЕНИЕ ЗДЕСЬ
    await update.message.reply_text(report, disable_web_page_preview=True)

# <<< ОБНОВЛЕННАЯ ВЕРСИЯ >>>
# <<< ОБНОВЛЕННАЯ ВЕРСИЯ, СПОСОБНАЯ ИСКАТЬ ПО ТЕМЕ >>>
async def get_latest_news(query: str | None = None, country="us", category="general", num=5) -> str:
    """
    Асинхронно получает новости.
    - Если 'query' указан, ищет по всему миру по этому ключевому слову (на всех языках).
    - Если 'query' не указан, возвращает главные новости для указанной страны.
    """
    if not NEWS_API_KEY:
        return "Ключ News API отсутствует."

    params = {"apiKey": NEWS_API_KEY, "pageSize": num}
    base_url = "https://newsapi.org/v2/"

    if query:
        logger.info(f"Выполняется поиск новостей по запросу: '{query}'")
        # Используем endpoint для поиска по ключевому слову
        endpoint = "everything"
        # <<< ИЗМЕНЕНИЕ ЗДЕСЬ: параметр 'language' полностью убран >>>
        params.update({"q": query, "sortBy": "publishedAt"})
    else:
        logger.info(f"Запрашиваются главные новости для страны '{country}'")
        # Используем endpoint для главных заголовков
        endpoint = "top-headlines"
        params.update({"country": country, "category": category})

    url = base_url + endpoint
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params)
            response.raise_for_status()

        articles = response.json().get("articles", [])
        if not articles:
            return f"Новости по запросу '{query}' не найдены." if query else "Новости не найдены."

        # Форматирование ответа
        title = f"Новости по запросу '{query}':" if query else "Последние новости:"
        headlines = [title] + [f"- {a.get('title','N/A')} ({a.get('source',{}).get('name','N/A')})\n  Читать: {a.get('url','#')}" for a in articles]
        return "\n\n".join(headlines)
        
    except httpx.HTTPStatusError as e:
        logger.error(f"Ошибка HTTP при запросе новостей: {e}")
        return f"Не удалось получить новости. Сервис вернул ошибку {e.response.status_code}."
    except Exception as e:
        logger.error(f"Неожиданная ошибка NewsAPI: {e}", exc_info=True)
        return "Произошла непредвиденная ошибка при получении новостей."
async def backfill_existing_user_name_variations():
    """
    Разовый скрипт для заполнения вариантов имен для всех пользователей,
    которые уже существуют в базе данных.
    """
    logger.info("Запуск скрипта миграции для заполнения вариантов имен существующих пользователей...")
    conn = get_db_connection()
    if not conn:
        logger.error("Миграция невозможна: нет подключения к БД.")
        return

    users_to_migrate = []
    try:
        with conn.cursor() as cursor:
            # Получаем всех пользователей из профилей
            sql = "SELECT UserID, FirstName, Username FROM UserProfiles"
            cursor.execute(sql)
            users_to_migrate = cursor.fetchall()
    except pyodbc.Error as e:
        logger.error(f"Ошибка при получении списка пользователей для миграции: {e}")
        conn.close()
        return

    logger.info(f"Найдено {len(users_to_migrate)} существующих пользователей для обработки.")

    # Для каждого пользователя запускаем уже существующую функцию генерации имен
    for user in users_to_migrate:
        logger.info(f"Обработка пользователя ID: {user.UserID}, Имя: {user.FirstName}")
        try:
            # Мы можем повторно использовать нашу функцию!
            await populate_name_variations_with_llm(user.UserID, user.FirstName, user.Username)
            # Добавим небольшую задержку, чтобы не перегружать API
            await asyncio.sleep(1) 
        except Exception as e:
            logger.error(f"Ошибка при миграции пользователя {user.UserID}: {e}")

    conn.close()
    logger.info("Миграция имен пользователей завершена.")
def main() -> None:
    """Главная функция, которая запускает бота."""
    initialize_database()
    if not TELEGRAM_BOT_TOKEN:
        logger.critical("Токен бота отсутствует. Запуск невозможен.")
        return

    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("news", news_command)) 
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_error_handler(error_handler) 

    logger.info("Запуск бота...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == '__main__':
    main()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/__init__.py ---
======================================================================

# This file intentionally left blank.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/combine_files.py ---
======================================================================

import os
import logging

# --- Configuration ---
# The root directory of your project source code.
# IMPORTANT: Please ensure this path is correct for your system.
PROJECT_ROOT = r'c:\Projects\EnkiBot\EnkiBot\EnkiBot'
# The name of the file that will contain all the combined code.
OUTPUT_FILENAME = 'combined_enkibot_python_source.txt' # Changed name to reflect content
# --- End Configuration ---

# Setup basic logging for the script itself.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_all_python_files(root_dir):
    """
    Walks through the directory structure and collects paths of .py files.
    
    Args:
        root_dir (str): The starting directory to walk.
        
    Returns:
        list: A sorted list of all .py file paths found.
    """
    file_paths = []
    for dirpath, _, filenames in os.walk(root_dir):
        # Exclude __pycache__ directories
        if '__pycache__' in dirpath:
            continue
        for filename in filenames:
            # --- MODIFICATION IS HERE ---
            # Only include files that end with the .py extension.
            if filename.endswith('.py'):
                file_paths.append(os.path.join(dirpath, filename))
                
    return sorted(file_paths)

def combine_project_files(root_dir, output_file):
    """
    Reads all .py files from a project directory and writes their contents
    into a single output file, with headers for each file.
    
    Args:
        root_dir (str): The root directory of the project to combine.
        output_file (str): The path to the single output file.
    """
    logging.info(f"Starting to combine only .py files from project root: '{root_dir}'")
    
    all_files = get_all_python_files(root_dir)
    
    if not all_files:
        logging.error(f"No .py files found in '{root_dir}'. Please check the PROJECT_ROOT path.")
        return

    try:
        # Open the output file in write mode with UTF-8 encoding
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for file_path in all_files:
                # Normalize path for consistent display
                normalized_path = file_path.replace('\\', '/')
                header = f"======================================================================\n"
                header += f"--- File: {normalized_path} ---\n"
                header += f"======================================================================\n\n"
                
                outfile.write(header)
                logging.info(f"Processing: {normalized_path}")
                
                try:
                    # Open the source file in read mode with UTF-8 encoding
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(infile.read())
                    # Add newlines for separation between files
                    outfile.write("\n\n\n")
                except Exception as e:
                    # Handle potential read errors
                    error_message = f"*** ERROR: Could not read file. Reason: {e} ***\n\n\n"
                    outfile.write(error_message)
                    logging.warning(f"Could not read {file_path}: {e}")

    except IOError as e:
        logging.error(f"Fatal error writing to output file '{output_file}': {e}")
        return

    logging.info(f"Successfully combined {len(all_files)} .py files into '{output_file}'.")

if __name__ == '__main__':
    if not os.path.isdir(PROJECT_ROOT):
        print(f"Error: The project directory '{PROJECT_ROOT}' was not found.")
        print("Please make sure the PROJECT_ROOT path is correct and you are running this script from a location that can access it.")
    else:
        combine_project_files(PROJECT_ROOT, OUTPUT_FILENAME)


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/config.py ---
======================================================================

﻿# ==================================================================================================
# === EnkiBot Configuration ===
# ==================================================================================================
# Central configuration file for API keys, model IDs, database settings, and other constants.
# It is best practice to load sensitive information from environment variables.
# ==================================================================================================

import os
import logging

# --- Core Bot Settings ---
TELEGRAM_BOT_TOKEN = os.getenv('ENKI_BOT_TOKEN')
# A list of bot nicknames that trigger a response in group chats.
BOT_NICKNAMES_TO_CHECK = ["enki", "enkibot", "энки", "энкибот", "бот", "bot"]

# --- Database Configuration (MS SQL Server) ---
SQL_SERVER_NAME = os.getenv('ENKI_BOT_SQL_SERVER_NAME')
SQL_DATABASE_NAME = os.getenv('ENKI_BOT_SQL_DATABASE_NAME')
DB_CONNECTION_STRING = (
    f"DRIVER={{ODBC Driver 17 for SQL Server}};"
    f"SERVER={SQL_SERVER_NAME};"
    f"DATABASE={SQL_DATABASE_NAME};"
    f"Trusted_Connection=yes;"
) if SQL_SERVER_NAME and SQL_DATABASE_NAME else None

# --- LLM Provider API Keys & Models ---
# OpenAI
OPENAI_API_KEY = os.getenv('ENKI_BOT_OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MODEL_ID', 'gpt-4o-mini')

# Groq
GROQ_API_KEY = os.getenv('ENKI_BOT_GROQ_API_KEY')
GROQ_MODEL_ID = os.getenv('ENKI_BOT_GROQ_MODEL_ID', 'llama3-8b-8192')
GROQ_ENDPOINT_URL = "https://api.groq.com/openai/v1/chat/completions"

# OpenRouter
OPENROUTER_API_KEY = os.getenv('ENKI_BOT_OPENROUTER_API_KEY')
OPENROUTER_MODEL_ID = os.getenv('ENKI_BOT_OPENROUTER_MODEL_ID', 'mistralai/mistral-7b-instruct:free')
OPENROUTER_ENDPOINT_URL = "https://openrouter.ai/api/v1/chat/completions"

# Google AI
GOOGLE_AI_API_KEY = os.getenv('ENKI_BOT_GOOGLE_AI_API_KEY')
GOOGLE_AI_MODEL_ID = os.getenv('ENKI_BOT_GOOGLE_AI_MODEL_ID', 'gemini-1.5-flash-latest')

# --- External Service API Keys ---
NEWS_API_KEY = os.getenv('ENKI_BOT_NEWS_API_KEY')
WEATHER_API_KEY = os.getenv('ENKI_BOT_WEATHER_API_KEY')

# --- Group Chat Access Control ---
ALLOWED_GROUP_IDS_STR = os.getenv('ENKI_BOT_ALLOWED_GROUP_IDS')
ALLOWED_GROUP_IDS = set()
if ALLOWED_GROUP_IDS_STR:
    try:
        ALLOWED_GROUP_IDS = set(int(id_str.strip()) for id_str in ALLOWED_GROUP_IDS_STR.split(','))
        if ALLOWED_GROUP_IDS:
            logging.info(f"Bot access is restricted to group IDs: {ALLOWED_GROUP_IDS}")
    except ValueError:
        logging.error(f"Invalid format for ENKI_BOT_ALLOWED_GROUP_IDS: '{ALLOWED_GROUP_IDS_STR}'. No group restrictions applied.")
else:
    logging.info("ENKI_BOT_ALLOWED_GROUP_IDS is not set. The bot will operate in all groups.")

# --- Language and Prompts Configuration ---
# The default language to use if detection fails.
DEFAULT_LANGUAGE = "en"
# The directory where language-specific prompt files (e.g., en.json, ru.json) are stored.
LANGUAGE_PACKS_DIR = os.path.join(os.path.dirname(__file__), 'lang')


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/__init__.py ---
======================================================================

# This file makes the 'core' directory a Python package.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/bot.py ---
======================================================================

﻿# enkibot/core/bot.py
# ==================================================================================================
# === EnkiBot - Main Bot Logic ===
# ==================================================================================================

# print(f"***** EXECUTING BOT.PY - VERSION: {__file__} *****") 

import logging
import json
import os
import re
import asyncio
from typing import Dict, Any, Optional, Tuple, List 

from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, ConversationHandler
from telegram.constants import ChatAction

from langdetect import detect, DetectorFactory, detect_langs # Import detect_langs
from langdetect.lang_detect_exception import LangDetectException

from enkibot import config 
from enkibot.utils.database import DatabaseManager
from enkibot.core.llm_services import LLMServices
from enkibot.modules.intent_recognizer import IntentRecognizer
from enkibot.modules.profile_manager import ProfileManager
from enkibot.modules.api_router import ApiRouter
from enkibot.modules.response_generator import ResponseGenerator

try:
    DetectorFactory.seed = 0 
except Exception as e:
    logging.warning(f"Could not seed DetectorFactory (this is okay if it's already seeded): {e}")

logger = logging.getLogger(__name__)
ASK_CITY = 1

class EnkiBot:
    def __init__(self, application: Application):
        logger.info("EnkiBot __init__ STARTING")

        self.application = application
        self.db_manager = DatabaseManager(config.DB_CONNECTION_STRING)
        self.llm_services = LLMServices(
            openai_api_key=config.OPENAI_API_KEY, openai_model_id=config.OPENAI_MODEL_ID,
            groq_api_key=config.GROQ_API_KEY, groq_model_id=config.GROQ_MODEL_ID, groq_endpoint_url=config.GROQ_ENDPOINT_URL,
            openrouter_api_key=config.OPENROUTER_API_KEY, openrouter_model_id=config.OPENROUTER_MODEL_ID, openrouter_endpoint_url=config.OPENROUTER_ENDPOINT_URL,
            google_ai_api_key=config.GOOGLE_AI_API_KEY, google_ai_model_id=config.GOOGLE_AI_MODEL_ID
        )
        self.intent_recognizer = IntentRecognizer(self.llm_services)
        self.profile_manager = ProfileManager(self.llm_services, self.db_manager)
        self.api_router = ApiRouter(
            weather_api_key=config.WEATHER_API_KEY, news_api_key=config.NEWS_API_KEY,
            llm_services=self.llm_services
        )
        self.response_generator = ResponseGenerator(self.llm_services, self.db_manager, self.intent_recognizer)

        self.language_packs: Dict[str, Dict[str, Any]] = {}
        self.llm_prompt_sets: Dict[str, Dict[str, Dict[str, str]]] = {}
        self.response_strings: Dict[str, Dict[str, str]] = {}
        
        self.current_lang: str = config.DEFAULT_LANGUAGE
        self.current_llm_prompt_sets: Dict[str, Dict[str, str]] = {}
        self.current_response_strings: Dict[str, str] = {}
        self.current_lang_pack_full: Dict[str, Any] = {}

        self._load_language_packs()
        self.pending_action_data: Dict[int, Dict[str, Any]] = {}
        logger.info("EnkiBot __init__ COMPLETED")

    def _load_language_packs(self):
        self.language_packs = {}
        self.llm_prompt_sets = {}
        self.response_strings = {}
        if not os.path.exists(config.LANGUAGE_PACKS_DIR):
            logger.error(f"Language packs directory not found: {config.LANGUAGE_PACKS_DIR}")
            try:
                os.makedirs(config.LANGUAGE_PACKS_DIR, exist_ok=True)
                logger.info(f"Created language packs directory: {config.LANGUAGE_PACKS_DIR}")
            except OSError as e:
                logger.error(f"Could not create language packs directory {config.LANGUAGE_PACKS_DIR}: {e}")
        for lang_file in os.listdir(config.LANGUAGE_PACKS_DIR):
            if lang_file.endswith(".json"):
                lang_code = lang_file[:-5]
                file_path = os.path.join(config.LANGUAGE_PACKS_DIR, lang_file)
                try:
                    with open(file_path, 'r', encoding='utf-8-sig') as f: 
                        pack_content = json.load(f)
                        self.language_packs[lang_code] = pack_content
                        self.llm_prompt_sets[lang_code] = pack_content.get("prompts", {})
                        self.response_strings[lang_code] = pack_content.get("responses", {})
                        logger.info(f"Successfully loaded language pack: {lang_code} from {file_path}")
                except json.JSONDecodeError as jde:
                    logger.error(f"Error decoding JSON from language file: {lang_file}. Error: {jde.msg} at line {jde.lineno} col {jde.colno} (char {jde.pos})", exc_info=False)
                except Exception as e:
                    logger.error(f"Error loading language file {lang_file}: {e}", exc_info=True)
        self._set_current_language_internals(config.DEFAULT_LANGUAGE)

    def _set_current_language_internals(self, lang_code_to_set: str):
        primary_fallback_lang = config.DEFAULT_LANGUAGE 
        secondary_fallback_lang = "ru"
        chosen_lang_code = lang_code_to_set

        if chosen_lang_code not in self.language_packs:
            logger.warning(f"Language pack for initially requested '{chosen_lang_code}' not found.")
            if primary_fallback_lang in self.language_packs:
                logger.info(f"Falling back to primary fallback: '{primary_fallback_lang}'.")
                chosen_lang_code = primary_fallback_lang
            elif secondary_fallback_lang in self.language_packs:
                logger.info(f"Primary fallback '{primary_fallback_lang}' not found. Falling back to secondary: '{secondary_fallback_lang}'.")
                chosen_lang_code = secondary_fallback_lang
            elif self.language_packs: 
                first_available = next(iter(self.language_packs))
                logger.error(f"Fallbacks ('{primary_fallback_lang}', '{secondary_fallback_lang}') not found. Using first available: '{first_available}'.")
                chosen_lang_code = first_available
            else: 
                logger.critical("CRITICAL: No language packs loaded at all. Bot will not function correctly.")
                self.current_lang = "none" 
                self.current_lang_pack_full = {}
                self.current_llm_prompt_sets = {}
                self.current_response_strings = {}
                return

        self.current_lang = chosen_lang_code
        self.current_lang_pack_full = self.language_packs.get(chosen_lang_code, {})
        self.current_llm_prompt_sets = self.llm_prompt_sets.get(chosen_lang_code, {})
        self.current_response_strings = self.response_strings.get(chosen_lang_code, {})
        
        if not self.current_llm_prompt_sets and not self.current_response_strings:
             logger.error(f"Language '{self.current_lang}' pack loaded, but it has no 'prompts' or 'responses' sections defined.")
        else:
            logger.info(f"Successfully set current language context to: '{self.current_lang}'")

    async def _create_and_load_language_pack(self, new_lang_code: str, update_context: Optional[Update] = None) -> bool:
        logger.info(f"Attempting to create language pack for new language: {new_lang_code}")
        english_pack_key = "en"
        if english_pack_key not in self.language_packs:
            logger.error(f"Cannot create new language pack: Source English ('{english_pack_key}') pack not found.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text("My apologies, I'm having trouble setting up support for this language right now as my base language files are missing.")
            return False

        english_pack_content_str = json.dumps(self.language_packs[english_pack_key], ensure_ascii=False, indent=2)
        target_language_name = new_lang_code 

        translation_system_prompt = (
             f"You are an expert translation AI. Your task is to translate a complete JSON language pack from English to {target_language_name} (language code: {new_lang_code}).\n"
            "You MUST maintain the original JSON structure and all original keys (e.g., \"prompts\", \"responses\", \"weather_conditions_map\", \"days_of_week\", and all keys within them). Only translate the string values associated with the keys.\n"
            "The output MUST be a single, valid JSON object and nothing else. Do not add any explanatory text, comments, or markdown before or after the JSON.\n"
            "Ensure all translated strings are appropriate for a friendly AI assistant and are natural-sounding in the target language. Pay special attention to escaping characters within JSON strings if necessary (e.g. double quotes inside a string should be \\\", newlines as \\n)."
        )
        translation_user_prompt = f"Translate the following English JSON language pack to {target_language_name} ({new_lang_code}):\n\n{english_pack_content_str}"
        
        messages_for_api = [{"role": "system", "content": translation_system_prompt}, {"role": "user", "content": translation_user_prompt}]
        response_format_arg = {"response_format": {"type": "json_object"}}
        
        translated_content_str: Optional[str] = None
        try:
            translated_content_str = await self.llm_services.call_openai_llm(
                messages_for_api, model_id=config.OPENAI_MODEL_ID, 
                temperature=0.1, max_tokens=4000, **response_format_arg
            )
        except Exception as e:
            logger.error(f"LLM call itself failed during translation for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self._get_response_string("language_pack_creation_failed_fallback"))
            return False

        if not translated_content_str:
            logger.error(f"LLM failed to provide a translation string for {new_lang_code}.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self._get_response_string("language_pack_creation_failed_fallback"))
            return False
        
        clean_response = translated_content_str.strip() 
        try:
            match = re.search(r"```json\s*(.*?)\s*```", clean_response, re.DOTALL | re.IGNORECASE)
            if match: clean_response = match.group(1).strip()
            else:
                if clean_response.startswith("```json"): clean_response = clean_response[7:]
                if clean_response.endswith("```"): clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            logger.debug(f"Attempting to parse cleaned LLM translation for {new_lang_code}: '{clean_response[:300]}...'")
            translated_pack_content = json.loads(clean_response) 
            
            if not all(k in translated_pack_content for k in ["prompts", "responses", "weather_conditions_map", "days_of_week"]):
                logger.error(f"Translated pack for {new_lang_code} is missing core top-level keys. Aborting save.")
                raise ValueError("Translated JSON missing core keys.")

            new_pack_path = os.path.join(config.LANGUAGE_PACKS_DIR, f"{new_lang_code}.json")
            with open(new_pack_path, 'w', encoding='utf-8') as f: 
                json.dump(translated_pack_content, f, ensure_ascii=False, indent=2)
            logger.info(f"Successfully created and saved new language pack: {new_lang_code}.json")
            
            self.language_packs[new_lang_code] = translated_pack_content
            self.llm_prompt_sets[new_lang_code] = translated_pack_content.get("prompts", {})
            self.response_strings[new_lang_code] = translated_pack_content.get("responses", {})
            logger.info(f"New language pack for {new_lang_code} is now available at runtime.")
            return True
        except json.JSONDecodeError as jde:
            logger.error(
                f"Failed to decode LLM translation JSON for {new_lang_code}. Error: {jde.msg} "
                f"at L{jde.lineno} C{jde.colno} (char {jde.pos}). "
                f"Nearby: '{clean_response[max(0, jde.pos-50):jde.pos+50]}'", 
                exc_info=False 
            )
            log_limit = 3000
            full_resp_to_log = translated_content_str 
            if len(full_resp_to_log) < log_limit: logger.debug(f"Full problematic translated content for {new_lang_code}:\n{full_resp_to_log}")
            else: logger.debug(f"Problematic translated content (first {log_limit} chars) for {new_lang_code}:\n{full_resp_to_log[:log_limit]}")
            if update_context and update_context.effective_message:
                await update_context.effective_message.reply_text(self._get_response_string("language_pack_creation_failed_fallback"))
            return False
        except Exception as e:
            logger.error(f"Error processing/saving new lang pack for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message: 
                await update_context.effective_message.reply_text(self._get_response_string("language_pack_creation_failed_fallback"))
            return False

    async def _detect_language_and_set_prompts(self, 
                                               current_message_text: Optional[str], 
                                               chat_id: Optional[int], # Added chat_id
                                               update_context: Optional[Update] = None) -> str:
        
        CONFIDENCE_THRESHOLD = 0.60  # Minimum probability for a detected language
        NUM_RECENT_MESSAGES_FOR_DETECTION = 3 
        MIN_AGGREGATED_TEXT_LENGTH = 20 # Min length of combined text to run detect_langs

        # Start with the current session's language or the primary default.
        candidate_lang_code = self.current_lang if self.current_lang != "none" else config.DEFAULT_LANGUAGE
        
        texts_for_detection: List[str] = []
        if current_message_text and current_message_text.strip():
            texts_for_detection.append(current_message_text.strip())

        # Fetch recent messages if current message is short or for general robustness
        if chat_id and (not texts_for_detection or len(texts_for_detection[0]) < 50): 
            logger.debug(f"Fetching last {NUM_RECENT_MESSAGES_FOR_DETECTION} messages from chat {chat_id} for lang detection.")
            try:
                if self.db_manager: # Ensure db_manager is initialized
                    recent_messages = await self.db_manager.get_recent_chat_texts(chat_id, limit=NUM_RECENT_MESSAGES_FOR_DETECTION)
                    if recent_messages:
                        texts_for_detection.extend(recent_messages) 
                        logger.debug(f"Fetched {len(recent_messages)} recent messages. Total texts for detection: {len(texts_for_detection)}")
                else:
                    logger.warning("db_manager not available in _detect_language_and_set_prompts.")
            except Exception as e:
                logger.error(f"Error fetching recent chat texts for language detection: {e}", exc_info=True)
        
        aggregated_text = " . ".join(reversed(texts_for_detection)).strip() # Most recent last
        
        detected_languages_with_probs: List[Any] = [] 
        attempted_new_detection_on_aggregated = False

        if aggregated_text and len(aggregated_text) >= MIN_AGGREGATED_TEXT_LENGTH:
            attempted_new_detection_on_aggregated = True
            try:
                # DetectorFactory.seed = 0 # Re-seeding here might be too frequent. Seed once at module level.
                detected_languages_with_probs = detect_langs(aggregated_text)
                logger.info(f"LangDetect results on aggregated text (len {len(aggregated_text)}): {detected_languages_with_probs}")
            except LangDetectException:
                logger.warning(f"LangDetectException on aggregated text. Current candidate: '{candidate_lang_code}'.")
            except Exception as e: 
                logger.error(f"Unexpected error during langdetect.detect_langs: {e}. Current candidate: '{candidate_lang_code}'.")
        else:
            logger.info(f"Aggregated text too short or empty ('{aggregated_text[:50]}...'). Using current candidate: '{candidate_lang_code}'.")

        # Process detected languages to choose a candidate
        chosen_lang_from_detection = None
        if detected_languages_with_probs:
            for lang_obj in sorted(detected_languages_with_probs, key=lambda x: x.prob, reverse=True): # Ensure sorted by prob
                lang_code_detected = lang_obj.lang
                lang_prob = lang_obj.prob
                logger.debug(f"Checking detected lang: {lang_code_detected} with prob: {lang_prob:.3f}")

                if lang_prob >= CONFIDENCE_THRESHOLD:
                    if lang_code_detected in self.language_packs:
                        logger.info(f"Prioritizing existing pack for detected language '{lang_code_detected}' (prob: {lang_prob:.3f}).")
                        chosen_lang_from_detection = lang_code_detected
                        break # Found a good existing pack
                    elif chosen_lang_from_detection is None: # No existing pack found yet for high confidence, take the first one
                        chosen_lang_from_detection = lang_code_detected
                        logger.info(f"High-confidence lang '{lang_code_detected}' (prob {lang_prob:.3f}) is first candidate for creation.")
                else:
                    logger.debug(f"Detected language '{lang_code_detected}' prob {lang_prob:.3f} below threshold {CONFIDENCE_THRESHOLD}. Stopping checks.")
                    break # Since list is sorted, subsequent probs will also be too low
        
        if chosen_lang_from_detection:
            candidate_lang_code = chosen_lang_from_detection
            logger.info(f"Candidate language after detection logic: '{candidate_lang_code}'")
        else:
            logger.info(f"No suitable language found from detection (or detection not run). Sticking with previous candidate: '{candidate_lang_code}'")


        # Attempt to set/create pack for the final candidate_lang_code
        if candidate_lang_code not in self.language_packs:
            # Only attempt creation if we had a basis for choosing this language (e.g., from detection)
            # or if the initial candidate_lang_code (current/default) was itself the one missing.
            if attempted_new_detection_on_aggregated or candidate_lang_code != (self.current_lang if self.current_lang != "none" else config.DEFAULT_LANGUAGE):
                logger.warning(f"Language '{candidate_lang_code}' pack not found. Attempting to create one.")
                if await self._create_and_load_language_pack(candidate_lang_code, update_context=update_context):
                    self._set_current_language_internals(candidate_lang_code)
                else:
                    logger.warning(f"Failed to create pack for '{candidate_lang_code}'. Applying prioritized fallbacks.")
                    self._set_current_language_internals(config.DEFAULT_LANGUAGE) # Triggers en -> ru -> first available
            else:
                # No new detection, current/default pack missing
                logger.info(f"Pack for current/default candidate '{candidate_lang_code}' is missing. Applying fallbacks directly.")
                self._set_current_language_internals(config.DEFAULT_LANGUAGE) # Start fallback chain
        else: # Pack for candidate_lang_code already exists
            self._set_current_language_internals(candidate_lang_code)
        
        return self.current_lang

    async def log_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        # logger.info("EnkiBot.log_message CALLED")
        if not update.message or not update.message.text or not update.effective_user: 
            logger.debug("log_message: Skipping due to missing message, text, or user.")
            return
        
        chat_id = update.effective_chat.id
        user = update.effective_user
        message = update.message

        if config.ALLOWED_GROUP_IDS and chat_id not in config.ALLOWED_GROUP_IDS:
            logger.debug(f"log_message: Skipping message from chat {chat_id}, not in allowed group IDs.")
            return

        action_taken = await self.db_manager.log_chat_message_and_upsert_user(
            chat_id=chat_id, user_id=user.id, username=user.username,
            first_name=user.first_name, last_name=user.last_name,
            message_id=message.message_id, message_text=message.text,
            preferred_language=self.current_lang 
        )
        logger.info(f"Message from user {user.id} logged. Profile action: {action_taken}.")

        name_var_prompts = self._get_llm_prompt_set("name_variation_generator")
        if action_taken and action_taken.lower() == "insert" and name_var_prompts:
            logger.info(f"New user {user.id} ({user.first_name}). Queuing name variation generation.")
            asyncio.create_task(self.profile_manager.populate_name_variations_with_llm(
                user_id=user.id, first_name=user.first_name, last_name=user.last_name, username=user.username,
                system_prompt=name_var_prompts["system"], 
                user_prompt_template=name_var_prompts.get("user","Generate for: {name_info}")
            ))

        profile_create_prompts = self._get_llm_prompt_set("profile_creator")
        profile_update_prompts = self._get_llm_prompt_set("profile_updater")
        if message.text and len(message.text.strip()) > 10:
            if profile_create_prompts and profile_update_prompts:
                logger.info(f"Message from user {user.id} meets criteria for profile analysis. Queuing task.")
                asyncio.create_task(self.profile_manager.analyze_and_update_user_profile(
                    user_id=user.id, message_text=message.text,
                    create_system_prompt=profile_create_prompts["system"],
                    create_user_prompt_template=profile_create_prompts.get("user","Analyze: {message_text}"),
                    update_system_prompt=profile_update_prompts["system"],
                    update_user_prompt_template=profile_update_prompts.get("user","Update based on: {message_text} with existing: {current_profile_notes}")
                ))
            else: 
                logger.warning(f"Profile prompts missing for lang '{self.current_lang}'. Skipping profile analysis for user {user.id}.")

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        # logger.info(f"DIAGNOSTIC (handle_message START): Current lang before detection: {self.current_lang} for chat {update.effective_chat.id if update.effective_chat else 'N/A'}")
        if not update.message or not update.message.text or not update.effective_chat or not update.effective_user: return None 
        
        # 1. Detect language & set context
        await self._detect_language_and_set_prompts(
            update.message.text, 
            chat_id=update.effective_chat.id,
            update_context=update
        )
        # logger.info(f"DIAGNOSTIC (handle_message after detect): Lang set to: {self.current_lang} for user {update.effective_user.id}")
        
        # 2. Log message
        if hasattr(self, 'log_message') and callable(getattr(self, 'log_message')): 
            await self.log_message(update, context)
        else: 
            logger.critical("CRITICAL: log_message method not found! Logging skipped.")
            if update.message: 
                 await update.message.reply_text("An internal error occurred (L). Please try again later.")
            return None

        # 3. Handle pending actions
        pending_action = self.pending_action_data.pop(update.effective_chat.id, None)
        if pending_action and pending_action["action_type"] == "ask_city_weather":
            city_name = update.message.text
            logger.info(f"Received city '{city_name}' for pending weather request.")
            await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
            weather_report = await self.api_router.get_weather_data( 
                location=city_name, forecast_type=pending_action["forecast_type"], 
                days=pending_action["days"], lang_pack_full=self.current_lang_pack_full )
            await update.message.reply_text(weather_report, reply_to_message_id=pending_action["original_message_id"])
            return ConversationHandler.END 

        user_msg_txt = update.message.text
        user_msg_txt_lower = user_msg_txt.lower()

        # 4. Check trigger
        if not await self._is_triggered(update, context, user_msg_txt_lower): return None

        # 5. Master Intent Classification
        master_intent_prompts = self._get_llm_prompt_set("master_intent_classifier")
        master_intent = "UNKNOWN_INTENT"
        if master_intent_prompts and "system" in master_intent_prompts:
            user_template_for_master = master_intent_prompts.get("user","{text_to_classify}")
            master_intent = await self.intent_recognizer.classify_master_intent(
                text=user_msg_txt, lang_code=self.current_lang,
                system_prompt=master_intent_prompts["system"], user_prompt_template=user_template_for_master )
        else: logger.error(f"Master intent classification prompt set missing/malformed for lang '{self.current_lang}'.")
        
        logger.info(f"Master Intent for '{user_msg_txt[:50]}...' classified as: {master_intent}")

        # 6. Route based on Master Intent
        if master_intent == "WEATHER_QUERY":
            state = await self._handle_weather_intent(update, context, user_msg_txt)
            return state 
        elif master_intent == "NEWS_QUERY": await self._handle_news_intent(update, context, user_msg_txt)
        elif master_intent == "MESSAGE_ANALYSIS_QUERY": await self._handle_message_analysis_query(update, context, user_msg_txt)
        elif master_intent in ["USER_PROFILE_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]:
            await self._handle_user_profile_query_or_general_chat(update, context, user_msg_txt, master_intent)
        else: 
            logger.warning(f"Unhandled master intent type: {master_intent}. Falling back to general.")
            await self._handle_user_profile_query_or_general_chat(update, context, user_msg_txt, "UNKNOWN_INTENT")
        return None
    
    def _get_llm_prompt_set(self, key: str) -> Optional[Dict[str, str]]:
        current_prompts_to_check = self.current_llm_prompt_sets
        prompt_set = current_prompts_to_check.get(key)
        primary_fallback_lang = config.DEFAULT_LANGUAGE
        secondary_fallback_lang = "ru"

        if not prompt_set: 
            logger.debug(f"LLM prompt set key '{key}' not in current lang '{self.current_lang}'. Trying '{primary_fallback_lang}'.")
            current_prompts_to_check = self.llm_prompt_sets.get(primary_fallback_lang, {})
            prompt_set = current_prompts_to_check.get(key)
            if not prompt_set and primary_fallback_lang != secondary_fallback_lang: 
                 logger.debug(f"LLM prompt set key '{key}' not in '{primary_fallback_lang}'. Trying '{secondary_fallback_lang}'.")
                 current_prompts_to_check = self.llm_prompt_sets.get(secondary_fallback_lang, {})
                 prompt_set = current_prompts_to_check.get(key)
        
        if not prompt_set:
            logger.error(f"LLM prompt set for key '{key}' ultimately not found.")
            return None
        if not isinstance(prompt_set, dict) or "system" not in prompt_set: 
            logger.error(f"LLM prompt set for key '{key}' (found in lang or fallback) is malformed: {prompt_set}")
            return None
        return prompt_set

    def _get_response_string(self, key: str, default_value: Optional[str] = None, **kwargs) -> str:
        raw_string = self.current_response_strings.get(key)
        lang_tried = self.current_lang
        primary_fallback_lang = config.DEFAULT_LANGUAGE
        secondary_fallback_lang = "ru"

        if raw_string is None: 
            lang_tried = primary_fallback_lang
            raw_string = self.response_strings.get(primary_fallback_lang, {}).get(key)
            if raw_string is None and primary_fallback_lang != secondary_fallback_lang: 
                lang_tried = secondary_fallback_lang
                raw_string = self.response_strings.get(secondary_fallback_lang, {}).get(key)

        if raw_string is None: 
            if default_value is not None: raw_string = default_value
            else: logger.error(f"Response string for key '{key}' ultimately not found. Using placeholder."); raw_string = f"[[Missing response: {key}]]"
        
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError as e:
            logger.error(f"Missing format key '{e}' in response string for key '{key}' (lang tried: {lang_tried}, raw: '{raw_string}')")
            english_raw = self.response_strings.get("en", {}).get(key, f"[[Format error & missing English for key: {key}]]")
            try: return english_raw.format(**kwargs) if kwargs else english_raw
            except KeyError: return f"[[Formatting error for response key: {key} - check placeholders]]"

    async def _is_triggered(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt_lower: str) -> bool:
        # ... (implementation with enhanced logging for group ID check from message #26) ...
        if not update.message or not context.bot:
            # logger.debug("_is_triggered: False (no message or bot context)") # Can be too verbose
            return False
        
        current_chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ['group', 'supergroup']

        if not is_group:
            # logger.debug(f"_is_triggered: True (private chat with user {update.effective_user.id if update.effective_user else 'Unknown'})")
            return True

        # --- Group Chat Logic ---
        # logger.info(f"DIAGNOSTIC (_is_triggered): Group Chat ID: {current_chat_id}, Allowed Set: {config.ALLOWED_GROUP_IDS}")
        if config.ALLOWED_GROUP_IDS: 
            if current_chat_id not in config.ALLOWED_GROUP_IDS:
                # logger.info(f"_is_triggered: False (group {current_chat_id} not in ALLOWED_GROUP_IDS: {config.ALLOWED_GROUP_IDS})")
                return False
        
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        is_at_mentioned = bool(bot_username_lower and f"@{bot_username_lower}" in user_msg_txt_lower)
        is_nickname_mentioned = False
        for nick in config.BOT_NICKNAMES_TO_CHECK:
            if re.search(r'\b' + re.escape(nick.lower()) + r'\b', user_msg_txt_lower, re.I):
                is_nickname_mentioned = True; break
        
        is_bot_mentioned = is_at_mentioned or is_nickname_mentioned
        is_reply_to_bot = (update.message.reply_to_message and 
                           update.message.reply_to_message.from_user and
                           update.message.reply_to_message.from_user.id == context.bot.id)
        
        final_trigger_decision = is_bot_mentioned or is_reply_to_bot
        # logger.info(f"_is_triggered (Group: {current_chat_id}): Text='{user_msg_txt_lower[:30]}...' @Mention={is_at_mentioned}, NickMention={is_nickname_mentioned}, ReplyToBot={is_reply_to_bot} => Trigger={final_trigger_decision}")
        return final_trigger_decision

    async def _handle_message_analysis_query(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> bool:
        # ... (implementation as in message #26, ensuring prompt fetching is robust) ...
        if not (update.message and update.message.reply_to_message and 
                update.message.reply_to_message.text and 
                update.message.reply_to_message.from_user and
                context.bot and update.message.reply_to_message.from_user.id != context.bot.id):
            return False
        logger.info("Processing MESSAGE_ANALYSIS_QUERY.")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        original_text = update.message.reply_to_message.text
        question_for_analysis = user_msg_txt
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        cleaned_question = user_msg_txt.lower()
        for nick in config.BOT_NICKNAMES_TO_CHECK + ([f"@{bot_username_lower}"] if bot_username_lower else []):
            cleaned_question = cleaned_question.replace(nick.lower(), "").strip()
        if len(cleaned_question) < 5: question_for_analysis = self._get_response_string("replied_message_default_question")
        analyzer_prompts = self._get_llm_prompt_set("replied_message_analyzer")
        if not analyzer_prompts : logger.error("Prompt set for replied message analysis is missing."); await update.message.reply_text(self._get_response_string("generic_error_message")); return True
        analysis_result = await self.response_generator.analyze_replied_message(
            original_text=original_text, user_question=question_for_analysis,
            system_prompt=analyzer_prompts["system"], user_prompt_template=analyzer_prompts.get("user") )
        await update.message.reply_text(analysis_result); return True

    async def _handle_weather_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        # ... (implementation as in message #26, ensure prompt fetching is robust) ...
        logger.info(f"Handling WEATHER_QUERY: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        weather_analysis_prompts = self._get_llm_prompt_set("weather_intent_analyzer")
        location_extract_prompts = self._get_llm_prompt_set("location_extractor")
        if not (weather_analysis_prompts and location_extract_prompts):
            logger.error("Weather intent prompts are missing or malformed."); await update.message.reply_text(self._get_response_string("generic_error_message")); return None 
        intent_data = await self.intent_recognizer.analyze_weather_request_with_llm(
            text=user_msg_txt, lang_code=self.current_lang, system_prompt=weather_analysis_prompts["system"], user_prompt_template=weather_analysis_prompts.get("user") )
        city = await self.intent_recognizer.extract_location_with_llm(
            text=user_msg_txt, lang_code=self.current_lang, system_prompt=location_extract_prompts["system"], user_prompt_template=location_extract_prompts.get("user") )
        if city:
            weather_report = await self.api_router.get_weather_data(location=city, forecast_type=intent_data.get("type", "current"), days=intent_data.get("days", 7), lang_pack_full=self.current_lang_pack_full )
            await update.message.reply_text(weather_report); return None 
        else:
            self.pending_action_data[update.effective_chat.id] = { "action_type": "ask_city_weather", "forecast_type": intent_data.get("type", "current"), "days": intent_data.get("days", 7), "original_message_id": update.message.message_id }
            await update.message.reply_text(self._get_response_string("weather_ask_city")); return ASK_CITY

    async def _handle_news_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        # ... (implementation as in message #26, ensure prompt fetching is robust) ...
        logger.info(f"Handling NEWS_QUERY: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        news_topic_prompts = self._get_llm_prompt_set("news_topic_extractor")
        if not news_topic_prompts: logger.error("News topic extraction prompt is missing or malformed."); await update.message.reply_text(self._get_response_string("generic_error_message")); return
        topic = await self.intent_recognizer.extract_news_topic_with_llm(
            text=user_msg_txt, lang_code=self.current_lang, system_prompt=news_topic_prompts["system"], user_prompt_template=news_topic_prompts.get("user") )
        news_report = await self.api_router.get_latest_news( query=topic, lang_code=self.current_lang, response_strings=self.current_response_strings )
        await update.message.reply_text(news_report, disable_web_page_preview=True)

    async def _handle_user_profile_query_or_general_chat(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str, master_intent: str) -> None:
        # ... (implementation as in message #26, ensure prompt fetching is robust) ...
        logger.info(f"Handling {master_intent}: '{user_msg_txt}'")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        main_orchestrator_prompts = self._get_llm_prompt_set("main_orchestrator")
        system_prompt_override = (main_orchestrator_prompts["system"] if main_orchestrator_prompts else "You are EnkiBot, a helpful AI assistant.")
        reply = await self.response_generator.get_orchestrated_llm_response(
            prompt_text=user_msg_txt, chat_id=update.effective_chat.id, user_id=update.effective_user.id, message_id=update.message.message_id, context=context, lang_code=self.current_lang,
            system_prompt_override=system_prompt_override, user_search_ambiguous_response_template=self._get_response_string("user_search_ambiguous_clarification"),
            user_search_not_found_response_template=self._get_response_string("user_search_not_found_in_db") )
        if reply: await update.message.reply_text(reply)
        else: await update.message.reply_text(self._get_response_string("llm_error_fallback"))
        
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.effective_user or not update.message: return
        self._set_current_language_internals(config.DEFAULT_LANGUAGE) # For commands, usually best to default
        await update.message.reply_html(self._get_response_string("start", user_mention=update.effective_user.mention_html()))

    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message: return
        self._set_current_language_internals(config.DEFAULT_LANGUAGE)
        await update.message.reply_text(self._get_response_string("help"))

    async def news_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message or not update.effective_chat: return
        self._set_current_language_internals(config.DEFAULT_LANGUAGE)
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        report = await self.api_router.get_latest_news(query=None, lang_code=self.current_lang, response_strings=self.current_response_strings)
        await update.message.reply_text(report, disable_web_page_preview=True)

    async def error_handler(self, update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
        logger.error(f'Update "{update}" caused error "{context.error}"', exc_info=True)
        if isinstance(update, Update) and update.effective_chat:
            try:
                error_responses = self.language_packs.get(config.DEFAULT_LANGUAGE, {}).get("responses", {})
                if not error_responses and self.language_packs: error_responses = next(iter(self.language_packs.values()), {}).get("responses", {})
                error_msg = error_responses.get("generic_error_message", "Oops! Something went very wrong on my end. I've logged the issue.")
                await context.bot.send_message(chat_id=update.effective_chat.id, text=error_msg)
            except Exception as e: logger.error(f"CRITICAL: Error sending error message to user: {e}", exc_info=True)
    
    def register_handlers(self, application: Application):
        application.add_handler(CommandHandler("start", self.start_command))
        application.add_handler(CommandHandler("help", self.help_command))
        application.add_handler(CommandHandler("news", self.news_command))
        conv_handler = ConversationHandler(
            entry_points=[MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message)],
            states={ ASK_CITY: [MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message)],},
            fallbacks=[], allow_reentry=True 
        )
        application.add_handler(conv_handler)
        application.add_error_handler(self.error_handler)
        logger.info("All Telegram handlers have been registered.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/llm_services.py ---
======================================================================

# enkibot/core/llm_services.py
# ==================================================================================================
# === EnkiBot LLM Services ===
# ==================================================================================================
import logging
import httpx
import openai # Direct import for openai.AsyncOpenAI
import asyncio
from typing import List, Dict, Optional, Any

logger = logging.getLogger(__name__)

class LLMServices:
    def __init__(self, openai_api_key: Optional[str], openai_model_id: str,
                 groq_api_key: Optional[str], groq_model_id: str, groq_endpoint_url: str,
                 openrouter_api_key: Optional[str], openrouter_model_id: str, openrouter_endpoint_url: str,
                 google_ai_api_key: Optional[str], google_ai_model_id: str):
        
        print("***** LLMServices __init__ STARTING *****")
        logger.info("LLMServices __init__ STARTING")
        
        self.openai_api_key = openai_api_key
        self.openai_model_id = openai_model_id 
        self.openai_async_client: Optional[openai.AsyncOpenAI] = None
        if self.openai_api_key:
            try:
                self.openai_async_client = openai.AsyncOpenAI(api_key=self.openai_api_key)
                logger.info("OpenAI AsyncClient initialized successfully.")
                print("INFO: OpenAI AsyncClient initialized successfully.")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI AsyncClient: {e}")
                print(f"ERROR: Failed to initialize OpenAI AsyncClient: {e}")
        else:
            logger.warning("OpenAI API key not provided. OpenAI calls will be disabled.")
            print("WARN: OpenAI API key not provided.")

        self.groq_api_key = groq_api_key
        self.groq_model_id = groq_model_id
        self.groq_endpoint_url = groq_endpoint_url
        if self.groq_api_key: print(f"INFO: Groq configured with key: {self.groq_api_key[:5]}...")
        else: print("WARN: Groq API key not provided.")

        self.openrouter_api_key = openrouter_api_key
        self.openrouter_model_id = openrouter_model_id
        self.openrouter_endpoint_url = openrouter_endpoint_url
        if self.openrouter_api_key: print(f"INFO: OpenRouter configured with key: {self.openrouter_api_key[:5]}...")
        else: print("WARN: OpenRouter API key not provided.")
        
        self.google_ai_api_key = google_ai_api_key
        self.google_ai_model_id = google_ai_model_id
        if self.google_ai_api_key: print(f"INFO: Google AI configured with key: {self.google_ai_api_key[:5]}...")
        else: print("WARN: Google AI API key not provided.")
        
        print("***** LLMServices __init__ COMPLETED *****")
        logger.info("LLMServices __init__ COMPLETED")

    def is_provider_configured(self, provider_name: str) -> bool:
        if provider_name == "openai":
            return bool(self.openai_async_client and self.openai_api_key)
        elif provider_name == "groq":
            return bool(self.groq_api_key and self.groq_endpoint_url and self.groq_model_id)
        elif provider_name == "openrouter":
            return bool(self.openrouter_api_key and self.openrouter_endpoint_url and self.openrouter_model_id)
        # Add Google AI check if implemented
        return False

    async def call_openai_llm(self, messages: List[Dict[str, str]], 
                              model_id: Optional[str] = None, 
                              temperature: float = 0.7, 
                              max_tokens: int = 2000, 
                              **kwargs) -> Optional[str]:
        print(f"DEBUG: Attempting call_openai_llm. Configured: {self.is_provider_configured('openai')}")
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not initialized or API key missing. Cannot make call.")
            print("WARN: OpenAI client not init or key missing in call_openai_llm.")
            return None
        
        actual_model_id = model_id or self.openai_model_id
        logger.info(f"Calling OpenAI (model: {actual_model_id}) with {len(messages)} messages.")
        print(f"INFO: Calling OpenAI (model: {actual_model_id}) messages_count: {len(messages)}")
        # logger.debug(f"OpenAI messages: {messages}") # Potentially very verbose

        call_params = {
            "model": actual_model_id, "messages": messages,
            "temperature": temperature, "max_tokens": max_tokens,
            **kwargs 
        }
        try:
            print(f"DEBUG: Before OpenAI completions.create with params: {call_params.get('model')}, temp: {call_params.get('temperature')}")
            completion = await self.openai_async_client.chat.completions.create(**call_params)
            print(f"DEBUG: OpenAI completion object received: {type(completion)}")
            if completion.choices and completion.choices[0].message and completion.choices[0].message.content:
                response_content = completion.choices[0].message.content.strip()
                print(f"INFO: OpenAI successful response (first 50 chars): {response_content[:50]}")
                return response_content
            logger.warning(f"OpenAI call to {actual_model_id} returned no content or unexpected structure. Choices: {completion.choices}")
            print(f"WARN: OpenAI call to {actual_model_id} no content. Choices: {completion.choices}")
            return None
        except openai.APIStatusError as e: # More specific error type
            logger.error(f"OpenAI API Status Error (model: {actual_model_id}): HTTP Status {e.status_code} - {e.message}", exc_info=False)
            print(f"ERROR: OpenAI API Status Error (model: {actual_model_id}): HTTP Status {e.status_code} - {e.message}")
            logger.debug(f"OpenAI API Full Status Error Details: {e.response.text if e.response else 'No response body'}")
            return None
        except openai.APIError as e: 
            logger.error(f"OpenAI API Error (model: {actual_model_id}): {e.message}", exc_info=False)
            print(f"ERROR: OpenAI API Error (model: {actual_model_id}): {e.message}")
            logger.debug(f"OpenAI API Full Error Details: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error with OpenAI API (model: {actual_model_id}): {e}", exc_info=True)
            print(f"ERROR: Unexpected OpenAI error (model: {actual_model_id}): {e}")
            return None

    async def call_llm_api(self, provider_name: str, api_key: Optional[str], endpoint_url: Optional[str], 
                           model_id: str, messages: List[Dict[str, str]], 
                           temperature: float = 0.7, max_tokens: int = 2000) -> Optional[str]:
        print(f"DEBUG: Attempting call_llm_api for {provider_name}. Key: {'Set' if api_key else 'Not Set'}")
        if not api_key or not endpoint_url:
            logger.warning(f"{provider_name} not configured (key or URL missing). Skipping call.")
            print(f"WARN: {provider_name} not configured in call_llm_api.")
            return None
        
        logger.info(f"Calling {provider_name} ({model_id}) with {len(messages)} messages.")
        print(f"INFO: Calling {provider_name} ({model_id}) messages_count: {len(messages)}")
        # logger.debug(f"{provider_name} messages: {messages}") # Potentially verbose

        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        if provider_name.lower() == "openrouter": 
            headers.update({"HTTP-Referer": "http://localhost:8000", "X-Title": "EnkiBot"}) 

        payload = {"model": model_id, "messages": messages, "max_tokens": max_tokens, "temperature": temperature}
        
        try:
            print(f"DEBUG: Before {provider_name} POST to {endpoint_url}. Model: {model_id}")
            async with httpx.AsyncClient() as client:
                resp = await client.post(endpoint_url, json=payload, headers=headers, timeout=30.0)
            print(f"DEBUG: {provider_name} response status: {resp.status_code}")
            resp.raise_for_status()
            data = resp.json()
            if data.get("choices") and data["choices"][0].get("message") and data["choices"][0]["message"].get("content"):
                response_content = data["choices"][0]["message"]["content"].strip()
                print(f"INFO: {provider_name} successful response (first 50 chars): {response_content[:50]}")
                return response_content
            logger.warning(f"{provider_name} call to {model_id} returned no content or unexpected structure. Data: {data.get('choices')}")
            print(f"WARN: {provider_name} call to {model_id} no content. Data: {data.get('choices')}")
            return None
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error for {provider_name} ({model_id}): {e.response.status_code} - {e.response.text}", exc_info=False)
            print(f"ERROR: HTTP Error for {provider_name} ({model_id}): {e.response.status_code} - {e.response.text}")
            logger.debug(f"{provider_name} Full Error Response Content: {e.response.content}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error with {provider_name} API ({model_id}): {e}", exc_info=True)
            print(f"ERROR: Unexpected {provider_name} error ({model_id}): {e}")
            return None

    async def race_llm_calls(self, messages: List[Dict[str, str]]) -> Optional[str]:
        tasks = []
        # Store provider name with task for better logging
        task_info: List[Tuple[asyncio.Task, str]] = []

        print("DEBUG: Preparing tasks for race_llm_calls.")
        if self.is_provider_configured("openai"):
            task = asyncio.create_task(self.call_openai_llm(messages, model_id=self.openai_model_id))
            task_info.append((task, "OpenAI"))
            print("DEBUG: OpenAI task created for race.")
        if self.is_provider_configured("groq"):
            task = asyncio.create_task(self.call_llm_api("Groq", self.groq_api_key, self.groq_endpoint_url, self.groq_model_id, messages))
            task_info.append((task, "Groq"))
            print("DEBUG: Groq task created for race.")
        if self.is_provider_configured("openrouter"):
            task = asyncio.create_task(self.call_llm_api("OpenRouter", self.openrouter_api_key, self.openrouter_endpoint_url, self.openrouter_model_id, messages))
            task_info.append((task, "OpenRouter"))
            print("DEBUG: OpenRouter task created for race.")
        
        if not task_info:
            logger.warning("No LLM providers configured for racing calls.")
            print("WARN: No LLM providers for race_llm_calls.")
            return None

        logger.info(f"Racing LLM calls to: {[name for _, name in task_info]}")
        print(f"INFO: Racing LLM calls to: {[name for _, name in task_info]}")
        
        for future, provider_name in [(ti[0], ti[1]) for ti in task_info]: # Iterate through original task_info list
            # This inner loop structure with as_completed is a bit off.
            # Correct way: iterate through as_completed(just the tasks)
            # then find which provider it was. Simpler:
            pass # Will be rewritten below

        # Corrected loop for asyncio.as_completed
        tasks_only = [ti[0] for ti in task_info]
        for future in asyncio.as_completed(tasks_only):
            provider_name_for_log = "Unknown"
            # Find the provider name for the completed future
            for task_obj, name_str in task_info:
                if task_obj == future:
                    provider_name_for_log = name_str
                    break
            try:
                result = await future
                if result and result.strip():
                    logger.info(f"Successful response from {provider_name_for_log} in race.")
                    print(f"INFO: Successful response from {provider_name_for_log} in race.")
                    return result.strip()
                else:
                    # This case (result is None or empty) is already logged in individual call methods
                    logger.warning(f"{provider_name_for_log} returned no content in race.")
                    print(f"WARN: {provider_name_for_log} returned no content in race.")
            except Exception as e: 
                logger.warning(f"Provider {provider_name_for_log} failed in race_llm_calls (error should be logged by provider's method): {type(e).__name__}")
                print(f"WARN: Provider {provider_name_for_log} failed in race: {type(e).__name__}")
        
        logger.error("All LLM providers failed or returned no content in race_llm_calls.")
        print("ERROR: All LLM providers failed in race_llm_calls.")
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/__init__.py ---
======================================================================

*** ERROR: Could not read file. Reason: 'utf-8' codec can't decode byte 0xf6 in position 149: invalid start byte ***


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/module_tester.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Module Tester (Placeholder) ===
# ==================================================================================================
# This module will provide the framework for rigorously evaluating evolved
# variants of EnkiBot's modules and prompts.
# It will be responsible for:
# - Executing tests in a secure, sandboxed environment.
# - Running benchmark tests for Python modules (e.g., unit tests, performance tests).
# - Evaluating LLM prompt effectiveness using frameworks like LLM-as-a-Judge.
# - Collecting and returning detailed performance metrics to the coordinator.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

def test_variant(parent_variant, modification):
    """
    Tests a new, modified variant of a module or prompt.

    Args:
        parent_variant: The original version of the bot component.
        modification: The proposed change to be applied.

    Returns:
        A tuple containing the new child variant and its performance data.
    """
    logger.info(f"Testing a new variant with modification: {modification} (mock).")
    # In the future, this function would:
    # 1. Apply the modification in a sandboxed environment.
    # 2. Run a suite of tests (unit, integration, performance).
    # 3. Evaluate against the multi-objective fitness function.
    # 4. Return the results.
    
    mock_performance_data = {"task_success": 0.95, "efficiency": 120, "safety_score": 1.0}
    
    # The new variant would be a representation of the modified code/prompt
    new_child_variant = {"id": "variant-002", "parent": "variant-001", "modification": modification}
    
    return new_child_variant, mock_performance_data


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/self_improvement_coordinator.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Self-Improvement Coordinator (Placeholder) ===
# ==================================================================================================
# This module will serve as the central nervous system for EnkiBot's evolution.
# It will be responsible for:
# - Orchestrating the evolutionary loop: selection, modification, evaluation.
# - Managing the "Agent Variant Archive" of different EnkiBot versions.
# - Triggering the modification of Python modules and LLM prompts.
# - Recording performance metadata to guide the evolutionary process.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

class SelfImprovementCoordinator:
    def __init__(self):
        logger.info("Self-Improvement Coordinator initialized (Placeholder).")
        # In the future, this will initialize the Agent Variant Archive connection.
        self.agent_variant_archive = {}

    def run_evolutionary_cycle(self):
        """
        Executes a single cycle of selection, modification, and evaluation.
        """
        logger.info("Executing a mock evolutionary cycle...")
        # 1. Select parent variant(s) from the archive.
        # parent = self.select_parent()

        # 2. Propose modifications to code or prompts.
        # modification = self.propose_modification(parent)

        # 3. Create and test the new child variant.
        # child_variant, performance_data = module_tester.test_variant(parent, modification)

        # 4. Add the new variant and its performance data to the archive.
        # self.archive_variant(child_variant, performance_data)
        logger.info("Mock evolutionary cycle complete.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/__init__.py ---
======================================================================

# This directory holds the language packs (JSON files) for EnkiBot.
# Each file corresponds to a language code (e.g., 'en', 'ru') and contains
# all user-facing strings and system prompts for that language.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/main.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Main Entry Point ===
# ==================================================================================================
import os
import logging
import asyncio
from typing import Optional 
from telegram import Update # <--- IMPORT Update HERE
from telegram.ext import Application 

from enkibot import config
from enkibot.utils.logging_config import setup_logging
from enkibot.utils.database import initialize_database
from enkibot.core.bot import EnkiBot
# ... (rest of your main.py, the run_backfill_async and main functions)

# Logger will be configured by setup_logging
logger: Optional[logging.Logger] = None 

async def run_backfill_async(): 
    local_logger = logging.getLogger(__name__ + ".backfill") 
    local_logger.info("Starting one-time script to backfill name variations for existing users...")
    # ... (rest of the backfill logic, keeping it commented out for now as it needs careful handling)
    # from enkibot.utils.database import DatabaseManager
    # from enkibot.core.llm_services import LLMServices
    # from enkibot.modules.profile_manager import ProfileManager
    # ...
    local_logger.info("Backfill script (manual run from main.py) finished or was skipped.")

def clear_terminal():
    os.system('cls' if os.name == 'nt' else 'clear')

def main() -> None:
    global logger 
    clear_terminal()
    setup_logging() 
    logger = logging.getLogger(__name__) 

    logger.info("Initializing database schema...")
    initialize_database()

    if not config.TELEGRAM_BOT_TOKEN:
        logger.critical("FATAL: TELEGRAM_BOT_TOKEN missing. Bot cannot start.")
        return

    # To run the backfill (do this once, carefully, and ensure prompts are correctly sourced):
    # logger.info("Attempting to run async backfill script...")
    # try:
    #     asyncio.run(run_backfill_async())
    # except RuntimeError as e:
    #     if "cannot be called from a running event loop" in str(e).lower():
    #         logger.warning(f"Backfill script not run: {e}. Consider running it separately if needed.")
    #     else:
    #         logger.error(f"Error during backfill execution: {e}", exc_info=True)
    # except Exception as e:
    #     logger.error(f"Error during backfill execution: {e}", exc_info=True)
    # logger.info("Continuing with bot startup...")

    try:
        logger.info("Initializing Telegram Application...")
        application = Application.builder().token(config.TELEGRAM_BOT_TOKEN).build()
        
        logger.info("Initializing EnkiBot...")
        enkibot_instance = EnkiBot(application) 
        enkibot_instance.register_handlers(application) 

        logger.info("Starting EnkiBot polling...")
        # Now Update is defined
        application.run_polling(allowed_updates=Update.ALL_TYPES) 
        logger.info("EnkiBot has stopped.")
    except Exception as e:
        logger.critical(f"Unrecoverable error during bot setup or run: {e}", exc_info=True)

if __name__ == '__main__':
    main()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/__init__.py ---
======================================================================

# This file intentionally left blank.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/api_router.py ---
======================================================================

﻿# ==================================================================================================
# === EnkiBot API Router ===
# ==================================================================================================
import logging
import httpx 
from datetime import datetime
from typing import Dict, Any, Optional

from enkibot.core.llm_services import LLMServices 

logger = logging.getLogger(__name__)

class ApiRouter:
    def __init__(self, weather_api_key: str | None, news_api_key: str | None, llm_services: LLMServices):
        self.weather_api_key = weather_api_key
        self.news_api_key = news_api_key
        self.llm_services = llm_services
        self.lang_to_country_map = {
            "en": "us", "ru": "ru", "de": "de", "fr": "fr", "es": "es", 
            "it": "it", "ja": "jp", "ko": "kr", "zh": "cn", "bg": "bg",
            "ua": "ua", "pl": "pl", "tr": "tr", "pt": "pt", # "br" might be better for pt for news
        }
        self.default_news_country = "us"

    def _get_localized_response(self, response_strings: Dict[str, str], key: str, default_value: str, **kwargs) -> str:
        raw_string = response_strings.get(key, default_value)
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError as e:
            logger.error(f"Missing format key '{e}' in response string for key '{key}'. Raw: '{raw_string}'")
            return default_value # Or a more generic error placeholder

    async def get_weather_data(self, location: str, forecast_type: str = 'current', days: int = 7, 
                               lang_pack_full: Optional[Dict[str, Any]] = None) -> str:
        
        response_strings = lang_pack_full.get("responses", {}) if lang_pack_full else {}
        weather_conditions_map = lang_pack_full.get("weather_conditions_map", {}) if lang_pack_full else {}
        days_of_week_map = lang_pack_full.get("days_of_week", {}) if lang_pack_full else {}

        if not self.weather_api_key:
            logger.warning("Weather API key is not configured.")
            return self._get_localized_response(response_strings, "weather_api_key_missing", "Weather service: API key missing.")

        api_lang = self._get_localized_response(response_strings, "api_lang_code_openweathermap", "en")

        if forecast_type == 'current':
            url, params = "https://api.openweathermap.org/data/2.5/weather", {"q": location, "appid": self.weather_api_key, "units": "metric", "lang": api_lang}
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(url, params=params); response.raise_for_status()
                data = response.json()
                city = data.get("name", location)
                desc_key = data["weather"][0].get("description", "unknown_condition").lower().replace(" ", "_")
                desc = weather_conditions_map.get(desc_key, data["weather"][0].get("description", "N/A").capitalize())
                
                return "\n".join([
                    self._get_localized_response(response_strings, "weather_report_intro_current", "Current weather in {city}:", city=city),
                    self._get_localized_response(response_strings, "weather_current_conditions", "  - Currently: {description}", description=desc),
                    self._get_localized_response(response_strings, "weather_temperature", "  - Temperature: {temp:.1f}°C (feels like {feels_like:.1f}°C)", temp=data["main"]["temp"], feels_like=data["main"]["feels_like"]),
                    self._get_localized_response(response_strings, "weather_wind", "  - Wind: {wind_speed:.1f} m/s", wind_speed=data["wind"]["speed"])
                ])
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404: return self._get_localized_response(response_strings, "weather_city_not_found", "Sorry, I couldn't find the city '{location}'.", location=location)
                logger.error(f"HTTP error current weather {location}: {e}")
                return self._get_localized_response(response_strings, "weather_server_error", "Could not get weather data due to a server error.")
            except Exception as e:
                logger.error(f"Unexpected error current weather: {e}", exc_info=True)
                return self._get_localized_response(response_strings, "weather_unexpected_error", "An unexpected error occurred while fetching weather.")

        elif forecast_type == 'forecast':
            url, params = "https://api.openweathermap.org/data/2.5/forecast", {"q": location, "appid": self.weather_api_key, "units": "metric", "lang": api_lang, "cnt": 40}
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(url, params=params); response.raise_for_status()
                data = response.json()
                city = data.get("city", {}).get("name", location)
                if not data.get("list"): return self._get_localized_response(response_strings, "weather_forecast_unavailable", "Forecast data unavailable for '{location}'.", location=location)

                daily_summary = {}
                for item in data["list"]:
                    day_str = datetime.fromtimestamp(item["dt"]).strftime('%Y-%m-%d')
                    if day_str not in daily_summary:
                        eng_day_name = datetime.fromtimestamp(item["dt"]).strftime('%A')
                        daily_summary[day_str] = {'day_name': days_of_week_map.get(eng_day_name, eng_day_name), 'temps': [], 'descs': set()}
                    daily_summary[day_str]['temps'].append(item['main']['temp'])
                    desc_key = item['weather'][0].get("description", "unknown_condition").lower().replace(" ", "_")
                    daily_summary[day_str]['descs'].add(weather_conditions_map.get(desc_key, item['weather'][0].get("description", "").capitalize()))
                
                report = [self._get_localized_response(response_strings, "weather_report_intro_forecast", "Weather forecast for {city}:", city=city)]
                processed_days = 0
                for day_str_sorted in sorted(daily_summary.keys()):
                    if processed_days >= days: break
                    day_data = daily_summary[day_str_sorted]
                    avg_temp = sum(day_data['temps']) / len(day_data['temps'])
                    # Prioritize common descriptions or just join them
                    desc_text = ", ".join(filter(None, day_data['descs'])) or "N/A"
                    report.append(self._get_localized_response(response_strings, "weather_forecast_day_item", "  - {day_name}: {temp:.0f}°C, {description}", day_name=day_data['day_name'], temp=avg_temp, description=desc_text))
                    processed_days += 1
                return "\n".join(report)
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404: return self._get_localized_response(response_strings, "weather_city_not_found_forecast", "Sorry, I couldn't find '{location}' for the forecast.", location=location)
                logger.error(f"HTTP error forecast {location}: {e}")
                return self._get_localized_response(response_strings, "weather_server_error_forecast", "Could not get forecast data: server error.")
            except Exception as e:
                logger.error(f"Unexpected error forecast: {e}", exc_info=True)
                return self._get_localized_response(response_strings, "weather_unexpected_error_forecast", "Unexpected error fetching forecast.")
        return self._get_localized_response(response_strings, "weather_unknown_type", "Unknown weather request type.")

    async def get_latest_news(self, query: str | None = None, lang_code: str = "en", 
                              num: int = 5, response_strings: Optional[Dict[str, str]] = None) -> str:
        if not response_strings: response_strings = {} # Ensure it's a dict
        if not self.news_api_key:
            logger.warning("News API key is not configured.")
            return self._get_localized_response(response_strings, "news_api_key_missing", "News service: API key missing.")

        params: Dict[str, Any] = {"apiKey": self.news_api_key, "pageSize": num}
        base_url, endpoint = "https://newsapi.org/v2/", ""

        if query:
            logger.info(f"Fetching news for query: '{query}', language: {lang_code}")
            endpoint, params["q"], params["language"], params["sortBy"] = "everything", query, lang_code, "relevancy"
        else:
            country = self.lang_to_country_map.get(lang_code, self.default_news_country)
            logger.info(f"Fetching top headlines for country: '{country}' (from lang: {lang_code})")
            endpoint, params["country"], params["category"] = "top-headlines", country, "general"
        
        url = base_url + endpoint
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(url, params=params); logger.debug(f"NewsAPI URL: {resp.url}"); resp.raise_for_status()
            data, articles = resp.json(), resp.json().get("articles", [])
            logger.info(f"NewsAPI {len(articles)} articles (total: {data.get('totalResults')}) for {params}")

            if not articles:
                return self._get_localized_response(response_strings, "news_api_no_articles" if query else "news_api_no_general_articles", 
                                                 "No news found for '{query}'." if query else "No general news found.", query=query)

            title = self._get_localized_response(response_strings, "news_report_title_topic" if query else "news_report_title_general",
                                              "News on '{topic}':" if query else "Latest News:", topic=query)
            headlines = [title] + [
                f"- {a.get('title','N/A')} ({a.get('source',{}).get('name','N/A')})\n  {self._get_localized_response(response_strings, 'news_read_more', 'Read: {url}', url=a.get('url','#'))}" 
                for a in articles
            ]
            return "\n\n".join(headlines)
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error news ({url}): {e.response.status_code} - {e.response.text}")
            return self._get_localized_response(response_strings, "news_api_error", "Could not fetch news. Service error {status_code}.", status_code=e.response.status_code)
        except Exception as e:
            logger.error(f"Unexpected error news: {e}", exc_info=True)
            return self._get_localized_response(response_strings, "news_unexpected_error", "Unexpected error fetching news.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/base_module.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Base Module ===
# ==================================================================================================
# This file is intended to hold an Abstract Base Class (ABC) for all functional modules.
# In a future evolution, all modules (e.g., IntentRecognizer, FactExtractor) would inherit
# from this class to ensure a consistent interface, for example, requiring an `execute` method.
# For now, it serves as a structural placeholder.
# ==================================================================================================

class BaseModule:
    """
    Abstract Base Class for all EnkiBot modules.
    """
    def __init__(self, name: str):
        self.name = name

    def execute(self, *args, **kwargs):
        """
        The main method to be implemented by all subclasses.
        """
        raise NotImplementedError("Each module must implement the 'execute' method.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/fact_extractor.py ---
======================================================================

﻿# ==================================================================================================
# === EnkiBot Fact Extractor ===
# ==================================================================================================
# This module uses linguistic rules and morphological analysis to extract specific
# pieces of information from user text, such as a person's name mentioned in a query.
# This is faster and cheaper than using an LLM for simple, well-defined extraction tasks.
# ==================================================================================================

import logging
import re
import pymorphy3

logger = logging.getLogger(__name__)

# Initialize the morphological analyzer for Russian.
try:
    morph = pymorphy3.MorphAnalyzer()
except Exception as e:
    logger.error(f"Could not initialize pymorphy3 MorphAnalyzer: {e}. Fact extraction might fail.")
    morph = None

def find_user_search_query_in_text(text: str) -> str | None:
    """
    Analyzes text by lemmatizing words to their base form and looks for a combination
    of trigger words and prepositions to extract a user name.

    This is a classic Natural Language Processing (NLP) technique for entity extraction.

    Args:
        text: The user's message text.

    Returns:
        The extracted name as a string, or None if no name is found.
    """
    if not morph:
        logger.warning("Morphological analyzer not available. Skipping user search query extraction.")
        return None
        
    # Dictionaries of trigger word lemmas (base forms).
    # This makes the system robust to different word forms (e.g., 'tell', 'tells', 'told').
    TELL_LEMMAS = {'рассказать', 'поведать', 'сообщить', 'описать'}
    INFO_LEMMAS = {'информация', 'инфо', 'справка', 'досье', 'данные'}
    WHO_LEMMAS = {'кто', 'что'}
    EXPLAIN_LEMMAS = {'пояснить', 'объяснить'}
    REMEMBER_LEMMAS = {'помнить', 'напомнить'}

    # Prepositions that typically follow trigger words before a name.
    PREPOSITIONS = {'о', 'про', 'за', 'на', 'по'}

    # Split the text into words
    words = re.findall(r"[\w'-]+", text.lower())
    
    for i, word in enumerate(words):
        try:
            # Get the lemma (normal form) of the word
            lemma = morph.parse(word)[0].normal_form
            
            # Check if the lemma is one of our triggers
            is_trigger = (lemma in TELL_LEMMAS or
                          lemma in INFO_LEMMAS or
                          lemma in WHO_LEMMAS or
                          lemma in EXPLAIN_LEMMAS or
                          lemma in REMEMBER_LEMMAS)

            if is_trigger:
                # We found a trigger word. The name should follow it.
                start_index = i + 1
                
                # If the next word is a preposition, skip it.
                if start_index < len(words) and words[start_index] in PREPOSITIONS:
                    start_index += 1
                
                # Everything that follows (up to 3 words) is considered the name.
                if start_index < len(words):
                    # Capture 1 to 3 words after the trigger/preposition.
                    name_parts = words[start_index : start_index + 3]
                    extracted_name = " ".join(name_parts)
                    logger.info(f"Extracted potential user search query: '{extracted_name}'")
                    return extracted_name

        except Exception as e:
            logger.error(f"Error during lemmatization of word '{word}': {e}")
            continue
            
    return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/intent_recognizer.py ---
======================================================================

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
print(f"%%%%% EXECUTING INTENT_RECOGNIZER.PY - VERSION FROM: {__file__} %%%%%") 

import logging
import json
from typing import Dict, Any, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices 

logger = logging.getLogger(__name__)

class IntentRecognizer:
    # <<<--- DIAGNOSTIC PRINT IR-2: INSIDE CLASS DEFINITION --- >>>
    print("%%%%% IntentRecognizer CLASS DEFINITION BEING PARSED %%%%%")

    def __init__(self, llm_services: 'LLMServices'): 
        # <<<--- DIAGNOSTIC PRINT IR-3: START OF IntentRecognizer __INIT__ --- >>>
        print("%%%%% IntentRecognizer __init__ STARTING %%%%%")
        logger.info("IntentRecognizer __init__ STARTING")
        self.llm_services = llm_services
        
        # <<< --- DIAGNOSTIC PRINT IR-4: END OF IntentRecognizer __INIT__ --- >>>
        if hasattr(self, 'analyze_weather_request_with_llm') and callable(getattr(self, 'analyze_weather_request_with_llm')):
            logger.info("DIAGNOSTIC (IntentRecognizer.__init__): self.analyze_weather_request_with_llm IS a callable attribute.")
        else:
            logger.error("DIAGNOSTIC (IntentRecognizer.__init__): self.analyze_weather_request_with_llm IS MISSING or NOT CALLABLE!")
        if hasattr(self, 'classify_master_intent') and callable(getattr(self, 'classify_master_intent')):
            logger.info("DIAGNOSTIC (IntentRecognizer.__init__): self.classify_master_intent IS a callable attribute.")
        else:
            logger.error("DIAGNOSTIC (IntentRecognizer.__init__): self.classify_master_intent IS MISSING or NOT CALLABLE!")
        print("%%%%% IntentRecognizer __init__ COMPLETED %%%%%")
        logger.info("IntentRecognizer __init__ COMPLETED")


    async def classify_master_intent(self, text: str, lang_code: str, 
                                     system_prompt: str, user_prompt_template: str) -> str:
        # ... (method content as previously provided in message #22)
        logger.info(f"Classifying master intent (lang: {lang_code}): '{text[:100]}...'")
        user_prompt = user_prompt_template.format(text_to_classify=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        if self.llm_services.openai_model_id and \
           any(model_prefix in self.llm_services.openai_model_id for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}

        classified_intent_value = "UNKNOWN_INTENT" 

        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, temperature=0.0, max_tokens=100,
                **response_format_arg 
            )
            if completion_str:
                try:
                    clean_comp_str = completion_str.strip()
                    if clean_comp_str.startswith("```json"):
                        clean_comp_str = clean_comp_str[7:]
                    if clean_comp_str.endswith("```"):
                        clean_comp_str = clean_comp_str[:-3]
                    
                    data = json.loads(clean_comp_str.strip())
                    intent_from_json = data.get("intent", data.get("INTENT")) 

                    if intent_from_json and isinstance(intent_from_json, str):
                        processed_intent = intent_from_json.strip().strip('_').upper().replace(" ", "_")
                        known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]
                        if processed_intent in known_categories:
                            classified_intent_value = processed_intent
                            logger.info(f"Master intent classified as: {classified_intent_value} via JSON.")
                        else:
                            logger.warning(f"LLM JSON with unknown category '{processed_intent}'. Raw: '{intent_from_json}'. Defaulting UNKNOWN.")
                    else:
                        logger.warning(f"LLM JSON for master intent missing 'intent' key or not string. Data: {data}. Defaulting UNKNOWN.")
                except json.JSONDecodeError:
                    logger.warning(f"Failed to decode JSON from master_intent_classifier. LLM raw: '{completion_str}'. Attempting direct parse.")
                    raw_intent = completion_str.strip().strip('_').upper().replace(" ", "_")
                    known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                    if raw_intent in known_categories:
                        classified_intent_value = raw_intent
                        logger.info(f"Master intent classified as: {classified_intent_value} via direct string parse fallback.")
                    else:
                         logger.warning(f"Direct string parse fallback failed for intent: '{raw_intent}'. Defaulting UNKNOWN.")
            else:
                logger.warning("Master intent classification LLM call returned no content.")
        except Exception as e:
            logger.error(f"Error during master intent classification LLM call: {e}", exc_info=True)
        
        return classified_intent_value

    async def analyze_weather_request_with_llm(self, text: str, lang_code: str, 
                                               system_prompt: str, user_prompt_template: Optional[str]) -> Dict[str, Any]:
        # <<<--- DIAGNOSTIC PRINT IR-5: INSIDE analyze_weather_request_with_llm --- >>>
        print(f"%%%%% IntentRecognizer.analyze_weather_request_with_llm CALLED with text: {text[:30]}... %%%%%")
        logger.info(f"DIAGNOSTIC: analyze_weather_request_with_llm CALLED for text: '{text[:30]}...'")
        
        logger.info(f"Analyzing weather request type (lang: {lang_code}): '{text}' with LLM.")
        user_prompt = (user_prompt_template or "{text}").format(text=text) 
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        if self.llm_services.openai_model_id and \
           any(model_prefix in self.llm_services.openai_model_id for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}
        
        default_response = {"type": "current"} 

        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                temperature=0, 
                **response_format_arg
            )
            if completion_str:
                logger.info(f"LLM response for weather analysis: {completion_str}")
                clean_comp_str = completion_str.strip()
                if clean_comp_str.startswith("```json"):
                    clean_comp_str = clean_comp_str[7:]
                if clean_comp_str.endswith("```"):
                    clean_comp_str = clean_comp_str[:-3]
                
                return json.loads(clean_comp_str)
            else:
                logger.warning("LLM returned no content for weather analysis.")
        except json.JSONDecodeError as e:
            logger.error(f"Error decoding JSON from LLM for weather analysis: '{completion_str}'. Error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error in LLM call during weather request analysis: {e}", exc_info=True)
        
        return default_response

    async def extract_location_with_llm(self, text: str, lang_code: str, 
                                        system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        # ... (method content as previously provided in message #22)
        logger.info(f"Requesting LLM location extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        try:
            if self.llm_services.is_provider_configured("openai"):
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api,
                    temperature=0, 
                    max_tokens=50 
                )
                if completion: location = completion.strip()
            
            if not location and self.llm_services.is_provider_configured("groq"): 
                logger.info("OpenAI location extraction failed or not configured, trying Groq as fallback.")
                completion = await self.llm_services.call_llm_api(
                    provider_name="Groq", 
                    api_key=self.llm_services.groq_api_key, 
                    endpoint_url=self.llm_services.groq_endpoint_url, 
                    model_id=self.llm_services.groq_model_id, 
                    messages=messages_for_api,
                    temperature=0, max_tokens=50
                )
                if completion: location = completion.strip()

        except Exception as e:
            logger.error(f"Error during LLM location extraction: {e}", exc_info=True)

        if location and location.lower() != 'none' and location.strip() != "":
            logger.info(f"LLM successfully extracted location: '{location}'")
            return location
        
        logger.warning(f"LLM could not extract a location from the text: '{text}'.")
        return None


    async def extract_news_topic_with_llm(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        # ... (method content as previously provided in message #22)
        logger.info(f"Requesting LLM news topic extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        topic = None
        try:
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api,
                    temperature=0,
                    max_tokens=50 
                )
                if completion: topic = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM news topic extraction: {e}", exc_info=True)

        if topic and topic.lower() != 'none' and topic.strip() != "":
            logger.info(f"LLM successfully extracted news topic: '{topic}'")
            return topic
        
        logger.info(f"LLM found no specific news topic in '{text}'. General news might be fetched.")
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/profile_manager.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Profile Manager ===
# ==================================================================================================
# Manages user psychological profiles and name variations using LLMs and database interaction.
# ==================================================================================================
import logging
import json
from typing import Optional, Dict, TYPE_CHECKING

from enkibot.utils.database import DatabaseManager 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices # For type hinting

logger = logging.getLogger(__name__)

class ProfileManager:
    def __init__(self, llm_services: 'LLMServices', db_manager: DatabaseManager):
        self.llm_services = llm_services # Type hint is string literal
        self.db_manager = db_manager
        self.MAX_PROFILE_SIZE = 4000

    async def populate_name_variations_with_llm(self, user_id: int, first_name: Optional[str], 
                                                last_name: Optional[str], username: Optional[str],
                                                system_prompt: str, user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"): # Check specific provider
            logger.warning(f"Name variation for user {user_id} skipped: OpenAI not configured in LLMServices.")
            return

        name_parts = [part for part in [first_name, last_name, username] if part and str(part).strip()]
        if not name_parts:
            logger.info(f"No valid name parts for user {user_id}, skipping name variation.")
            return
            
        name_info = ", ".join(name_parts)
        logger.info(f"Requesting name variations for user {user_id} ({name_info}).")
        user_prompt = user_prompt_template.format(name_info=name_info)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        name_variations = set([p.lower() for p in name_parts])
        
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages, temperature=0.3, response_format={"type": "json_object"}
            ) # Removed model_id, should be handled by call_openai_llm default
            if completion_str:
                data = json.loads(completion_str)
                variations_list = data.get('variations')
                if isinstance(variations_list, list):
                    name_variations.update([str(v).lower().strip() for v in variations_list if v and str(v).strip()])
                    logger.info(f"LLM got {len(variations_list)} raw variations. Total unique: {len(name_variations)} for user {user_id}.")
                else:
                    logger.warning(f"LLM name variations for {user_id} no 'variations' list. Resp: {completion_str[:200]}")
            else:
                logger.warning(f"LLM no content for name variations for user {user_id}.")
        except Exception as e:
            logger.error(f"LLM name variation error (user {user_id}): {e}", exc_info=True)

        if name_variations:
            await self.db_manager.save_user_name_variations(user_id, list(name_variations))
        else:
            logger.info(f"No name variations to save for user {user_id}.")

    async def analyze_and_update_user_profile(self, user_id: int, message_text: str,
                                              create_system_prompt: str, create_user_prompt_template: str,
                                              update_system_prompt: str, update_user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"):
            logger.warning(f"Profiling for {user_id} skipped: OpenAI not configured.")
            return
        logger.info(f"Starting/Updating profile analysis for user {user_id}...")
        current_notes = await self.db_manager.get_user_profile_notes(user_id)
        sys_prompt, user_prompt = "", ""

        if not current_notes:
            logger.info(f"No profile for {user_id}. Creating new.")
            sys_prompt, user_prompt = create_system_prompt, create_user_prompt_template.format(message_text=message_text)
        else:
            logger.info(f"Existing profile for {user_id}. Updating.")
            sys_prompt, user_prompt = update_system_prompt, update_user_prompt_template.format(
                current_profile_notes=current_notes, message_text=message_text)
        
        messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}]
        updated_notes_str: Optional[str] = None
        try:
            updated_notes_str = await self.llm_services.call_openai_llm(messages, temperature=0.5, max_tokens=1000)
        except Exception as e:
            logger.error(f"LLM profile analysis error for {user_id}: {e}", exc_info=True)

        if updated_notes_str and updated_notes_str.strip():
            await self.db_manager.update_user_profile_notes(user_id, updated_notes_str.strip()[:self.MAX_PROFILE_SIZE])
            logger.info(f"Profile updated for {user_id}.")
        else:
            logger.warning(f"Profile analysis no content for {user_id}.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/response_generator.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Response Generator ===
# ==================================================================================================
import logging
import asyncio
from typing import List, Dict, Any, Optional, TYPE_CHECKING

from telegram.ext import ContextTypes 

# Use TYPE_CHECKING for imports that could cause circular dependencies if imported directly
if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices
    from enkibot.utils.database import DatabaseManager
    from enkibot.modules.intent_recognizer import IntentRecognizer 

from enkibot.modules.fact_extractor import find_user_search_query_in_text 

logger = logging.getLogger(__name__)

class ResponseGenerator: # Make sure this class definition is present and correct
    def __init__(self, llm_services: 'LLMServices', db_manager: 'DatabaseManager', intent_recognizer: 'IntentRecognizer'):
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.intent_recognizer = intent_recognizer 

    async def analyze_replied_message(self, original_text: str, user_question: str,
                                      system_prompt: str, user_prompt_template: Optional[str]) -> str:
        logger.info(f"Analyzing replied message. Original length: {len(original_text)}, Question: '{user_question}'")
        user_prompt = (user_prompt_template or "Original: {original_text}\nQuestion: {user_question}\nAnalysis:").format(
            original_text=original_text, user_question=user_question
        )
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        try:
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(messages_for_api, temperature=0.5, max_tokens=1000)
                if completion: return completion.strip()
            # Fallback if OpenAI is not configured or call fails
            return "Analysis client not configured or error in API call." 
        except Exception as e:
            logger.error(f"Error in LLM call during replied message analysis: {e}", exc_info=True)
            return "Sorry, an error occurred during text analysis."

    async def get_orchestrated_llm_response(
        self, prompt_text: str, chat_id: int, user_id: int, message_id: int,
        context: ContextTypes.DEFAULT_TYPE, lang_code: str,
        system_prompt_override: str, 
        user_search_ambiguous_response_template: str,
        user_search_not_found_response_template: str
    ) -> str:
        logger.info(f"Orchestrating LLM response for: '{prompt_text[:100]}...' in chat {chat_id}")
        
        history_from_db: List[Dict[str, str]] = []
        profile_context_messages: List[Dict[str, str]] = []
        keyword_context_messages: List[Dict[str, str]] = [] 

        history_from_db = await self.db_manager.get_conversation_history(chat_id, limit=10)

        # Call the imported function directly
        search_term_original = find_user_search_query_in_text(prompt_text) 
        
        if search_term_original:
            logger.info(f"User profile query detected for term: '{search_term_original}' in get_orchestrated_llm_response")
            matched_profiles_data = await self.db_manager.find_user_profiles_by_name_variation(search_term_original)

            if len(matched_profiles_data) == 1:
                profile_data = matched_profiles_data[0]
                target_user_id_found = profile_data.get("UserID") 
                user_identifier = profile_data.get("FirstName") or profile_data.get("Username") or f"User ID {target_user_id_found}"

                if profile_data.get("Notes"): 
                    profile_context_messages.append({
                        "role": "system",
                        "content": f"Important Context (User Dossier) for '{user_identifier}':\n---\n{profile_data['Notes']}\n---"
                    })
                
                if target_user_id_found: 
                    user_specific_messages = await self.db_manager.get_user_messages_from_chat_log(target_user_id_found, chat_id, limit=5)
                    if user_specific_messages:
                        valid_user_messages = [msg for msg in user_specific_messages if isinstance(msg, str) and msg.strip()]
                        if valid_user_messages:
                            formatted_msgs = "\n".join([f'- "{msg_text}"' for msg_text in valid_user_messages])
                            keyword_context_messages.append({
                                "role": "system",
                                "content": f"Additional Context (recent raw messages) from '{user_identifier}'. Use with dossier for freshest insights:\n---\n{formatted_msgs}\n---"
                            })
            elif len(matched_profiles_data) > 1:
                user_options = [ f"@{p.get('Username')}" if p.get('Username') else f"{p.get('FirstName') or ''} {p.get('LastName') or ''}".strip() for p in matched_profiles_data ]
                user_options_str = ", ".join(filter(None, user_options))
                return user_search_ambiguous_response_template.format(user_options=user_options_str)
            else: 
                 logger.info(f"No specific user profile found for search term '{search_term_original}'.")
        
        messages_for_api = []
        if system_prompt_override and isinstance(system_prompt_override, str):
            messages_for_api.append({"role": "system", "content": system_prompt_override})
        else:
            logger.error(f"system_prompt_override is invalid or None: {system_prompt_override}. Using a default.")
            messages_for_api.append({"role": "system", "content": "You are a helpful AI assistant."})

        messages_for_api.extend(profile_context_messages) 
        messages_for_api.extend(keyword_context_messages) 
        messages_for_api.extend(history_from_db) 
        
        if prompt_text and isinstance(prompt_text, str):
            messages_for_api.append({"role": "user", "content": prompt_text})
        else:
            logger.error(f"prompt_text is invalid or None: {prompt_text}. Appending a placeholder user message.")
            messages_for_api.append({"role": "user", "content": "Hello."})

        logger.info(f"Constructed messages_for_api for race_llm_calls. Total messages: {len(messages_for_api)}")
        for i, msg in enumerate(messages_for_api):
            role = msg.get("role")
            content_sample = str(msg.get("content"))[:70] + "..." if msg.get("content") else "None"
            logger.debug(f"  Msg {i}: Role='{role}', ContentSample='{content_sample}'")
            if not isinstance(msg.get("role"), str) or not isinstance(msg.get("content"), str):
                logger.error(f"INVALID MESSAGE PART DETECTED in messages_for_api at index {i}: Role type {type(msg.get('role'))}, Content type {type(msg.get('content'))}")
        
        MAX_CONTEXT_MSGS = 20 
        if len(messages_for_api) > MAX_CONTEXT_MSGS:
            system_prompts_initial = [m for m in messages_for_api if m.get("role") == "system"]
            other_messages = [m for m in messages_for_api if m.get("role") != "system"]
            num_other_to_keep = MAX_CONTEXT_MSGS - len(system_prompts_initial)
            if num_other_to_keep < 0: num_other_to_keep = 5 
            
            final_messages_for_api = system_prompts_initial + other_messages[-num_other_to_keep:]
            logger.info(f"Message context truncated from {len(messages_for_api)} to {len(final_messages_for_api)} messages.")
            messages_for_api = final_messages_for_api

        final_reply = await self.llm_services.race_llm_calls(messages_for_api)

        if not final_reply:
            final_reply = "I'm currently unable to get a response from my core AI. Please try again shortly."
            logger.error("All LLM providers failed in race_llm_calls. Final_reply set to fallback.")

        await self.db_manager.save_to_conversation_history(
            chat_id, user_id, message_id, 'user', prompt_text
        )
        if context.bot: 
            await self.db_manager.save_to_conversation_history(
                chat_id, context.bot.id, None, 'assistant', final_reply
            )
        return final_reply


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/__init__.py ---
======================================================================

# This file makes the 'utils' directory a Python package.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/database.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Database Utilities ===
# ==================================================================================================
# Handles all database interactions for EnkiBot, including setup,
# connection management, and data querying/storage.
# ==================================================================================================
import logging
import pyodbc
from typing import List, Dict, Any, Optional

from enkibot import config # For DB_CONNECTION_STRING

logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self, connection_string: Optional[str]):
        self.connection_string = connection_string
        if not self.connection_string:
            logger.warning("Database connection string is not configured. Database operations will be disabled.")

    def get_db_connection(self) -> Optional[pyodbc.Connection]:
        """Establishes and returns a new database connection."""
        if not self.connection_string:
            return None
        try:
            # autocommit=False by default, manage transactions explicitly
            conn = pyodbc.connect(self.connection_string, autocommit=False)
            logger.debug("Database connection established.")
            return conn
        except pyodbc.Error as ex:
            logger.error(f"Database connection error: {ex.args[0] if ex.args else ''} - {ex}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"Unexpected error establishing database connection: {e}", exc_info=True)
            return None

    async def execute_query(self, query: str, params: Optional[tuple] = None, fetch_one: bool = False, fetch_all: bool = False, commit: bool = False):
        """Executes a given SQL query with optional parameters and commit."""
        if not self.connection_string:
            logger.warning("Query execution skipped: Database not configured.")
            return None if fetch_one or fetch_all else (True if commit else False)


        conn = self.get_db_connection()
        if not conn:
            return None if fetch_one or fetch_all else (True if commit else False)

        try:
            with conn.cursor() as cursor:
                logger.debug(f"Executing query: {query[:150]}... with params: {params}")
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                if commit:
                    conn.commit()
                    logger.debug("Query committed successfully.")
                    return True

                if fetch_one:
                    row = cursor.fetchone()
                    logger.debug(f"Query fetch_one result: {row}")
                    return row
                if fetch_all:
                    rows = cursor.fetchall()
                    logger.debug(f"Query fetch_all result count: {len(rows)}")
                    return rows
            return True # Default success for non-fetching, non-committing queries
        except pyodbc.Error as ex:
            logger.error(f"Database query error on '{query[:100]}...': {ex}", exc_info=True)
            try: conn.rollback(); logger.info("Transaction rolled back.")
            except pyodbc.Error as rb_ex: logger.error(f"Error during rollback: {rb_ex}", exc_info=True)
            return None if fetch_one or fetch_all else False
        except Exception as e:
            logger.error(f"Unexpected error during query execution '{query[:100]}...': {e}", exc_info=True)
            return None if fetch_one or fetch_all else False
        finally:
            if conn:
                conn.close()
                logger.debug("Database connection closed post-execution.")
    async def get_recent_chat_texts(self, chat_id: int, limit: int = 3) -> List[str]:
        """
        Fetches the text of the last N messages from a given chat_id from ChatLog.
        Filters out potential null or empty messages.
        """
        if not self.connection_string:
            return []
        
        query = """
            SELECT TOP (?) MessageText
            FROM ChatLog
            WHERE ChatID = ? AND MessageText IS NOT NULL AND MessageText != ''
            ORDER BY Timestamp DESC
        """
        # Ensure 'limit' is a positive integer
        actual_limit = max(1, limit) 
        
        rows = await self.execute_query(query, (actual_limit, chat_id), fetch_all=True)
        if rows:
            # Messages are fetched in reverse chronological order, reverse again for natural order
            return [row.MessageText for row in reversed(rows) if row.MessageText] 
        return []
    async def log_chat_message_and_upsert_user(
        self, chat_id: int, user_id: int, username: Optional[str],
        first_name: Optional[str], last_name: Optional[str],
        message_id: int, message_text: str, preferred_language: Optional[str] = None
    ) -> Optional[str]:
        """Logs a chat message and creates or updates the user profile."""
        if not self.connection_string: return None
        upsert_user_sql = """
            MERGE UserProfiles AS t
            USING (VALUES(?,?,?,?,GETDATE(),?)) AS s(UserID,Username,FirstName,LastName,LastSeen,PreferredLanguage)
            ON t.UserID = s.UserID
            WHEN MATCHED THEN
                UPDATE SET Username=s.Username, FirstName=s.FirstName, LastName=s.LastName, LastSeen=s.LastSeen, MessageCount=ISNULL(t.MessageCount,0)+1, PreferredLanguage=ISNULL(s.PreferredLanguage, t.PreferredLanguage)
            WHEN NOT MATCHED THEN
                INSERT(UserID,Username,FirstName,LastName,LastSeen,MessageCount,ProfileLastUpdated, PreferredLanguage)
                VALUES(s.UserID,s.Username,s.FirstName,s.LastName,s.LastSeen,1,GETDATE(),s.PreferredLanguage)
            OUTPUT $action AS Action;
        """
        user_params = (user_id, username, first_name, last_name, preferred_language)
        
        chat_log_sql = "INSERT INTO ChatLog (ChatID, UserID, Username, FirstName, MessageID, MessageText, Timestamp) VALUES (?, ?, ?, ?, ?, ?, GETDATE())"
        chat_log_params = (chat_id, user_id, username, first_name, message_id, message_text)

        conn = self.get_db_connection()
        if not conn: return None
        action_taken = None
        try:
            with conn.cursor() as cursor:
                cursor.execute(upsert_user_sql, user_params)
                row = cursor.fetchone()
                if row: action_taken = row.Action
                cursor.execute(chat_log_sql, chat_log_params)
            conn.commit()
            logger.info(f"Logged message for user {user_id}. Profile action: {action_taken}")
            return action_taken
        except pyodbc.Error as ex:
            logger.error(f"DB error logging/upserting user {user_id}: {ex}", exc_info=True); conn.rollback()
            return None
        finally:
            if conn: conn.close()

    async def get_conversation_history(self, chat_id: int, limit: int = 20) -> List[Dict[str, str]]:
        query = "SELECT TOP (?) Role, Content FROM ConversationHistory WHERE ChatID = ? ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, chat_id), fetch_all=True)
        return [{"role": row.Role.lower(), "content": row.Content} for row in reversed(rows)] if rows else []

    async def save_to_conversation_history(self, chat_id: int, entity_id: int, message_id_telegram: Optional[int], role: str, content: str):
        query = "INSERT INTO ConversationHistory (ChatID, UserID, MessageID, Role, Content, Timestamp) VALUES (?, ?, ?, ?, ?, GETDATE())"
        await self.execute_query(query, (chat_id, entity_id, message_id_telegram, role, content), commit=True)

    async def get_user_profile_notes(self, user_id: int) -> Optional[str]:
        row = await self.execute_query("SELECT Notes FROM UserProfiles WHERE UserID = ?", (user_id,), fetch_one=True)
        return row.Notes if row and row.Notes else None

    async def update_user_profile_notes(self, user_id: int, notes: str):
        await self.execute_query("UPDATE UserProfiles SET Notes = ?, ProfileLastUpdated = GETDATE() WHERE UserID = ?", (notes, user_id), commit=True)
        logger.info(f"Profile notes updated for user {user_id}.")

    async def save_user_name_variations(self, user_id: int, variations: List[str]):
        if not self.connection_string or not variations: return
        sql_merge = """
            MERGE INTO UserNameVariations AS t USING (SELECT ? AS UserID, ? AS NameVariation) AS s
            ON (t.UserID = s.UserID AND t.NameVariation = s.NameVariation)
            WHEN NOT MATCHED THEN INSERT (UserID, NameVariation) VALUES (s.UserID, s.NameVariation);
        """
        conn = self.get_db_connection()
        if not conn: return
        try:
            with conn.cursor() as cursor:
                params_to_insert = [(user_id, var) for var in variations if var and str(var).strip()]
                if params_to_insert: cursor.executemany(sql_merge, params_to_insert)
            conn.commit()
            logger.info(f"Saved/updated {len(params_to_insert)} name vars for user {user_id}.")
        except pyodbc.Error as ex:
            logger.error(f"DB error saving name vars for user {user_id}: {ex}", exc_info=True); conn.rollback()
        finally:
            if conn: conn.close()
            
    async def find_user_profiles_by_name_variation(self, name_variation_query: str) -> List[Dict[str, Any]]:
        query = """
            SELECT DISTINCT up.UserID, up.FirstName, up.LastName, up.Username, up.Notes
            FROM UserProfiles up JOIN UserNameVariations unv ON up.UserID = unv.UserID
            WHERE unv.NameVariation = ?
        """
        rows = await self.execute_query(query, (name_variation_query.lower(),), fetch_all=True)
        return [{"UserID": r.UserID, "FirstName": r.FirstName, "LastName": r.LastName, "Username": r.Username, "Notes": r.Notes} for r in rows] if rows else []

    async def get_user_messages_from_chat_log(self, user_id: int, chat_id: int, limit: int = 10) -> List[str]:
        query = "SELECT TOP (?) MessageText FROM ChatLog WHERE UserID = ? AND ChatID = ? AND MessageText IS NOT NULL AND MessageText != '' ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, user_id, chat_id), fetch_all=True)
        return [row.MessageText for row in rows] if rows else []

def initialize_database():
    if not config.DB_CONNECTION_STRING:
        logger.warning("Cannot initialize database: Connection string not configured.")
        return
    db_mngr = DatabaseManager(config.DB_CONNECTION_STRING) # Use the manager
    conn = db_mngr.get_db_connection() # Get connection via manager
    if not conn: return
    conn.autocommit = True 
    table_queries = { # Simplified from your example for brevity, ensure all are there
        "UserProfiles": "CREATE TABLE UserProfiles (UserID BIGINT PRIMARY KEY, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, LastName NVARCHAR(255) NULL, LastSeen DATETIME2 DEFAULT GETDATE(), MessageCount INT DEFAULT 0, PreferredLanguage NVARCHAR(10) NULL, Notes NVARCHAR(MAX) NULL, ProfileLastUpdated DATETIME2 DEFAULT GETDATE());",
        "UserNameVariations": "CREATE TABLE UserNameVariations (VariationID INT IDENTITY(1,1) PRIMARY KEY, UserID BIGINT NOT NULL, NameVariation NVARCHAR(255) NOT NULL, FOREIGN KEY (UserID) REFERENCES UserProfiles(UserID) ON DELETE CASCADE);",
        "IX_UserNameVariations_UserID_NameVariation": "CREATE UNIQUE INDEX IX_UserNameVariations_UserID_NameVariation ON UserNameVariations (UserID, NameVariation);",
        "ConversationHistory": "CREATE TABLE ConversationHistory (MessageDBID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageID BIGINT NULL, Role NVARCHAR(50) NOT NULL, Content NVARCHAR(MAX) NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ConversationHistory_ChatID_Timestamp": "CREATE INDEX IX_ConversationHistory_ChatID_Timestamp ON ConversationHistory (ChatID, Timestamp DESC);",
        "ChatLog": "CREATE TABLE ChatLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, MessageID BIGINT NOT NULL, MessageText NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ChatLog_ChatID_Timestamp": "CREATE INDEX IX_ChatLog_ChatID_Timestamp ON ChatLog (ChatID, Timestamp DESC);",
        "IX_ChatLog_UserID": "CREATE INDEX IX_ChatLog_UserID ON ChatLog (UserID);",
        "ErrorLog": "CREATE TABLE ErrorLog (ErrorID INT IDENTITY(1,1) PRIMARY KEY, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, LogLevel NVARCHAR(50) NOT NULL, LoggerName NVARCHAR(255) NULL, ModuleName NVARCHAR(255) NULL, FunctionName NVARCHAR(255) NULL, LineNumber INT NULL, ErrorMessage NVARCHAR(MAX) NOT NULL, ExceptionInfo NVARCHAR(MAX) NULL);",
        "IX_ErrorLog_Timestamp": "CREATE INDEX IX_ErrorLog_Timestamp ON ErrorLog (Timestamp DESC);",
    }
    try:
        with conn.cursor() as cursor:
            logger.info("Initializing database tables...")
            for name, query in table_queries.items():
                is_idx = name.startswith("IX_")
                obj_type = "INDEX" if is_idx else "TABLE"
                obj_name = name if is_idx else name
                tbl_for_idx = name.split("_on_")[-1] if "_on_" in name else (name.split("_")[1] if is_idx and len(name.split("_")) > 1 else name)
                
                check_q = "SELECT name FROM sys.indexes WHERE name = ? AND object_id = OBJECT_ID(?)" if is_idx else "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = ?"
                params_check = (obj_name, tbl_for_idx) if is_idx else (obj_name,)
                
                cursor.execute(check_q, params_check)
                if cursor.fetchone(): logger.info(f"{obj_type} '{obj_name}' already exists.")
                else: logger.info(f"{obj_type} '{obj_name}' not found. Creating..."); cursor.execute(query); logger.info(f"{obj_type} '{obj_name}' created.")
            logger.info("Database initialization check complete.")
    except Exception as e: logger.error(f"DB init error: {e}", exc_info=True)
    finally:
        if conn: conn.autocommit = False; conn.close()

# backfill_existing_user_name_variations should be moved to main.py or a utility script
# to be run in a context where ProfileManager is available.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/logging_config.py ---
======================================================================

# ==================================================================================================
# === EnkiBot Logging Configuration ===
# ==================================================================================================
# Sets up application-wide structured logging to both file and console.
# Includes a custom logging handler to write critical errors to the SQL database
# and clears the previous log file on startup.
# ==================================================================================================

import logging
import traceback
import pyodbc
import os # <--- IMPORT OS MODULE
from enkibot import config

def setup_logging():
    """Initializes the logging configuration for the entire application."""

    # --- START: CLEAR PREVIOUS LOG FILE ---
    log_file_name = "bot_activity.log"
    try:
        if os.path.exists(log_file_name):
            os.remove(log_file_name)
            # This print goes to console before logging is fully set up for the bot's logger.
            print(f"INFO: Previous log file '{log_file_name}' removed successfully.")
    except OSError as e:
        # This print also goes to console.
        print(f"WARNING: Error removing previous log file '{log_file_name}': {e}")
    # --- END: CLEAR PREVIOUS LOG FILE ---
    
    # Define a custom handler for logging errors to the database
    class SQLDBLogHandler(logging.Handler):
        """
        A logging handler that writes log records with level ERROR or higher
        to a dedicated table in the SQL Server database.
        """
        def __init__(self):
            super().__init__()
            self.conn = None

        def _get_db_conn_for_logging(self):
            """Establishes a database connection specifically for logging."""
            if not config.DB_CONNECTION_STRING:
                return None
            try:
                # Use autocommit=True for logging to ensure errors are written immediately.
                return pyodbc.connect(config.DB_CONNECTION_STRING, autocommit=True)
            except pyodbc.Error:
                # If the DB is down, we can't log to it. Silently fail for now.
                # A print statement could be added here for immediate feedback if needed.
                # print("WARNING: SQLDBLogHandler could not connect to the database for logging.")
                return None

        def emit(self, record: logging.LogRecord):
            """
            Writes the log record to the ErrorLog table in the database.
            """
            if self.conn is None:
                self.conn = self._get_db_conn_for_logging()

            if self.conn:
                try:
                    msg = self.format(record)
                    exc_info_str = traceback.format_exc() if record.exc_info else None
                    sql = "INSERT INTO ErrorLog (LogLevel, LoggerName, ModuleName, FunctionName, LineNumber, ErrorMessage, ExceptionInfo) VALUES (?, ?, ?, ?, ?, ?, ?)"
                    with self.conn.cursor() as cursor:
                        cursor.execute(sql, record.levelname, record.name, record.module, record.funcName, record.lineno, msg, exc_info_str)
                except pyodbc.Error:
                    # If an error occurs during logging, handle it and sever the connection.
                    self.handleError(record)
                    self.conn = None # Reset connection to be re-established on next emit.
        
        def close(self):
            """Closes the database connection if it's open."""
            if self.conn:
                try:
                    self.conn.close()
                except pyodbc.Error:
                    pass
            super().close()

    # --- Main Logging Configuration ---
    log_level = logging.INFO
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s'
    
    # Configure root logger
    # By using basicConfig with force=True (Python 3.8+), we can reconfigure if needed.
    # For older Python, ensure basicConfig is called only once application-wide.
    # Given setup_logging() is called once from main.py, this should be fine.
    logging.basicConfig(
        format=log_format,
        level=log_level,
        handlers=[
            logging.FileHandler(log_file_name, encoding='utf-8'), # Use variable
            logging.StreamHandler()
        ]
        # force=True # Add this if using Python 3.8+ and re-running setup_logging,
                   # but it should not be necessary with current structure.
    )
    
    module_logger = logging.getLogger(__name__) # Logger for this specific module (logging_config.py)

    # Add the custom DB handler if the database is configured
    if config.DB_CONNECTION_STRING:
        db_log_handler = SQLDBLogHandler()
        db_log_handler.setLevel(logging.ERROR) # Only log ERROR and CRITICAL to DB
        formatter = logging.Formatter(log_format)
        db_log_handler.setFormatter(formatter)
        logging.getLogger().addHandler(db_log_handler) # Add to the root logger
        module_logger.info("Configured logging of ERROR-level messages to the SQL database.")
    else:
        module_logger.warning("Logging to SQL database is NOT configured (DB_CONNECTION_STRING is missing).")


