======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/__init__.py ---
======================================================================

# enkibot/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file intentionally left blank.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/app.py ---
======================================================================

# enkibot/app.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
from telegram.ext import Application

from enkibot import config
from enkibot.utils.database import DatabaseManager
from enkibot.core.llm_services import LLMServices
from enkibot.core.language_service import LanguageService
from enkibot.modules.intent_recognizer import IntentRecognizer
from enkibot.modules.profile_manager import ProfileManager
from enkibot.modules.api_router import ApiRouter
from enkibot.modules.response_generator import ResponseGenerator
from enkibot.core.telegram_handlers import TelegramHandlerService

logger = logging.getLogger(__name__)

class EnkiBotApplication:
    def __init__(self, ptb_application: Application):
        logger.info("EnkiBotApplication initializing...")
        self.ptb_application = ptb_application

        # Initialize core services
        self.db_manager = DatabaseManager(config.DB_CONNECTION_STRING)
        self.llm_services = LLMServices(
            openai_api_key=config.OPENAI_API_KEY, openai_model_id=config.OPENAI_MODEL_ID,
            groq_api_key=config.GROQ_API_KEY, groq_model_id=config.GROQ_MODEL_ID, groq_endpoint_url=config.GROQ_ENDPOINT_URL,
            openrouter_api_key=config.OPENROUTER_API_KEY, openrouter_model_id=config.OPENROUTER_MODEL_ID, openrouter_endpoint_url=config.OPENROUTER_ENDPOINT_URL,
            google_ai_api_key=config.GOOGLE_AI_API_KEY, google_ai_model_id=config.GOOGLE_AI_MODEL_ID
        )
        self.language_service = LanguageService(
            llm_services=self.llm_services, 
            db_manager=self.db_manager # Pass db_manager for fetching chat history
        )
        
        # Initialize functional modules/services
        self.intent_recognizer = IntentRecognizer(self.llm_services)
        self.profile_manager = ProfileManager(self.llm_services, self.db_manager)
        self.api_router = ApiRouter(
            weather_api_key=config.WEATHER_API_KEY, 
            news_api_key=config.NEWS_API_KEY,
            llm_services=self.llm_services
        )
        self.response_generator = ResponseGenerator(
            self.llm_services, 
            self.db_manager, 
            self.intent_recognizer
        )

        # Initialize Telegram handlers, passing all necessary services
        self.handler_service = TelegramHandlerService(
            application=self.ptb_application,
            db_manager=self.db_manager,
            llm_services=self.llm_services,
            intent_recognizer=self.intent_recognizer,
            profile_manager=self.profile_manager,
            api_router=self.api_router,
            response_generator=self.response_generator,
            language_service=self.language_service,
            allowed_group_ids=config.ALLOWED_GROUP_IDS, # Pass as set
            bot_nicknames=config.BOT_NICKNAMES_TO_CHECK # Pass as list
        )
        
        logger.info("EnkiBotApplication initialized all services.")

    def register_handlers(self):
        """Registers all Telegram handlers."""
        self.handler_service.register_all_handlers()
        logger.info("EnkiBotApplication: All handlers registered with PTB Application.")

    def run(self):
        """Starts the bot polling."""
        logger.info("EnkiBotApplication: Starting PTB Application polling...")
        self.ptb_application.run_polling(allowed_updates=Update.ALL_TYPES)
        logger.info("EnkiBotApplication: Polling stopped.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/combine_files.py ---
======================================================================

# enkibot/combine_files.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
import os
import logging

# --- Configuration ---
# The root directory of your project source code.
# IMPORTANT: Please ensure this path is correct for your system.
PROJECT_ROOT = r'c:\Projects\EnkiBot\EnkiBot\EnkiBot'
# The name of the file that will contain all the combined code.
OUTPUT_FILENAME = 'combined_enkibot_python_source.txt' # Changed name to reflect content
# --- End Configuration ---

# Setup basic logging for the script itself.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_all_python_files(root_dir):
    """
    Walks through the directory structure and collects paths of .py files.
    
    Args:
        root_dir (str): The starting directory to walk.
        
    Returns:
        list: A sorted list of all .py file paths found.
    """
    file_paths = []
    for dirpath, _, filenames in os.walk(root_dir):
        # Exclude __pycache__ directories
        if '__pycache__' in dirpath:
            continue
        for filename in filenames:
            # --- MODIFICATION IS HERE ---
            # Only include files that end with the .py extension.
            if filename.endswith(('.py','.json')):
                file_paths.append(os.path.join(dirpath, filename))
                
    return sorted(file_paths)

def combine_project_files(root_dir, output_file):
    """
    Reads all .py files from a project directory and writes their contents
    into a single output file, with headers for each file.
    
    Args:
        root_dir (str): The root directory of the project to combine.
        output_file (str): The path to the single output file.
    """
    logging.info(f"Starting to combine only .py files from project root: '{root_dir}'")
    
    all_files = get_all_python_files(root_dir)
    
    if not all_files:
        logging.error(f"No .py files found in '{root_dir}'. Please check the PROJECT_ROOT path.")
        return

    try:
        # Open the output file in write mode with UTF-8 encoding
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for file_path in all_files:
                # Normalize path for consistent display
                normalized_path = file_path.replace('\\', '/')
                header = f"======================================================================\n"
                header += f"--- File: {normalized_path} ---\n"
                header += f"======================================================================\n\n"
                
                outfile.write(header)
                logging.info(f"Processing: {normalized_path}")
                
                try:
                    # Open the source file in read mode with UTF-8 encoding
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(infile.read())
                    # Add newlines for separation between files
                    outfile.write("\n\n\n")
                except Exception as e:
                    # Handle potential read errors
                    error_message = f"*** ERROR: Could not read file. Reason: {e} ***\n\n\n"
                    outfile.write(error_message)
                    logging.warning(f"Could not read {file_path}: {e}")

    except IOError as e:
        logging.error(f"Fatal error writing to output file '{output_file}': {e}")
        return

    logging.info(f"Successfully combined {len(all_files)} .py files into '{output_file}'.")

if __name__ == '__main__':
    if not os.path.isdir(PROJECT_ROOT):
        print(f"Error: The project directory '{PROJECT_ROOT}' was not found.")
        print("Please make sure the PROJECT_ROOT path is correct and you are running this script from a location that can access it.")
    else:
        combine_project_files(PROJECT_ROOT, OUTPUT_FILENAME)


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/config.py ---
======================================================================

﻿# enkibot/config.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Configuration ===
# ==================================================================================================
# Central configuration file for API keys, model IDs, database settings, and other constants.
# It is best practice to load sensitive information from environment variables.
# ==================================================================================================

import os
import logging

# --- Core Bot Settings ---
TELEGRAM_BOT_TOKEN = os.getenv('ENKI_BOT_TOKEN')
# A list of bot nicknames that trigger a response in group chats.
BOT_NICKNAMES_TO_CHECK = ["enki", "enkibot", "энки", "энкибот", "бот", "bot"]

# --- Database Configuration (MS SQL Server) ---
SQL_SERVER_NAME = os.getenv('ENKI_BOT_SQL_SERVER_NAME')
SQL_DATABASE_NAME = os.getenv('ENKI_BOT_SQL_DATABASE_NAME')
DB_CONNECTION_STRING = (
    f"DRIVER={{ODBC Driver 17 for SQL Server}};"
    f"SERVER={SQL_SERVER_NAME};"
    f"DATABASE={SQL_DATABASE_NAME};"
    f"Trusted_Connection=yes;"
) if SQL_SERVER_NAME and SQL_DATABASE_NAME else None

# --- LLM Provider API Keys & Models ---
# OpenAI
OPENAI_API_KEY = os.getenv('ENKI_BOT_OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MODEL_ID', 'gpt-4o')                 # General purpose (if not overridden by task-specific models)
OPENAI_CLASSIFICATION_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_CLASSIFICATION_MODEL_ID', 'gpt-3.5-turbo') # For faster tasks like intent classification
OPENAI_TRANSLATION_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_TRANSLATION_MODEL_ID', 'gpt-4o-mini')      # For language pack creation
OPENAI_MULTIMODAL_IMAGE_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_MULTIMODAL_IMAGE_MODEL_ID', 'gpt-4.1-mini')
OPENAI_DALLE_MODEL_ID = os.getenv('ENKI_BOT_OPENAI_DALLE_MODEL_ID', 'dall-e-3')
DEFAULT_IMAGE_N = 1
DEFAULT_IMAGE_SIZE = "1024x1024"
DEFAULT_IMAGE_QUALITY = "standard"
# Groq
GROQ_API_KEY = os.getenv('ENKI_BOT_GROQ_API_KEY')
GROQ_MODEL_ID = os.getenv('ENKI_BOT_GROQ_MODEL_ID', 'llama-3.3-70b-versatile')
GROQ_ENDPOINT_URL = "https://api.groq.com/openai/v1/chat/completions"

# OpenRouter
OPENROUTER_API_KEY = os.getenv('ENKI_BOT_OPENROUTER_API_KEY')
OPENROUTER_MODEL_ID = os.getenv('ENKI_BOT_OPENROUTER_MODEL_ID', 'mistralai/mistral-7b-instruct:free')
OPENROUTER_ENDPOINT_URL = "https://openrouter.ai/api/v1/chat/completions"

# Google AI
GOOGLE_AI_API_KEY = os.getenv('ENKI_BOT_GOOGLE_AI_API_KEY')
GOOGLE_AI_MODEL_ID = os.getenv('ENKI_BOT_GOOGLE_AI_MODEL_ID', 'gemini-1.5-flash-latest')

# --- External Service API Keys ---
NEWS_API_KEY = os.getenv('ENKI_BOT_NEWS_API_KEY')
WEATHER_API_KEY = os.getenv('ENKI_BOT_WEATHER_API_KEY')

# --- Group Chat Access Control ---
ALLOWED_GROUP_IDS_STR = os.getenv('ENKI_BOT_ALLOWED_GROUP_IDS')
ALLOWED_GROUP_IDS = set()
if ALLOWED_GROUP_IDS_STR:
    try:
        ALLOWED_GROUP_IDS = set(int(id_str.strip()) for id_str in ALLOWED_GROUP_IDS_STR.split(','))
        if ALLOWED_GROUP_IDS:
            logging.info(f"Bot access is restricted to group IDs: {ALLOWED_GROUP_IDS}")
    except ValueError:
        logging.error(f"Invalid format for ENKI_BOT_ALLOWED_GROUP_IDS: '{ALLOWED_GROUP_IDS_STR}'. No group restrictions applied.")
else:
    logging.info("ENKI_BOT_ALLOWED_GROUP_IDS is not set. The bot will operate in all groups.")

# --- Language and Prompts Configuration ---
# The default language to use if detection fails.
DEFAULT_LANGUAGE = "en"
# The directory where language-specific prompt files (e.g., en.json, ru.json) are stored.
LANGUAGE_PACKS_DIR = os.path.join(os.path.dirname(__file__), 'lang')


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/__init__.py ---
======================================================================

# enkibot/core/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# This file makes the 'core' directory a Python package.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/general_handler.py ---
======================================================================


# enkibot/core/intent_handlers/general_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.


import logging
from typing import Optional, TYPE_CHECKING

from telegram import Update
from telegram.ext import ContextTypes
from telegram.constants import ChatAction

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.response_generator import ResponseGenerator
    # from enkibot.utils.database import DatabaseManager # If direct DB access is needed later

logger = logging.getLogger(__name__)

class GeneralIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 response_generator: 'ResponseGenerator'
                 # db_manager: Optional['DatabaseManager'] = None # Add if needed
                 ):
        logger.info("GeneralIntentHandler __init__ STARTING")
        self.language_service = language_service
        self.response_generator = response_generator
        # self.db_manager = db_manager
        logger.info("GeneralIntentHandler __init__ COMPLETED")

    async def handle_request(self, 
                             update: Update, 
                             context: ContextTypes.DEFAULT_TYPE, 
                             user_msg_txt: str, 
                             master_intent: str) -> None:
        """
        Handles USER_PROFILE_QUERY, GENERAL_CHAT, and UNKNOWN_INTENT,
        and can serve as a fallback for other intents if needed.
        """
        if not update.message or not update.effective_chat or not update.effective_user:
            logger.warning("GeneralIntentHandler.handle_request called with invalid update/context.")
            return

        logger.info(f"GeneralIntentHandler: Handling intent '{master_intent}' for: '{user_msg_txt[:70]}...'")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        
        main_orchestrator_prompts = self.language_service.get_llm_prompt_set("main_orchestrator")
        system_prompt_override = "You are EnkiBot, a helpful and friendly AI assistant." 
        
        if main_orchestrator_prompts and "system" in main_orchestrator_prompts:
            system_prompt_template = main_orchestrator_prompts["system"]
            language_name_for_prompt = self.language_service.current_lang 
            try: 
                from babel import Locale
                locale = Locale.parse(self.language_service.current_lang)
                language_name_for_prompt = locale.get_display_name('en') # Get language name in English
            except ImportError: 
                logger.debug("Babel not installed, using ISO code as language_name in main_orchestrator prompt.")
            except Exception as e: 
                logger.warning(f"Could not get display name for lang {self.language_service.current_lang}: {e}")

            try:
                system_prompt_override = system_prompt_template.format(
                    language_name=language_name_for_prompt, 
                    lang_code=self.language_service.current_lang
                )
            except KeyError as ke:
                logger.error(f"KeyError formatting main_orchestrator system prompt: {ke}. Using template as is: '{system_prompt_template}'")
                system_prompt_override = system_prompt_template # Use unformatted if keys missing
            
            logger.info(f"GeneralIntentHandler: Using system prompt with lang instruction for {self.language_service.current_lang}")
        
        logger.info(f"GeneralIntentHandler: FINAL SYSTEM PROMPT for get_orchestrated_llm_response (lang: {self.language_service.current_lang}): '{system_prompt_override[:100]}...'")
        
        reply = await self.response_generator.get_orchestrated_llm_response(
            prompt_text=user_msg_txt, 
            chat_id=update.effective_chat.id, 
            user_id=update.effective_user.id, 
            message_id=update.message.message_id, 
            context=context, 
            lang_code=self.language_service.current_lang,
            system_prompt_override=system_prompt_override, 
            user_search_ambiguous_response_template=self.language_service.get_response_string("user_search_ambiguous_clarification"),
            user_search_not_found_response_template=self.language_service.get_response_string("user_search_not_found_in_db") 
        )
        
        if reply: 
            await update.message.reply_text(reply)
        else: 
            await update.message.reply_text(self.language_service.get_response_string("llm_error_fallback"))


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/image_generation_handler.py ---
======================================================================

﻿# enkibot/core/intent_handlers/image_generation_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging 
import base64 
import re
from typing import Optional, TYPE_CHECKING

from telegram import Update
# InputFile might not be strictly necessary if only sending URLs or bytes for photos
from telegram.ext import ContextTypes 
from telegram.constants import ChatAction

from enkibot import config # Import config directly

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.core.llm_services import LLMServices

logger = logging.getLogger(__name__)

class ImageGenerationIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer',
                 llm_services: 'LLMServices'):
        logger.info("ImageGenerationIntentHandler initialized.")
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer
        self.llm_services = llm_services

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        if not update.message or not update.effective_chat:
            return

        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.UPLOAD_PHOTO)
        preliminary_reply = await update.message.reply_text(
            self.language_service.get_response_string("image_generation_start")
        )

        extractor_prompts = self.language_service.get_llm_prompt_set("image_generation_prompt_extractor")
        clean_prompt: Optional[str] = None
        
        if extractor_prompts and "system" in extractor_prompts and extractor_prompts.get("user_template"):
            clean_prompt = await self.intent_recognizer.extract_image_prompt_with_llm(
                text=user_msg_txt,
                lang_code=self.language_service.current_lang,
                system_prompt=extractor_prompts["system"],
                user_prompt_template=extractor_prompts["user_template"]
            )
        else:
            logger.error("Image generation prompt extractor prompts are missing or malformed (expecting 'user_template')!")
            # Basic fallback prompt cleaning
            bot_nicknames_to_check = config.BOT_NICKNAMES_TO_CHECK 
            bot_name_pattern = r"(?i)\b(?:{})\b\s*[:,]?\s*".format("|".join(re.escape(name) for name in bot_nicknames_to_check))
            cleaned_text_intermediate = re.sub(bot_name_pattern, "", user_msg_txt, count=1).strip()
            triggers_to_remove = ["draw", "create", "generate", "image of", "picture of", "сделай картинку", "нарисуй", "создай", "сгенерируй", "картинку", "изображение"]
            for trigger in triggers_to_remove:
                cleaned_text_intermediate = re.sub(r'(?i)\b' + re.escape(trigger) + r'\b\s*', '', cleaned_text_intermediate, count=1).strip()
            cleaned_text_intermediate = re.sub(r"^(can you|could you|please|i want|i need|дай мне|хочу)\s*", "", cleaned_text_intermediate, flags=re.IGNORECASE).strip()
            cleaned_text_intermediate = re.sub(r"[?.!]$", "", cleaned_text_intermediate).strip()
            if cleaned_text_intermediate and len(cleaned_text_intermediate) > 2: 
                 clean_prompt = cleaned_text_intermediate
            else:
                 logger.warning("Fallback prompt cleaning also resulted in no usable prompt.")


        if not clean_prompt:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=preliminary_reply.message_id,
                text=self.language_service.get_response_string("image_generation_no_prompt")
            )
            return

        try:
            generated_images_data = await self.llm_services.generate_image_with_dalle(
                prompt=clean_prompt,
                n=config.DEFAULT_IMAGE_N,        
                size=config.DEFAULT_IMAGE_SIZE,  
                quality=config.DEFAULT_IMAGE_QUALITY, 
                response_format="url" 
            )
            
            await context.bot.delete_message(chat_id=update.effective_chat.id, message_id=preliminary_reply.message_id)

            if generated_images_data:
                caption_key = "image_generation_success_single" if len(generated_images_data) == 1 else "image_generation_success_multiple"
                
                for i, img_data in enumerate(generated_images_data):
                    current_caption = self.language_service.get_response_string(caption_key, image_prompt=clean_prompt) if i == 0 else None
                    if img_data.get("url"):
                        logger.info(f"DALL-E API returned a URL. Sending photo to user.")
                        await update.message.reply_photo(photo=img_data["url"], caption=current_caption)
                    elif img_data.get("b64_json"): 
                        logger.info("Image data returned as b64_json. Decoding and sending photo.")
                        image_bytes = base64.b64decode(img_data["b64_json"])
                        await update.message.reply_photo(photo=image_bytes, caption=current_caption)
                    else: 
                        logger.error(f"Image generation successful but no URL or b64_json data found for prompt: {clean_prompt}")
                        await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))

            else: 
                logger.error(f"Image generation failed for prompt: {clean_prompt} (no data returned from service or service call failed)")
                await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))

        except Exception as e:
            logger.error(f"An exception occurred while handling image generation: {e}", exc_info=True)
            try:
                await context.bot.edit_message_text(
                    chat_id=update.effective_chat.id,
                    message_id=preliminary_reply.message_id,
                    text=self.language_service.get_response_string("image_generation_error")
                )
            except Exception as edit_e: 
                logger.error(f"Failed to edit preliminary message during image error handling: {edit_e}")
                await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/news_handler.py ---
======================================================================


# enkibot/core/intent_handlers/news_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
from typing import Optional, Dict, Any, List, TYPE_CHECKING

from telegram import Update
from telegram.ext import ContextTypes, ConversationHandler
from telegram.constants import ChatAction

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator

logger = logging.getLogger(__name__)

# This state constant should ideally be defined in or imported from a central location
# (e.g., telegram_handlers.py or a new enkibot.core.states module)
# For now, ensure it matches the definition in telegram_handlers.py
ASK_NEWS_TOPIC = 2 

class NewsIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer',
                 api_router: 'ApiRouter',
                 response_generator: 'ResponseGenerator',
                 pending_action_data_ref: Dict[int, Dict[str, Any]]):
        logger.info("NewsIntentHandler initialized.")
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer
        self.api_router = api_router
        self.response_generator = response_generator
        self.pending_action_data = pending_action_data_ref # Reference to shared dict

    async def _process_news_request(self, update: Update, context: ContextTypes.DEFAULT_TYPE, topic: Optional[str], original_message_id: Optional[int] = None) -> int:
        """
        Fetches news articles based on the topic (or general if topic is None),
        compiles a summary using an LLM, and sends it to the user.
        Returns ConversationHandler.END.
        """
        if not update.message or not update.effective_chat: 
            logger.warning("NewsHandler._process_news_request called with invalid update/context.")
            return ConversationHandler.END

        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        logger.info(f"NewsHandler: Processing news for topic: '{topic if topic else 'General'}'")
        reply_to_id = original_message_id or update.message.message_id
        
        try:
            articles_structured = await self.api_router.get_latest_news_structured(
                query=topic, 
                lang_code=self.language_service.current_lang
                # num parameter can be added if configurable, defaults to 5 in api_router
            )

            if articles_structured is None: # API call itself failed
                logger.error(f"NewsHandler: API call to get_latest_news_structured failed for topic '{topic}'.")
                await update.message.reply_text(self.language_service.get_response_string("news_api_error"), reply_to_message_id=reply_to_id)
            elif not articles_structured: # API call succeeded but no articles found
                no_articles_key = "news_api_no_articles" if topic else "news_api_no_general_articles"
                await update.message.reply_text(self.language_service.get_response_string(no_articles_key, query=topic or ""), reply_to_message_id=reply_to_id)
            else: # Articles found, proceed to compile
                compiler_prompts = self.language_service.get_llm_prompt_set("news_compiler")
                if not (compiler_prompts and "system" in compiler_prompts and compiler_prompts.get("user_template")):
                    logger.error("NewsHandler: News compiler LLM prompts are missing or malformed (expecting 'user_template').")
                    await update.message.reply_text(self.language_service.get_response_string("news_api_data_error"), reply_to_message_id=reply_to_id)
                else:
                    compiled_response = await self.response_generator.compile_news_response(
                        articles_structured=articles_structured, topic=topic,
                        lang_code=self.language_service.current_lang,
                        system_prompt=compiler_prompts["system"],
                        user_prompt_template=compiler_prompts["user_template"]
                    )
                    await update.message.reply_text(compiled_response, disable_web_page_preview=True, reply_to_message_id=reply_to_id)
        
        except Exception as e:
            logger.error(f"NewsHandler: Unhandled exception in _process_news_request for topic '{topic}': {e}", exc_info=True)
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"), reply_to_message_id=reply_to_id)
        
        finally: 
            # Clean up conversation state regardless of success or failure in processing
            chat_id = update.effective_chat.id
            if chat_id in self.pending_action_data and \
               self.pending_action_data[chat_id].get("action_type") == "ask_news_topic":
                del self.pending_action_data[chat_id]
            if context.user_data and context.user_data.get('conversation_state') == ASK_NEWS_TOPIC:
                context.user_data.pop('conversation_state')
            return ConversationHandler.END

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        """
        Handles an initial news intent (triggered by master_intent == NEWS_QUERY).
        Extracts topic or asks for it.
        """
        logger.info(f"NewsHandler: Handling initial NEWS_QUERY: '{user_msg_txt}'")
        if not update.message or not update.effective_chat: return ConversationHandler.END

        news_topic_prompts = self.language_service.get_llm_prompt_set("news_topic_extractor")
        if not (news_topic_prompts and "system" in news_topic_prompts and news_topic_prompts.get("user_template")): 
            logger.error("NewsHandler: News topic extractor LLM prompts missing or malformed (expecting 'user_template').")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END
        
        topic = await self.intent_recognizer.extract_news_topic_with_llm( 
            text=user_msg_txt, 
            lang_code=self.language_service.current_lang, 
            system_prompt=news_topic_prompts["system"], 
            user_prompt_template=news_topic_prompts["user_template"] 
        )

        if topic: 
            return await self._process_news_request(update, context, topic, update.message.message_id)
        else: 
            logger.info("NewsHandler: No topic identified from initial query, asking user.")
            self.pending_action_data[update.effective_chat.id] = {
                "action_type": "ask_news_topic",
                "original_message_id": update.message.message_id
            }
            context.user_data['conversation_state'] = ASK_NEWS_TOPIC # Ensure state is set in PTB
            await update.message.reply_text(self.language_service.get_response_string("news_ask_topic"))
            return ASK_NEWS_TOPIC

    async def handle_command_entry(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        """
        Handles the /news command. This will always ask for a topic.
        """
        logger.info(f"NewsHandler: /news command received by user {update.effective_user.id if update.effective_user else 'Unknown'}")
        if not update.message or not update.effective_chat or not update.effective_user: 
            return ConversationHandler.END 
        
        # For /news, directly ask for topic.
        self.pending_action_data[update.effective_chat.id] = {
            "action_type": "ask_news_topic",
            "original_message_id": update.message.message_id
        }
        context.user_data['conversation_state'] = ASK_NEWS_TOPIC # Ensure state is set in PTB
        await update.message.reply_text(self.language_service.get_response_string("news_ask_topic"))
        return ASK_NEWS_TOPIC

    async def handle_topic_response(self, update: Update, context: ContextTypes.DEFAULT_TYPE, original_message_id: Optional[int]) -> int:
        """
        Handles the user's response when they provide a news topic after being asked.
        """
        if not update.message or not update.message.text:
            # If user sends something non-text (e.g., sticker) or empty message
            await update.message.reply_text(self.language_service.get_response_string("news_ask_topic")) # Re-ask
            return ASK_NEWS_TOPIC # Remain in the same state

        user_reply_topic_text = update.message.text.strip()
        logger.info(f"NewsHandler: Received news topic reply '{user_reply_topic_text}' in ASK_NEWS_TOPIC state.")

        extractor_prompts = self.language_service.get_llm_prompt_set("news_topic_reply_extractor")
        logger.debug(f"NewsHandler: news_topic_reply_extractor prompts: {extractor_prompts}") 
        
        if not (extractor_prompts and "system" in extractor_prompts and extractor_prompts.get("user_template")):
            logger.error("NewsHandler: News topic reply extractor LLM prompts are missing or malformed (expecting 'user_template').")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END # End conversation on prompt error

        extracted_topic = await self.intent_recognizer.extract_topic_from_reply(
            text=user_reply_topic_text,
            lang_code=self.language_service.current_lang,
            system_prompt=extractor_prompts["system"],
            user_prompt_template=extractor_prompts["user_template"] 
        )

        if extracted_topic: 
            if extracted_topic.lower() == "none": # If LLM explicitly says "None"
                 logger.info(f"NewsHandler: LLM explicitly extracted 'None' as topic from reply '{user_reply_topic_text}'. Processing as general news.")
                 return await self._process_news_request(update, context, None, original_message_id) # Treat as general
            return await self._process_news_request(update, context, extracted_topic, original_message_id)
        else: # intent_recognizer returned None (e.g., LLM call failed or returned empty after stripping "none")
            logger.info(f"NewsHandler: LLM couldn't extract specific topic (returned None/empty) from reply '{user_reply_topic_text}'. Processing as general news.")
            return await self._process_news_request(update, context, None, original_message_id)


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/intent_handlers/weather_handler.py ---
======================================================================


# enkibot/core/intent_handlers/weather_handler.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
from typing import Optional, Dict, Any, TYPE_CHECKING
from telegram import Update
from telegram.ext import ContextTypes, ConversationHandler
from telegram.constants import ChatAction

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator

logger = logging.getLogger(__name__)
ASK_CITY = 1 # Define state here or import from a central states definition

class WeatherIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer',
                 api_router: 'ApiRouter',
                 response_generator: 'ResponseGenerator',
                 pending_action_data_ref: Dict[int, Dict[str, Any]]): # Reference to the main pending_action_data
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer
        self.api_router = api_router
        self.response_generator = response_generator
        self.pending_action_data = pending_action_data_ref # Use the shared dictionary

    async def _process_weather_request(self, update: Update, context: ContextTypes.DEFAULT_TYPE, city: str, original_message_id: Optional[int] = None) -> int:
        # This is the _process_weather_request method moved from TelegramHandlerService
        if not update.message or not update.effective_chat: return ConversationHandler.END
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        logger.info(f"WeatherHandler: Processing weather for city: '{city}'")
        
        forecast_data = await self.api_router.get_weather_data_structured(
            location=city, 
            lang_pack_full=self.language_service.current_lang_pack_full
        )
        reply_to_id = original_message_id or update.message.message_id
        if forecast_data:
            compiler_prompts = self.language_service.get_llm_prompt_set("weather_forecast_compiler")
            if not (compiler_prompts and "system" in compiler_prompts and "user_template" in compiler_prompts):
                logger.error("Weather forecast compiler LLM prompts are missing.")
                await update.message.reply_text(self.language_service.get_response_string("weather_api_data_error", location=city), reply_to_message_id=reply_to_id)
            else:
                compiled_response = await self.response_generator.compile_weather_forecast_response(
                    forecast_data_structured=forecast_data,
                    lang_code=self.language_service.current_lang,
                    system_prompt=compiler_prompts["system"],
                    user_prompt_template=compiler_prompts["user_template"]
                )
                await update.message.reply_text(compiled_response, reply_to_message_id=reply_to_id)
        else:
            await update.message.reply_text(self.language_service.get_response_string("weather_city_not_found", location=city), reply_to_message_id=reply_to_id)
        return ConversationHandler.END

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> Optional[int]:
        # This is the _handle_weather_intent method logic
        logger.info(f"WeatherHandler: Initial WEATHER_QUERY: '{user_msg_txt}'")
        if not update.message or not update.effective_chat: return ConversationHandler.END

        location_extract_prompts = self.language_service.get_llm_prompt_set("location_extractor")
        if not (location_extract_prompts and "system" in location_extract_prompts):
            logger.error("Location extractor LLM prompts missing.")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END

        city = await self.intent_recognizer.extract_location_with_llm(
            text=user_msg_txt, lang_code=self.language_service.current_lang,
            system_prompt=location_extract_prompts["system"], 
            user_prompt_template=location_extract_prompts.get("user","{text}")
        )
        if city: # This will be false because extract_location_with_llm returns None
            return await self._process_weather_request(update, context, city, update.message.message_id)
        else:
            logger.info("WeatherHandler: No city identified, asking user.")
            self.pending_action_data[update.effective_chat.id] = {
                "action_type": "ask_city_weather", 
                "original_message_id": update.message.message_id 
            }
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city"))
            return ASK_CITY 

    async def handle_city_response(self, update: Update, context: ContextTypes.DEFAULT_TYPE, original_message_id: Optional[int]) -> int:
        """
        Handles the user's response when they provide a city name after being asked.
        """
        if not update.message or not update.message.text:
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city"))
            return ASK_CITY # Re-ask

        user_reply_city_text = update.message.text.strip()
        logger.info(f"WeatherHandler: Received city reply '{user_reply_city_text}' in ASK_CITY state.")

        extractor_prompts = self.language_service.get_llm_prompt_set("location_reply_extractor")
        if not (extractor_prompts and "system" in extractor_prompts and "user_template" in extractor_prompts):
            logger.error("Location reply extractor LLM prompts are missing or malformed.")
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message"))
            return ConversationHandler.END
            
        extracted_city = await self.intent_recognizer.extract_location_from_reply(
            text=user_reply_city_text,
            lang_code=self.language_service.current_lang,
            system_prompt=extractor_prompts["system"],
            user_prompt_template=extractor_prompts["user_template"]
        )

        if extracted_city:
            return await self._process_weather_request(update, context, extracted_city, original_message_id)
        else:
            # LLM couldn't extract a clear city from the reply
            await update.message.reply_text(self.language_service.get_response_string("weather_ask_city_failed_extraction", 
                                                                                      "Sorry, I didn't quite catch the city name. Could you please tell me the city again?"))
            return ASK_CITY # Remain in the same state, re-prompting



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/language_service.py ---
======================================================================

# enkibot/core/language_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.


import logging
import json
import os
import re
from typing import Dict, Any, Optional, List

from telegram import Update 

from enkibot import config 
from enkibot.core.llm_services import LLMServices 
from enkibot.utils.database import DatabaseManager 

logger = logging.getLogger(__name__)

class LanguageService:
    def __init__(self, 
                 llm_services: LLMServices, 
                 db_manager: DatabaseManager,
                 lang_packs_dir: str = config.LANGUAGE_PACKS_DIR, 
                 default_lang: str = config.DEFAULT_LANGUAGE):
        
        logger.info("LanguageService __init__ STARTING")
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.lang_packs_dir = lang_packs_dir
        self.default_language = default_lang
        self.primary_fallback_lang = default_lang 
        self.secondary_fallback_lang = "ru"

        self.language_packs: Dict[str, Dict[str, Any]] = {}
        self.llm_prompt_sets: Dict[str, Dict[str, Dict[str, str]]] = {}
        self.response_strings: Dict[str, Dict[str, str]] = {}
        
        self.current_lang: str = self.default_language
        self.current_llm_prompt_sets: Dict[str, Dict[str, str]] = {}
        self.current_response_strings: Dict[str, str] = {}
        self.current_lang_pack_full: Dict[str, Any] = {}

        self._load_language_packs() 
        logger.info("LanguageService __init__ COMPLETED")

    def _load_language_packs(self):
        self.language_packs = {}
        self.llm_prompt_sets = {}
        self.response_strings = {}
        if not os.path.exists(self.lang_packs_dir):
            logger.error(f"Language packs directory not found: {self.lang_packs_dir}")
            try:
                os.makedirs(self.lang_packs_dir, exist_ok=True)
                logger.info(f"Created language packs directory: {self.lang_packs_dir}")
            except OSError as e:
                logger.error(f"Could not create language packs directory {self.lang_packs_dir}: {e}")
        
        for lang_file in os.listdir(self.lang_packs_dir):
            if lang_file.endswith(".json"):
                lang_code = lang_file[:-5]
                file_path = os.path.join(self.lang_packs_dir, lang_file)
                try:
                    with open(file_path, 'r', encoding='utf-8-sig') as f: 
                        pack_content = json.load(f)
                        self.language_packs[lang_code] = pack_content
                        self.llm_prompt_sets[lang_code] = pack_content.get("prompts", {})
                        self.response_strings[lang_code] = pack_content.get("responses", {})
                        logger.info(f"Successfully loaded language pack: {lang_code} from {file_path}")
                except json.JSONDecodeError as jde:
                    logger.error(f"Error decoding JSON from language file: {lang_file}. Error: {jde.msg} at L{jde.lineno} C{jde.colno} (char {jde.pos})", exc_info=False)
                except Exception as e:
                    logger.error(f"Error loading language file {lang_file}: {e}", exc_info=True)
        
        self._set_current_language_internals(self.default_language)

    def _set_current_language_internals(self, lang_code_to_set: str):
        chosen_lang_code = lang_code_to_set
        if chosen_lang_code not in self.language_packs:
            logger.warning(f"Language pack for initially requested '{chosen_lang_code}' not found.")
            if self.primary_fallback_lang in self.language_packs:
                logger.info(f"Falling back to primary fallback: '{self.primary_fallback_lang}'.")
                chosen_lang_code = self.primary_fallback_lang
            elif self.secondary_fallback_lang in self.language_packs:
                logger.info(f"Primary fallback '{self.primary_fallback_lang}' not found. Falling back to secondary: '{self.secondary_fallback_lang}'.")
                chosen_lang_code = self.secondary_fallback_lang
            elif self.language_packs: 
                first_available = next(iter(self.language_packs))
                logger.error(f"Fallbacks ('{self.primary_fallback_lang}', '{self.secondary_fallback_lang}') not found. Using first available: '{first_available}'.")
                chosen_lang_code = first_available
            else: 
                logger.critical("CRITICAL: No language packs loaded at all. Service may be impaired.")
                self.current_lang = "none" 
                self.current_lang_pack_full = {}
                self.current_llm_prompt_sets = {}
                self.current_response_strings = {}
                return

        self.current_lang = chosen_lang_code
        self.current_lang_pack_full = self.language_packs.get(chosen_lang_code, {})
        self.current_llm_prompt_sets = self.llm_prompt_sets.get(chosen_lang_code, {})
        self.current_response_strings = self.response_strings.get(chosen_lang_code, {})
        
        if not self.current_llm_prompt_sets and not self.current_response_strings:
             logger.error(f"Language '{self.current_lang}' pack loaded, but it seems empty (no 'prompts' or 'responses').")
        else:
            logger.info(f"LanguageService: Successfully set current language context to: '{self.current_lang}'")

    async def _create_and_load_language_pack(self, new_lang_code: str, update_context: Optional[Update] = None) -> bool:
        logger.info(f"LanguageService: Attempting to create language pack for new language: {new_lang_code}")
        english_pack_key = "en"
        if english_pack_key not in self.language_packs:
            logger.error(f"Cannot create new language pack: Source English ('{english_pack_key}') pack not found.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback", 
                                                                                           "My apologies, I'm having trouble setting up support for this language right now (missing base files)."))
            return False

        english_pack_content_str = json.dumps(self.language_packs[english_pack_key], ensure_ascii=False, indent=2)
        target_language_name = new_lang_code 

        translation_system_prompt = (
             f"You are an expert translation AI. Your task is to translate a complete JSON language pack from English to {target_language_name} (language code: {new_lang_code}).\n"
            "You MUST maintain the original JSON structure and all original keys (e.g., \"prompts\", \"responses\", \"weather_conditions_map\", \"days_of_week\", and all keys within them). Only translate the string values associated with the keys.\n"
            "The output MUST be a single, valid JSON object and nothing else. Do not add any explanatory text, comments, or markdown before or after the JSON.\n"
            "Ensure all translated strings are appropriate for a friendly AI assistant and are natural-sounding in the target language. Pay special attention to escaping characters within JSON strings if necessary (e.g. double quotes inside a string should be \\\", newlines as \\n)."
        )
        translation_user_prompt = f"Translate the following English JSON language pack to {target_language_name} ({new_lang_code}):\n\n{english_pack_content_str}"
        
        messages_for_api = [{"role": "system", "content": translation_system_prompt}, {"role": "user", "content": translation_user_prompt}]
        response_format_arg = {"response_format": {"type": "json_object"}}
        
        translated_content_str: Optional[str] = None
        try:
            translator_model_id = config.OPENAI_TRANSLATION_MODEL_ID 
            logger.info(f"Using model {translator_model_id} for language pack translation to {new_lang_code}")
            translated_content_str = await self.llm_services.call_openai_llm(
                messages_for_api, model_id=translator_model_id, 
                temperature=0.1, max_tokens=4000, **response_format_arg
            )
        except Exception as e:
            logger.error(f"LLM call itself failed during translation for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False

        if not translated_content_str:
            logger.error(f"LLM failed to provide a translation string for {new_lang_code}.")
            if update_context and update_context.effective_message:
                 await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False
        
        clean_response = translated_content_str.strip() 
        try:
            match = re.search(r"```json\s*(.*?)\s*```", clean_response, re.DOTALL | re.IGNORECASE)
            if match: clean_response = match.group(1).strip()
            else:
                if clean_response.startswith("```json"): clean_response = clean_response[7:]
                if clean_response.endswith("```"): clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            logger.debug(f"Attempting to parse cleaned LLM translation for {new_lang_code}: '{clean_response[:300]}...'")
            translated_pack_content = json.loads(clean_response) 
            
            if not all(k in translated_pack_content for k in ["prompts", "responses", "weather_conditions_map", "days_of_week"]):
                logger.error(f"Translated pack for {new_lang_code} is missing core top-level keys. Aborting save.")
                raise ValueError("Translated JSON missing core keys.")

            new_pack_path = os.path.join(self.lang_packs_dir, f"{new_lang_code}.json")
            with open(new_pack_path, 'w', encoding='utf-8') as f: 
                json.dump(translated_pack_content, f, ensure_ascii=False, indent=2)
            logger.info(f"Successfully created and saved new language pack: {new_lang_code}.json")
            
            self.language_packs[new_lang_code] = translated_pack_content
            self.llm_prompt_sets[new_lang_code] = translated_pack_content.get("prompts", {})
            self.response_strings[new_lang_code] = translated_pack_content.get("responses", {})
            logger.info(f"New language pack for {new_lang_code} is now available at runtime.")
            return True
        except json.JSONDecodeError as jde:
            logger.error(
                f"Failed to decode LLM translation JSON for {new_lang_code}. Error: {jde.msg} "
                f"at L{jde.lineno} C{jde.colno} (char {jde.pos}). "
                f"Nearby: '{clean_response[max(0, jde.pos-50):jde.pos+50]}'", 
                exc_info=False 
            )
            log_limit = 3000
            full_resp_to_log = translated_content_str 
            if len(full_resp_to_log) < log_limit: logger.debug(f"Full problematic translated content for {new_lang_code}:\n{full_resp_to_log}")
            else: logger.debug(f"Problematic translated content (first {log_limit} chars) for {new_lang_code}:\n{full_resp_to_log[:log_limit]}")
            if update_context and update_context.effective_message:
                await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False
        except Exception as e:
            logger.error(f"Error processing/saving new lang pack for {new_lang_code}: {e}", exc_info=True)
            if update_context and update_context.effective_message: 
                await update_context.effective_message.reply_text(self.get_response_string("language_pack_creation_failed_fallback"))
            return False

    async def determine_language_context(self, 
                                         current_message_text: Optional[str], 
                                         chat_id: Optional[int], 
                                         update_context: Optional[Update] = None) -> str:
        LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD = 0.70  
        NUM_RECENT_MESSAGES_FOR_CONTEXT = 2 
        MIN_MESSAGE_LENGTH_FOR_LLM_INPUT = 5 
        MIN_AGGREGATED_TEXT_LENGTH_FOR_LLM = 15

        final_candidate_lang_code = self.current_lang if self.current_lang != "none" else self.default_language
        
        lang_detector_prompts = self.get_llm_prompt_set("language_detector_llm")

        if not (lang_detector_prompts and "system" in lang_detector_prompts and \
                lang_detector_prompts.get("user_template_full_context") and \
                lang_detector_prompts.get("user_template_latest_only") ):
            logger.error("LLM language detector prompts are incomplete/missing. Using current/default logic for language.")
            # No LLM detection possible, just ensure current lang is set via fallbacks
            self._set_current_language_internals(final_candidate_lang_code)
            return self.current_lang

        history_context_str = ""
        latest_message_payload = current_message_text or "" 

        if chat_id and (not latest_message_payload.strip() or len(latest_message_payload.strip()) < MIN_MESSAGE_LENGTH_FOR_LLM_INPUT):
            logger.debug(f"Current msg short or absent, fetching {NUM_RECENT_MESSAGES_FOR_CONTEXT} recent msgs from chat {chat_id}.")
            try:
                if self.db_manager:
                    recent_messages = await self.db_manager.get_recent_chat_texts(chat_id, limit=NUM_RECENT_MESSAGES_FOR_CONTEXT)
                    if recent_messages:
                        history_context_str = "\n".join(recent_messages)
                        logger.debug(f"Fetched {len(recent_messages)} messages for lang detection context.")
                else: logger.warning("db_manager not available in determine_language_context for history fetch.")
            except Exception as e: logger.error(f"Error fetching recent chat texts for lang detection: {e}", exc_info=True)
        
        user_prompt_template_key = "user_template_full_context" if history_context_str else "user_template_latest_only"
        user_prompt_template = lang_detector_prompts[user_prompt_template_key] # We checked existence above
            
        user_prompt_for_llm_detector = user_prompt_template.format(
            latest_message=latest_message_payload, 
            history_context=history_context_str 
        )
            
        messages_for_llm_detector = [
            {"role": "system", "content": lang_detector_prompts["system"]},
            {"role": "user", "content": user_prompt_for_llm_detector}
        ]

        llm_detected_primary_lang: Optional[str] = None
        llm_detected_confidence: float = 0.0
        
        aggregated_text_for_llm_prompt_check = f"{history_context_str}\n{latest_message_payload}".strip()

        if aggregated_text_for_llm_prompt_check and len(aggregated_text_for_llm_prompt_check) >= MIN_AGGREGATED_TEXT_LENGTH_FOR_LLM:
            try:
                detection_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID 
                logger.info(f"Requesting LLM language detection with model {detection_model_id} for text: '{aggregated_text_for_llm_prompt_check[:70]}...'")
                
                completion_str = await self.llm_services.call_openai_llm(
                    messages_for_llm_detector, model_id=detection_model_id,
                    temperature=0.0, max_tokens=150, response_format={"type": "json_object"}
                )
                if completion_str:
                    try:
                        detection_result = json.loads(completion_str)
                        logger.info(f"LLM language detection response: {detection_result}")
                        llm_detected_primary_lang = str(detection_result.get("primary_lang", "")).lower()
                        confidence_val = detection_result.get("confidence", 0.0)
                        try: llm_detected_confidence = float(confidence_val)
                        except (ValueError, TypeError): llm_detected_confidence = 0.0
                    except json.JSONDecodeError as e:
                         logger.error(f"Failed to decode JSON from LLM lang detector: {e}. Raw: {completion_str}")
                else:
                    logger.warning("LLM language detector returned no content.")
            except Exception as e:
                logger.error(f"Error calling LLM for language detection: {e}", exc_info=True)
        else:
            logger.info(f"Not enough aggregated text ('{aggregated_text_for_llm_prompt_check[:50]}...') for LLM language detection. Using current/default logic.")

        # Logic to use LLM detection result
        if llm_detected_primary_lang and llm_detected_confidence >= LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD:
            logger.info(f"LLM confidently detected primary lang '{llm_detected_primary_lang}' (conf: {llm_detected_confidence:.2f}).")
            final_candidate_lang_code = llm_detected_primary_lang
        else:
            if llm_detected_primary_lang: 
                 logger.warning(f"LLM detected lang '{llm_detected_primary_lang}' but confidence ({llm_detected_confidence:.2f}) "
                               f"< threshold ({LLM_LANG_DETECTION_CONFIDENCE_THRESHOLD}). Using current/default: {final_candidate_lang_code}")
            # If no LLM detection or low confidence, final_candidate_lang_code remains as initialized (current or default)
        
        # Set language context based on final_candidate_lang_code
        if final_candidate_lang_code not in self.language_packs:
            logger.warning(f"Language pack for candidate '{final_candidate_lang_code}' not found. Attempting creation.")
            if await self._create_and_load_language_pack(final_candidate_lang_code, update_context=update_context):
                self._set_current_language_internals(final_candidate_lang_code)
            else: 
                logger.warning(f"Failed to create pack for '{final_candidate_lang_code}'. Applying prioritized fallbacks.")
                self._set_current_language_internals(self.default_language) 
        else: 
            self._set_current_language_internals(final_candidate_lang_code)
            
        return self.current_lang
    
    def get_llm_prompt_set(self, key: str) -> Optional[Dict[str, str]]:
        current_prompts_to_check = self.current_llm_prompt_sets
        prompt_set = current_prompts_to_check.get(key)
        primary_fallback_lang = self.default_language
        secondary_fallback_lang = "ru"

        if not prompt_set: 
            logger.debug(f"LLM prompt set key '{key}' not in current lang '{self.current_lang}'. Trying '{primary_fallback_lang}'.")
            current_prompts_to_check = self.llm_prompt_sets.get(primary_fallback_lang, {})
            prompt_set = current_prompts_to_check.get(key)
            if not prompt_set and primary_fallback_lang != secondary_fallback_lang: 
                 logger.debug(f"LLM prompt set key '{key}' not in '{primary_fallback_lang}'. Trying '{secondary_fallback_lang}'.")
                 current_prompts_to_check = self.llm_prompt_sets.get(secondary_fallback_lang, {})
                 prompt_set = current_prompts_to_check.get(key)
        
        if not prompt_set:
            logger.error(f"LLM prompt set for key '{key}' ultimately not found.")
            return None
        if not isinstance(prompt_set, dict) or "system" not in prompt_set: 
            logger.error(f"LLM prompt set for key '{key}' (found in lang or fallback) is malformed: {prompt_set}")
            return None
        return prompt_set

    def get_response_string(self, key: str, default_value: Optional[str] = None, **kwargs) -> str:
        raw_string = self.current_response_strings.get(key)
        lang_tried = self.current_lang
        primary_fallback_lang = self.default_language
        secondary_fallback_lang = "ru"

        if raw_string is None: 
            lang_tried = primary_fallback_lang
            raw_string = self.response_strings.get(primary_fallback_lang, {}).get(key)
            if raw_string is None and primary_fallback_lang != secondary_fallback_lang: 
                lang_tried = secondary_fallback_lang
                raw_string = self.response_strings.get(secondary_fallback_lang, {}).get(key)

        if raw_string is None: 
            if default_value is not None: raw_string = default_value
            else: logger.error(f"Response string for key '{key}' ultimately not found. Using placeholder."); raw_string = f"[[Missing response: {key}]]"
        
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError as e:
            logger.error(f"Missing format key '{e}' in response string for key '{key}' (lang tried: {lang_tried}, raw: '{raw_string}')")
            english_raw = self.response_strings.get("en", {}).get(key, f"[[Format error & missing English for key: {key}]]")
            try: return english_raw.format(**kwargs) if kwargs else english_raw
            except KeyError: return f"[[Formatting error for response key: {key} - check placeholders/English pack]]"


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/llm_services.py ---
======================================================================

# enkibot/core/llm_services.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

# ==================================================================================================
# === EnkiBot LLM Services ===
# ==================================================================================================


import logging
import httpx
import openai 
import asyncio
from typing import List, Dict, Optional, Any, Tuple

from enkibot import config

logger = logging.getLogger(__name__)

class LLMServices:
    def __init__(self, openai_api_key: Optional[str], openai_model_id: str,
                 groq_api_key: Optional[str], groq_model_id: str, groq_endpoint_url: str,
                 openrouter_api_key: Optional[str], openrouter_model_id: str, openrouter_endpoint_url: str,
                 google_ai_api_key: Optional[str], google_ai_model_id: str):
        
        logger.info("***** LLMServices __init__ STARTING *****")
        
        self.openai_api_key = openai_api_key
        self.openai_model_id = openai_model_id 
        self.openai_classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        self.openai_translation_model_id = config.OPENAI_TRANSLATION_MODEL_ID
        # For image generation models
        self.openai_dalle_model_id = config.OPENAI_DALLE_MODEL_ID
        self.openai_multimodal_image_model_id = config.OPENAI_MULTIMODAL_IMAGE_MODEL_ID


        self.openai_async_client: Optional[openai.AsyncOpenAI] = None
        if self.openai_api_key:
            try:
                self.openai_async_client = openai.AsyncOpenAI(api_key=self.openai_api_key)
                logger.info("OpenAI AsyncClient initialized successfully.")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI AsyncClient: {e}")
        else:
            logger.warning("OpenAI API key not provided. OpenAI calls will be disabled.")

        self.groq_api_key = groq_api_key
        self.groq_model_id = groq_model_id
        self.groq_endpoint_url = groq_endpoint_url
        if self.groq_api_key: logger.info(f"Groq configured with key: {self.groq_api_key[:5]}...")
        else: logger.warning("Groq API key not provided.")

        self.openrouter_api_key = openrouter_api_key
        self.openrouter_model_id = openrouter_model_id
        self.openrouter_endpoint_url = openrouter_endpoint_url
        if self.openrouter_api_key: logger.info(f"OpenRouter configured with key: {self.openrouter_api_key[:5]}...")
        else: logger.warning("OpenRouter API key not provided.")
        
        self.google_ai_api_key = google_ai_api_key
        self.google_ai_model_id = google_ai_model_id
        if self.google_ai_api_key: logger.info(f"Google AI configured with key: {self.google_ai_api_key[:5]}...")
        else: logger.warning("Google AI API key not provided.")
        
        logger.info("***** LLMServices __init__ COMPLETED *****")

    def is_provider_configured(self, provider_name: str) -> bool:
        provider_name_lower = provider_name.lower()
        if provider_name_lower == "openai":
            return bool(self.openai_async_client and self.openai_api_key)
        elif provider_name_lower == "groq":
            return bool(self.groq_api_key and self.groq_endpoint_url and self.groq_model_id)
        elif provider_name_lower == "openrouter":
            return bool(self.openrouter_api_key and self.openrouter_endpoint_url and self.openrouter_model_id)
        return False

    async def call_openai_llm(self, messages: List[Dict[str, str]], 
                              model_id: Optional[str] = None,
                              temperature: float = 0.7, 
                              max_tokens: int = 2000, 
                              **kwargs) -> Optional[str]:
        if not self.is_provider_configured("openai"):
            logger.warning("OpenAI client not initialized or API key missing. Cannot make call.")
            return None
        
        actual_model_id = model_id or self.openai_model_id 
        logger.info(f"Calling OpenAI (model: {actual_model_id}) with {len(messages)} messages.")

        call_params = { "model": actual_model_id, "messages": messages, "temperature": temperature, "max_tokens": max_tokens, **kwargs }
        try:
            logger.debug(f"Before OpenAI completions.create with params: model='{call_params.get('model')}', temp: {call_params.get('temperature')}")
            completion = await self.openai_async_client.chat.completions.create(**call_params)
            if completion.choices and completion.choices[0].message and completion.choices[0].message.content:
                response_content = completion.choices[0].message.content.strip()
                logger.info(f"OpenAI successful response (first 50 chars): {response_content[:50]}")
                return response_content
            logger.warning(f"OpenAI call to {actual_model_id} returned no content. Choices: {completion.choices}")
            return None
        except openai.APIStatusError as e: 
            logger.error(f"OpenAI API Status Error (model: {actual_model_id}): HTTP Status {e.status_code if hasattr(e, 'status_code') else 'N/A'} - {e.message if hasattr(e, 'message') else str(e)}", exc_info=False)
            return None
        except openai.APIError as e: 
            logger.error(f"OpenAI API Error (model: {actual_model_id}): {e.message if hasattr(e, 'message') else str(e)}", exc_info=False)
            return None
        except Exception as e:
            logger.error(f"Unexpected error with OpenAI API (model: {actual_model_id}): {e}", exc_info=True)
            return None

    async def generate_image_with_dalle(self, 
                                        prompt: str,
                                        n: int = 1,
                                        size: str = "1024x1024",
                                        quality: str = "standard",
                                        response_format: str = "url"
                                        ) -> Optional[List[Dict[str, Any]]]:
        if not self.is_provider_configured("openai"):
            logger.error("OpenAI client not configured. Cannot generate image with DALL-E.")
            return None

        model_to_use = self.openai_dalle_model_id 
        logger.info(f"Requesting image generation via DALL-E API with prompt: '{prompt[:70]}...' using model {model_to_use}")

        try:
            response = await self.openai_async_client.images.generate(
                model=model_to_use,
                prompt=prompt,
                n=n,
                size=size,
                quality=quality,
                response_format=response_format
            )
            
            image_data_list = []
            if response.data:
                for image_object in response.data:
                    if response_format == "url" and image_object.url:
                        image_data_list.append({"url": image_object.url})
                        logger.info(f"DALL-E generated image URL: {image_object.url}")
                    elif response_format == "b64_json" and image_object.b64_json:
                        image_data_list.append({"b64_json": image_object.b64_json})
            
            return image_data_list if image_data_list else None
        except openai.APIError as e:
            logger.error(f"OpenAI DALL-E API Error (model: {model_to_use}): Status {e.status_code if hasattr(e, 'status_code') else 'N/A'} - {e.message if hasattr(e, 'message') else str(e)}", exc_info=False)
        except Exception as e:
            logger.error(f"Unexpected error during DALL-E image generation: {e}", exc_info=True)
        return None

    async def generate_image_with_responses_api(self, prompt: str) -> Optional[List[Dict[str, str]]]:
        if not self.is_provider_configured("openai"):
            logger.error("OpenAI client not configured. Cannot generate image via Responses API.")
            return None

        model_to_use = self.openai_multimodal_image_model_id
        logger.info(f"Requesting image generation via Responses API with prompt: '{prompt[:70]}...' using model {model_to_use}")

        try:
            response = await self.openai_async_client.responses.create(
                model=model_to_use,
                input=prompt,
                tools=[{"type": "image_generation"}] 
            )
            image_data_list = []
            if response.output:
                for output_item in response.output:
                    if output_item.type == "image_generation_call" and output_item.result:
                        image_data_list.append({"b64_json": output_item.result})
                        logger.info(f"Image generated via Responses API (base64, first 50 chars): {output_item.result[:50]}...")
            return image_data_list if image_data_list else None
        except openai.APIError as e:
            logger.error(f"OpenAI Responses API Error (model: {model_to_use}, image gen): Status {e.status_code if hasattr(e, 'status_code') else 'N/A'} - {e.message if hasattr(e, 'message') else str(e)}", exc_info=False)
        except Exception as e:
            logger.error(f"Unexpected error during Responses API image generation: {e}", exc_info=True)
        return None

    async def call_llm_api(self, provider_name: str, api_key: Optional[str], endpoint_url: Optional[str], 
                            model_id: str, messages: List[Dict[str, str]], 
                            temperature: float = 0.7, max_tokens: int = 2000,
                            **kwargs 
                            ) -> Optional[str]:
        if not api_key or not endpoint_url:
            logger.warning(f"{provider_name} not configured (key or URL missing). Skipping call.")
            return None
        
        logger.info(f"Calling {provider_name} (model: {model_id}) with {len(messages)} messages.")
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        if provider_name.lower() == "openrouter": 
            headers.update({"HTTP-Referer": "http://localhost:8000", "X-Title": "EnkiBot"})

        payload = {"model": model_id, "messages": messages, "max_tokens": max_tokens, "temperature": temperature, **kwargs}
        
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.post(endpoint_url, json=payload, headers=headers, timeout=30.0)
            resp.raise_for_status()
            data = resp.json()
            if data.get("choices") and data["choices"][0].get("message") and data["choices"][0]["message"].get("content"):
                response_content = data["choices"][0]["message"]["content"].strip()
                logger.info(f"{provider_name} successful response (first 50 chars): {response_content[:50]}")
                return response_content
            logger.warning(f"{provider_name} call to {model_id} returned no content. Data: {data.get('choices')}")
            return None
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error for {provider_name} ({model_id}): {e.response.status_code} - Response: {e.response.text[:500]}...", exc_info=False)
            return None
        except Exception as e:
            logger.error(f"Unexpected error with {provider_name} API ({model_id}): {e}", exc_info=True)
            return None

    async def race_llm_calls(self, messages: List[Dict[str, str]]) -> Optional[str]:
        task_info: List[Tuple[asyncio.Task, str]] = []

        if self.is_provider_configured("openai"):
            task = asyncio.create_task(self.call_openai_llm(messages, model_id=self.openai_model_id))
            task_info.append((task, "OpenAI"))
        if self.is_provider_configured("groq"):
            task = asyncio.create_task(self.call_llm_api("Groq", self.groq_api_key, self.groq_endpoint_url, self.groq_model_id, messages))
            task_info.append((task, "Groq"))
        if self.is_provider_configured("openrouter"):
            task = asyncio.create_task(self.call_llm_api("OpenRouter", self.openrouter_api_key, self.openrouter_endpoint_url, self.openrouter_model_id, messages))
            task_info.append((task, "OpenRouter"))
        
        if not task_info:
            logger.warning("No LLM providers configured for racing calls.")
            return None

        logger.info(f"Racing LLM calls to: {[name for _, name in task_info]}")
        
        task_to_provider_map = {task_obj: name for task_obj, name in task_info}
        tasks_only = [task_obj for task_obj, _ in task_info]

        for future in asyncio.as_completed(tasks_only):
            provider_name_for_log = task_to_provider_map.get(future, "UnknownProvider") 
            try:
                result = await future
                if result and result.strip():
                    logger.info(f"Successful response from {provider_name_for_log} in race.")
                    return result.strip()
                else:
                    logger.warning(f"{provider_name_for_log} returned no content in race.")
            except Exception as e: 
                logger.warning(f"Provider {provider_name_for_log} task failed in race: {type(e).__name__} - {e}")
        
        logger.error("All LLM providers failed or returned no content in race_llm_calls.")
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/core/telegram_handlers.py ---
======================================================================

# enkibot/core/telegram_handlers.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
import asyncio
import json 
from typing import Optional, Dict, Any, List, TYPE_CHECKING

from telegram import Update, ReplyKeyboardRemove
from telegram.ext import Application, ContextTypes, ConversationHandler, CommandHandler, MessageHandler, filters
from telegram.constants import ChatAction
import re 

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.utils.database import DatabaseManager
    from enkibot.core.llm_services import LLMServices
    from enkibot.modules.intent_recognizer import IntentRecognizer
    from enkibot.modules.profile_manager import ProfileManager
    from enkibot.modules.api_router import ApiRouter
    from enkibot.modules.response_generator import ResponseGenerator
    # Import new handlers
    from .intent_handlers.weather_handler import WeatherIntentHandler
    from .intent_handlers.news_handler import NewsIntentHandler
    from .intent_handlers.general_handler import GeneralIntentHandler
    from .intent_handlers.image_generation_handler import ImageGenerationIntentHandler


from enkibot import config as bot_config
# Import specialized handlers
from .intent_handlers.weather_handler import WeatherIntentHandler 
from .intent_handlers.news_handler import NewsIntentHandler
from .intent_handlers.general_handler import GeneralIntentHandler
from .intent_handlers.image_generation_handler import ImageGenerationIntentHandler


logger = logging.getLogger(__name__)

# Conversation states
ASK_CITY = 1
ASK_NEWS_TOPIC = 2 

class TelegramHandlerService:
    def __init__(self, 
                 application: Application, 
                 db_manager: 'DatabaseManager', 
                 llm_services: 'LLMServices',   
                 intent_recognizer: 'IntentRecognizer', 
                 profile_manager: 'ProfileManager',     
                 api_router: 'ApiRouter',             
                 response_generator: 'ResponseGenerator', 
                 language_service: 'LanguageService',
                 allowed_group_ids: set, 
                 bot_nicknames: list
                ):
        logger.info("TelegramHandlerService __init__ STARTING")
        self.application = application
        self.db_manager = db_manager
        self.llm_services = llm_services 
        self.intent_recognizer = intent_recognizer
        self.profile_manager = profile_manager
        self.api_router = api_router
        self.response_generator = response_generator
        self.language_service = language_service
        
        self.allowed_group_ids = allowed_group_ids 
        self.bot_nicknames = bot_nicknames 
        
        self.pending_action_data: Dict[int, Dict[str, Any]] = {}

        # Instantiate specialized handlers
        self.weather_handler = WeatherIntentHandler(
            language_service=self.language_service,
            intent_recognizer=self.intent_recognizer,
            api_router=self.api_router,
            response_generator=self.response_generator,
            pending_action_data_ref=self.pending_action_data
        )
        self.news_handler = NewsIntentHandler(
            language_service=self.language_service,
            intent_recognizer=self.intent_recognizer,
            api_router=self.api_router,
            response_generator=self.response_generator,
            pending_action_data_ref=self.pending_action_data
        )
        self.general_handler = GeneralIntentHandler(
            language_service=self.language_service,
            response_generator=self.response_generator
        )
        self.image_generation_handler = ImageGenerationIntentHandler(
            language_service=self.language_service,
            intent_recognizer=self.intent_recognizer,
            llm_services=self.llm_services 
        )
        logger.info("TelegramHandlerService __init__ COMPLETED")

    async def log_message_and_profile_tasks(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not update.message or not update.message.text or not update.effective_user: 
            logger.debug("log_message_and_profile_tasks: Skipping.")
            return
        chat_id = update.effective_chat.id
        user = update.effective_user
        message = update.message
        if self.allowed_group_ids and chat_id not in self.allowed_group_ids: 
            return 
        current_lang_for_log = self.language_service.current_lang 
        action_taken = await self.db_manager.log_chat_message_and_upsert_user(
            chat_id=chat_id, user_id=user.id, username=user.username,
            first_name=user.first_name, last_name=user.last_name,
            message_id=message.message_id, message_text=message.text,
            preferred_language=current_lang_for_log )
        logger.info(f"Message from user {user.id} logged. Profile action: {action_taken}.")
        name_var_prompts = self.language_service.get_llm_prompt_set("name_variation_generator")
        if action_taken and action_taken.lower() == "insert" and name_var_prompts and "system" in name_var_prompts:
            asyncio.create_task(self.profile_manager.populate_name_variations_with_llm(
                user_id=user.id, first_name=user.first_name, last_name=user.last_name, username=user.username,
                system_prompt=name_var_prompts["system"], user_prompt_template=name_var_prompts.get("user","Generate for: {name_info}")))
        elif action_taken and action_taken.lower() == "insert" and not name_var_prompts:
             logger.warning("Could not generate name variations: name_variation_generator prompt missing.")
        profile_create_prompts = self.language_service.get_llm_prompt_set("profile_creator")
        profile_update_prompts = self.language_service.get_llm_prompt_set("profile_updater")
        if message.text and len(message.text.strip()) > 10:
            if profile_create_prompts and "system" in profile_create_prompts and \
               profile_update_prompts and "system" in profile_update_prompts:
                asyncio.create_task(self.profile_manager.analyze_and_update_user_profile(
                    user_id=user.id, message_text=message.text,
                    create_system_prompt=profile_create_prompts["system"], create_user_prompt_template=profile_create_prompts.get("user","Analyze: {message_text}"),
                    update_system_prompt=profile_update_prompts["system"], update_user_prompt_template=profile_update_prompts.get("user","Update based on: {message_text} with existing: {current_profile_notes}")))
            else: logger.warning(f"Profile prompts missing for lang '{current_lang_for_log}'. Skipping profile analysis for user {user.id}.")

    async def _is_triggered(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt_lower: str) -> bool:
        if not update.message or not context.bot: return False
        current_chat_id = update.effective_chat.id
        is_group = update.message.chat.type in ['group', 'supergroup']
        if not is_group: return True
        if self.allowed_group_ids and current_chat_id not in self.allowed_group_ids: return False
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        is_at_mentioned = bool(bot_username_lower and f"@{bot_username_lower}" in user_msg_txt_lower)
        is_nickname_mentioned = any(re.search(r'\b' + re.escape(nick.lower()) + r'\b', user_msg_txt_lower, re.I) for nick in self.bot_nicknames)
        is_bot_mentioned = is_at_mentioned or is_nickname_mentioned
        is_reply_to_bot = (update.message.reply_to_message and update.message.reply_to_message.from_user and context.bot and update.message.reply_to_message.from_user.id == context.bot.id)
        final_trigger_decision = is_bot_mentioned or is_reply_to_bot
        if is_group and final_trigger_decision: logger.info(f"_is_triggered: True (Group: {current_chat_id}): @M={is_at_mentioned}, NickM={is_nickname_mentioned}, Reply={is_reply_to_bot}")
        return final_trigger_decision

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]:
        if not update.message or not update.message.text or not update.effective_chat or not update.effective_user: 
            return None 
        
        await self.language_service.determine_language_context(
            update.message.text, 
            chat_id=update.effective_chat.id,
            update_context=update
        )
        
        await self.log_message_and_profile_tasks(update, context)
        
        chat_id = update.effective_chat.id
        user_msg_txt = update.message.text
        
        current_conv_state = context.user_data.get('conversation_state') if context.user_data else None
        pending_action_details = self.pending_action_data.get(chat_id)

        if current_conv_state == ASK_CITY and pending_action_details and pending_action_details.get("action_type") == "ask_city_weather":
            self.pending_action_data.pop(chat_id, None) 
            context.user_data.pop('conversation_state', None)
            original_msg_id = pending_action_details.get("original_message_id")
            # This now calls the refined handler method in WeatherIntentHandler
            return await self.weather_handler.handle_city_response(update, context, original_msg_id)
        elif current_conv_state == ASK_NEWS_TOPIC and pending_action_details and pending_action_details.get("action_type") == "ask_news_topic":
            self.pending_action_data.pop(chat_id, None) 
            context.user_data.pop('conversation_state', None)
            original_msg_id = pending_action_details.get("original_message_id")
            # This now calls the refined handler method in NewsIntentHandler
            return await self.news_handler.handle_topic_response(update, context, original_msg_id)
        user_msg_txt_lower = user_msg_txt.lower()
        if not await self._is_triggered(update, context, user_msg_txt_lower): 
            return None

        master_intent_prompts = self.language_service.get_llm_prompt_set("master_intent_classifier")
        master_intent = "UNKNOWN_INTENT"
        if master_intent_prompts and "system" in master_intent_prompts:
            user_template_for_master = master_intent_prompts.get("user","{text_to_classify}")
            master_intent = await self.intent_recognizer.classify_master_intent(
                text=user_msg_txt, lang_code=self.language_service.current_lang,
                system_prompt=master_intent_prompts["system"], user_prompt_template=user_template_for_master )
        else: 
            logger.error(f"Master intent classification prompt set missing/malformed for lang '{self.language_service.current_lang}'.")
        
        logger.info(f"Master Intent for '{user_msg_txt[:50]}...' (lang: {self.language_service.current_lang}) classified as: {master_intent}")

        next_state: Optional[int] = None
        if master_intent == "WEATHER_QUERY":
            context.user_data['conversation_state'] = ASK_CITY
            next_state = await self.weather_handler.handle_intent(update, context, user_msg_txt)
        elif master_intent == "NEWS_QUERY": 
            context.user_data['conversation_state'] = ASK_NEWS_TOPIC
            next_state = await self.news_handler.handle_intent(update, context, user_msg_txt)
        elif master_intent == "IMAGE_GENERATION_QUERY": 
            await self.image_generation_handler.handle_intent(update, context, user_msg_txt)
            next_state = ConversationHandler.END 
        elif master_intent == "MESSAGE_ANALYSIS_QUERY": 
            await self._handle_message_analysis_query(update, context, user_msg_txt) 
        elif master_intent in ["USER_PROFILE_QUERY", "GENERAL_CHAT", "UNKNOWN_INTENT"]:
            await self.general_handler.handle_request(update, context, user_msg_txt, master_intent)
        else: 
            logger.warning(f"Unhandled master intent type: {master_intent}. Falling back to general handler.")
            await self.general_handler.handle_request(update, context, user_msg_txt, "UNKNOWN_INTENT")
        
        if next_state is None or next_state == ConversationHandler.END:
            if context.user_data and 'conversation_state' in context.user_data:
                context.user_data.pop('conversation_state')
            if chat_id in self.pending_action_data:
                 self.pending_action_data.pop(chat_id, None)
        return next_state

    async def _handle_message_analysis_query(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        is_valid_reply_scenario = (
            update.message and update.message.reply_to_message and 
            update.message.reply_to_message.text and 
            update.message.reply_to_message.from_user and context.bot and 
            update.message.reply_to_message.from_user.id != context.bot.id )
        if not is_valid_reply_scenario:
            logger.info("MESSAGE_ANALYSIS_QUERY classified, but not valid reply. Delegating to GeneralIntentHandler.")
            await self.general_handler.handle_request(update, context, user_msg_txt, "GENERAL_CHAT")
            return
        
        logger.info("TelegramHandlers: Processing MESSAGE_ANALYSIS_QUERY directly.")
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
        original_text = update.message.reply_to_message.text
        question_for_analysis = user_msg_txt
        bot_username_lower = getattr(context.bot, 'username', "").lower() if getattr(context.bot, 'username', None) else ""
        cleaned_question = user_msg_txt.lower()
        for nick in self.bot_nicknames + ([f"@{bot_username_lower}"] if bot_username_lower else []): 
            cleaned_question = cleaned_question.replace(nick.lower(), "").strip()
        if len(cleaned_question) < 5: 
            question_for_analysis = self.language_service.get_response_string("replied_message_default_question")
        
        analyzer_prompts = self.language_service.get_llm_prompt_set("replied_message_analyzer")
        if not (analyzer_prompts and "system" in analyzer_prompts) : 
            logger.error("Prompt set for replied message analysis is missing or malformed."); 
            await update.message.reply_text(self.language_service.get_response_string("generic_error_message")); 
            return 
        
        analysis_result = await self.response_generator.analyze_replied_message(
            original_text=original_text, user_question=question_for_analysis,
            system_prompt=analyzer_prompts["system"], user_prompt_template=analyzer_prompts.get("user") )
        await update.message.reply_text(analysis_result)
        
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.effective_user or not update.message: return
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE) 
        await update.message.reply_html(self.language_service.get_response_string("start", user_mention=update.effective_user.mention_html()))

    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
        if not update.message: return
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE)
        await update.message.reply_text(self.language_service.get_response_string("help"))

    async def news_command_entry(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> Optional[int]: 
        if not update.message or not update.effective_user: return ConversationHandler.END
        self.language_service._set_current_language_internals(bot_config.DEFAULT_LANGUAGE)
        context.user_data['conversation_state'] = ASK_NEWS_TOPIC
        return await self.news_handler.handle_command_entry(update, context)

    async def error_handler(self, update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
        logger.error(f'Update "{update}" caused error "{context.error}"', exc_info=True)
        if isinstance(update, Update) and update.effective_chat:
            try:
                error_msg = self.language_service.get_response_string("generic_error_message", "Oops! Something went very wrong on my end.")
                await context.bot.send_message(chat_id=update.effective_chat.id, text=error_msg)
            except Exception as e: 
                logger.error(f"CRITICAL: Error sending error message to user: {e}", exc_info=True)
    
    async def cancel_conversation(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
        chat_id = update.effective_chat.id
        if chat_id in self.pending_action_data:
            del self.pending_action_data[chat_id]
        if context.user_data and 'conversation_state' in context.user_data:
            context.user_data.pop('conversation_state')
            
        logger.info(f"User {update.effective_user.id if update.effective_user else ''} cancelled conversation.")
        if update.message: 
            await update.message.reply_text(
                self.language_service.get_response_string("conversation_cancelled", "Okay, current operation cancelled."), 
                reply_markup=ReplyKeyboardRemove() 
            )
        return ConversationHandler.END

    def register_all_handlers(self): 
        self.application.add_handler(CommandHandler("start", self.start_command))
        self.application.add_handler(CommandHandler("help", self.help_command))
        
        conv_handler = ConversationHandler(
            entry_points=[
                MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message),
                CommandHandler("news", self.news_command_entry) 
            ],
            states={
                ASK_CITY: [MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message)],
                ASK_NEWS_TOPIC: [MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message)],
            },
            fallbacks=[CommandHandler("cancel", self.cancel_conversation)],
            allow_reentry=True 
        )
        self.application.add_handler(conv_handler)
        self.application.add_error_handler(self.error_handler)
        logger.info("TelegramHandlerService: All handlers registered.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/__init__.py ---
======================================================================

*** ERROR: Could not read file. Reason: 'utf-8' codec can't decode byte 0xf6 in position 947: invalid start byte ***


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/module_tester.py ---
======================================================================

# enkibot/evolution/module_tester.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Module Tester (Placeholder) ===
# ==================================================================================================
# This module will provide the framework for rigorously evaluating evolved
# variants of EnkiBot's modules and prompts.
# It will be responsible for:
# - Executing tests in a secure, sandboxed environment.
# - Running benchmark tests for Python modules (e.g., unit tests, performance tests).
# - Evaluating LLM prompt effectiveness using frameworks like LLM-as-a-Judge.
# - Collecting and returning detailed performance metrics to the coordinator.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

def test_variant(parent_variant, modification):
    """
    Tests a new, modified variant of a module or prompt.

    Args:
        parent_variant: The original version of the bot component.
        modification: The proposed change to be applied.

    Returns:
        A tuple containing the new child variant and its performance data.
    """
    logger.info(f"Testing a new variant with modification: {modification} (mock).")
    # In the future, this function would:
    # 1. Apply the modification in a sandboxed environment.
    # 2. Run a suite of tests (unit, integration, performance).
    # 3. Evaluate against the multi-objective fitness function.
    # 4. Return the results.
    
    mock_performance_data = {"task_success": 0.95, "efficiency": 120, "safety_score": 1.0}
    
    # The new variant would be a representation of the modified code/prompt
    new_child_variant = {"id": "variant-002", "parent": "variant-001", "modification": modification}
    
    return new_child_variant, mock_performance_data


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/evolution/self_improvement_coordinator.py ---
======================================================================

# enkibot/evolution/self_improvement_coordinator.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Self-Improvement Coordinator (Placeholder) ===
# ==================================================================================================
# This module will serve as the central nervous system for EnkiBot's evolution.
# It will be responsible for:
# - Orchestrating the evolutionary loop: selection, modification, evaluation.
# - Managing the "Agent Variant Archive" of different EnkiBot versions.
# - Triggering the modification of Python modules and LLM prompts.
# - Recording performance metadata to guide the evolutionary process.
# ==================================================================================================

import logging

logger = logging.getLogger(__name__)

class SelfImprovementCoordinator:
    def __init__(self):
        logger.info("Self-Improvement Coordinator initialized (Placeholder).")
        # In the future, this will initialize the Agent Variant Archive connection.
        self.agent_variant_archive = {}

    def run_evolutionary_cycle(self):
        """
        Executes a single cycle of selection, modification, and evaluation.
        """
        logger.info("Executing a mock evolutionary cycle...")
        # 1. Select parent variant(s) from the archive.
        # parent = self.select_parent()

        # 2. Propose modifications to code or prompts.
        # modification = self.propose_modification(parent)

        # 3. Create and test the new child variant.
        # child_variant, performance_data = module_tester.test_variant(parent, modification)

        # 4. Add the new variant and its performance data to the archive.
        # self.archive_variant(child_variant, performance_data)
        logger.info("Mock evolutionary cycle complete.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/__init__.py ---
======================================================================

# enkibot/lang/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This directory holds the language packs (JSON files) for EnkiBot.
# Each file corresponds to a language code (e.g., 'en', 'ru') and contains
# all user-facing strings and system prompts for that language.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/en.json ---
======================================================================

﻿{
  "prompts": {
    "master_intent_classifier": {
      "system": "You are an AI routing assistant. Your task is to classify the user's intent based on their message. Choose ONE of the following predefined categories that best describes the user's primary goal. Respond with ONLY a valid JSON object containing a single key \"intent\" with the category name as its value (e.g., {\"intent\": \"WEATHER_QUERY\"}). Ensure your output is strictly in JSON format.\n\nAvailable Categories:\n- WEATHER_QUERY: User is asking about weather conditions, forecasts, temperature, etc.\n- NEWS_QUERY: User is asking for news articles, headlines, or updates on current breaking events or specific news topics.\n- IMAGE_GENERATION_QUERY: User is asking to create, draw, generate, or make a picture or image of something.\n- USER_PROFILE_QUERY: User is asking for information about a specific person (e.g., 'who is [name]?', 'tell me about [name]'), including requests for specific details, facts, lists, or analyses related to that person (e.g., 'Kiyosaki's failed predictions', 'biography of X', 'list of Y's accomplishments').\n- MESSAGE_ANALYSIS_QUERY: User is replying to another message and asking you (the bot) to analyze, summarize, or comment on that replied-to message.\n- GENERAL_CHAT: User is making a general statement, asking a general knowledge question, seeking information or analysis not fitting other specific categories (e.g. 'explain black holes', 'compare X and Y'), or engaging in casual conversation.\n- UNKNOWN_INTENT: If the intent is very unclear or doesn't fit any other category despite the broader definitions.",
      "user_template": "{text_to_classify}"
    },
    "image_generation_prompt_extractor": {
      "system": "You are an AI assistant that extracts the core subject from a user's request to be used as a prompt for an image generation model. Analyze the user's text and output only the essential descriptive part of the request. For example, if the user says 'Enki, can you please draw a picture of a majestic lion in the savanna at sunset?', you should output 'a majestic lion in the savanna at sunset'. If the text does not contain a clear request to generate an image, respond with the single word 'None'.",
      "user_template": "Extract the image generation prompt from this text:\n\n---\n{text}\n---"
    },
    "name_variation_generator": {
      "system": "You are a language expert specializing in Russian and English names. Your task is to generate a list of linguistic variations for a user's name. Focus ONLY on realistic, human-used variations. DO NOT generate technical usernames with numbers or suffixes like '_dev'.\n\n**Goal:** Create variations for recognition in natural language text.\n\n**Categories for Generation:**\n1.  **Original Forms:** The original first name, last name, and combinations.\n2.  **Diminutives & Nicknames:** Common short and affectionate forms (e.g., 'Antonina' -> 'Tonya'; 'Robert' -> 'Rob').\n3.  **Transliteration (with variants):** Provide multiple common Latin spellings for all Cyrillic forms (original and diminutives). Example for 'Тоня': 'tonya', 'tonia'.\n4.  **Reverse Transliteration:** If the source name is Latin, provide plausible Cyrillic versions. Example for 'Yael': 'Яэль', 'Йаэль'.\n5.  **Russian Declensions (Grammatical Cases):** For all primary Russian names (full and short forms), provide their forms in different grammatical cases (genitive, dative, accusative, instrumental, prepositional). Example for 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Output Format:** Return a single JSON object: {\"variations\": [\"variation1\", \"variation2\", ...]}. All variations must be in lowercase.",
      "user_template": "Generate linguistic variations for the user with the following info: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "You are an AI analyst. Your task is to analyze the 'Original Text' and provide a meaningful response to the 'User's Question' about that text. Your analysis should be objective, concise, and to the point. If the question is generic (e.g., 'what do you think?'), provide a brief summary, highlighting the key points or sentiment of the original text.",
      "user_template": "Original Text for analysis:\n---\n\"{original_text}\"\n---\n\nUser's question about this text:\n---\n\"{user_question}\"\n---\n\nYour analysis:"
    },
    "weather_intent_analyzer": {
      "system": "You are an expert in analyzing weather-related requests. Your task is to determine the user's intent. Does the user want the 'current' weather or a 'forecast' for several days? If it is a forecast, also determine for how many days. Your answer MUST be a valid JSON object and nothing else. Ensure the output is formatted as JSON.\n\nExamples:\n- User text: 'weather in London' -> Your response: {\"type\": \"current\"}\n- User text: 'what's the weather like?' -> Your response: {\"type\": \"current\"}\n- User text: 'weather in Tampa for the week' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'forecast for 5 days in Berlin' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'what will the weather be like tomorrow?' -> Your response: {\"type\": \"forecast\", \"days\": 2}\n- User text: 'give me the forecast for Saturday' -> Your response: {\"type\": \"forecast\", \"days\": 7}\n- User text: 'just give me a weather forecast' -> Your response: {\"type\": \"forecast\", \"days\": 5}\n- User text: 'forecast for the weekend' -> Your response: {\"type\": \"forecast\", \"days\": 3}\n\nFallback Rule: If you are unsure, always default to 'current'.",
      "user_template": "{text}"
    },
    "location_extractor": {
      "system": "You are an expert text analysis tool. Your task is to extract a city or location name from the user's text. Analyze the following text and identify the geographical location (city, region, country) mentioned. Return ONLY the name of the location in English, suitable for a weather API query. For example, if the text is 'what's the weather in Saint Petersburg', you must return 'Saint Petersburg'. If no specific location is found, you MUST return the single word: None",
      "user_template": "{text}"
    },
    "news_topic_extractor": {
      "system": "You are an expert text analysis tool. Your task is to extract the main topic, keyword, or location from a user's request for news. Analyze the text. If it contains a specific subject, you MUST return that subject in its base (nominative) case and in the original language of the request. For example, for a request 'news in Moscow', you must return 'Moscow'. For 'news about cars', return 'cars'. If the request is general (e.g., 'what's the news?', 'latest headlines'), you MUST return the single word: None",
      "user_template": "{text}"
    },
    "profile_creator": {
      "system": "You are an AI Behavioral Text Analyst. Your task is to create an initial, highly compressed summary of observable traits and patterns based *solely* on the user's provided message. Focus on:\n1.  **Communication Style:** (e.g., formal/informal, assertive/passive, inquisitive, declarative, use of slang, verbosity, lexical complexity, sentiment polarity observed).\n2.  **Cognitive Indicators:** (e.g., structured thought, associative thinking, problem-solving approach indicated, level of abstraction used).\n3.  **Expressed Interests/Themes:** (Key topics or subjects explicitly mentioned or strongly implied).\n\n**Output Format:** A concise, bulleted list using precise, objective behavioral or linguistic terminology. Avoid subjective interpretations, emotional language, psychobabble, or predicting future behavior/predispositions. Each point should be a factual inference from the text. Do not add introductory or concluding remarks. Focus on density and clarity.",
      "user_template": "Analyze the following message from a new user and generate a compressed behavioral trait summary:\n\nUser's message:\n---\n\"{message_text}\"\n---\n\nYour summary (bulleted list using precise terminology):"
    },
    "profile_updater": {
      "system": "You are an AI Behavioral Text Analyst updating an existing trait summary. Review the 'Existing Summary' and the 'New User Message'. Synthesize a new, highly compressed summary that incorporates new observations and refines previous ones, prioritizing recent information if contradictory. Maintain objectivity and use precise, scientific behavioral or linguistic terminology. Focus on consistency, conciseness, and factual inferences from the texts. Avoid subjective interpretations, emotional language, or predicting future behavior/predispositions. Do not add introductory or concluding remarks.\n\nOutput Format: A concise, bulleted list.",
      "user_template": "Existing Trait Summary:\n---\n{current_profile_notes}\n---\n\nNew User Message for analysis:\n---\n\"{message_text}\"\n---\n\nYour updated and synthesized compressed trait summary (bulleted list using precise terminology):"
    },
    "main_orchestrator": {
      "system": "You are EnkiBot, an intelligent and friendly AI assistant in a Telegram chat, created by Yael Demedetskaya. Your primary goal is to be helpful, engaging, and informative. You have access to long-term memory about conversations and user profiles. When asked about someone, synthesize information from their profile (dossier) and recent messages. **It is CRITICAL that your entire response be in {language_name} (language code: {lang_code}). Do not switch languages.** Respond naturally and be short in your answer. Be polite but not overly formal."
    },
    "language_detector_llm": {
      "system": "You are a language detection expert. Analyze the provided text, which includes the 'Latest User Message' and optionally 'Recent Chat History'. The 'Latest User Message' is the most important for determining its primary language. Respond ONLY with a valid JSON object like: {\"primary_lang\": \"en\", \"confidence\": 0.95, \"other_detected_langs\": [\"fr\", \"de\"]}. 'primary_lang' should be the ISO 639-1 code. 'confidence' is your certainty (0.0-1.0) for the primary_lang of the *latest message*. 'other_detected_langs' is an optional list of other significant languages found in the entire provided text. Your entire output must be a single JSON object.",
      "user_template_full_context": "Please determine the language of the 'Latest User Message' considering the 'Recent Chat History'.\n\nLatest User Message:\n```text\n{latest_message}\n```\n\nRecent Chat History (older messages first):\n```text\n{history_context}\n```",
      "user_template_latest_only": "Please determine the language of the following 'Latest User Message':\n\nLatest User Message:\n```text\n{latest_message}\n```"
    },
    "weather_forecast_compiler": {
      "system": "You are a helpful and friendly weather reporter. Based on the provided JSON data for a multi-day weather forecast for {location}, create a concise, easy-to-read, and engaging natural language summary for the user. Respond in {language_name}. Highlight any significant weather events, temperature trends, and overall conditions. For each day, you can mention the day name, expected condition, and temperature range (min/max or average). Aim for a conversational tone. Do not just list the data; interpret it into a nice forecast summary. The user has already been told you are generating the forecast.",
      "user_template": "Here is the weather data for {location}:\n```json\n{forecast_data_json}\n```\nPlease provide the forecast summary."
    },
    "news_compiler": {
      "system": "You are an expert news summarizer AI. You will be given a list of news articles as JSON data, potentially related to the topic: '{topic}'. Your task is to generate a concise, informative, and engaging news digest in {language_name}. Summarize the overall situation if a common theme emerges. Highlight 2-3 of the most important or interesting headlines, briefly stating their core point and source. Do not just list all articles. Provide a synthesized overview. If the topic is 'None' or general, summarize general top news. Ensure the output is well-formatted for a chat message.",
      "user_template": "Here are the latest news articles (topic: '{topic}'):\n```json\n{articles_json}\n```\nPlease provide a news digest."
    },
    "location_reply_extractor": {
      "system": "The user was previously asked 'For which city would you like the weather forecast?'. Their direct reply is provided below. Extract ONLY the city name from this reply and provide it in English, suitable for a weather API. If the reply is ambiguous, not a city, or unclear, return the single word: None.",
      "user_template": "User's reply: \"{text}\""
    },
    "news_topic_reply_extractor": {
      "system": "The user was previously asked 'What topic are you interested in for the news?'. Their direct reply is provided below. Your task is to identify and extract the primary noun phrase or keyword that represents the news topic from this reply. Return ONLY this extracted topic. If the reply is very short (e.g., one or two words), that reply itself is likely the topic. Do not translate the topic. If the reply is clearly not a topic (e.g., 'I don't know', 'nevermind') or is too ambiguous to determine a topic, return the single word: None.",
      "user_template": "User's reply: \"{text}\""
    }
  },
  "responses": {
    "news_ask_topic": "Sure, I can fetch the news for you! What topic are you interested in today?",
    "weather_api_data_error": "I received some weather data for {location}, but I'm having a bit of trouble interpreting it right now. You might want to check a standard weather app.",
    "news_api_data_error": "I found some news articles, but I'm having a little trouble summarizing them for you at the moment. You can try checking a news website directly.",
    "start": "Hello, {user_mention}! I am EnkiBot, created by Yael Demedetskaya. How can I help you?",
    "help": "I am EnkiBot, an AI assistant by Yael Demedetskaya.\nIn group chats, I respond when you mention me by name (@EnkiBot, Enki) or reply to my messages.\nYou can ask me 'tell me about [name/topic]' for me to search for information in the chat history.\nTo get the weather, ask 'what's the weather in [city]?'.\nTo get news, ask 'what's the news?' or 'news about [topic]?'.\n\n**Commands:**\n/start - Start interaction\n/help - This help message\n/news - Get the latest news",
    "weather_ask_city": "I can get the weather for you, but for which city?",
    "weather_ask_city_failed_extraction": "Sorry, I didn't quite catch the city name. Could you please tell me the city again?",
    "llm_error_fallback": "Sorry, I couldn't process that request right now. Please try again later.",
    "generic_error_message": "Oops! Something went wrong on my end. I've logged the issue and my developers will look into it.",
    "language_pack_creation_failed_fallback": "I'm having a little trouble understanding that language fully right now, but I'll try my best in English. How can I help?",
    "user_search_ambiguous_clarification": "I found multiple users matching that name: {user_options}. Who are you asking about? Please clarify (e.g., by @username).",
    "user_search_not_found_in_db": "I couldn't find any information about '{search_term}' in my records for this chat.",
    "api_lang_code_openweathermap": "en",
    "weather_api_key_missing": "Weather service: API key missing.",
    "weather_report_intro_current": "Current weather in {city}:",
    "weather_condition_label": "Condition",
    "weather_temp_label": "Temperature",
    "weather_feels_like_label": "feels like",
    "weather_wind_label": "Wind",
    "weather_city_not_found": "Sorry, I couldn't find the city '{location}'.",
    "weather_server_error": "Could not get weather data due to a server error.",
    "weather_unexpected_error": "An unexpected error occurred while fetching weather.",
    "weather_forecast_unavailable": "Forecast data is unavailable for '{location}'.",
    "weather_report_intro_forecast": "Weather forecast for {city}:",
    "weather_city_not_found_forecast": "Sorry, I couldn't find '{location}' for the forecast.",
    "weather_server_error_forecast": "Could not get forecast data due to a server error.",
    "weather_unexpected_error_forecast": "An unexpected error occurred while fetching the forecast.",
    "weather_unknown_type": "Unknown weather request type.",
    "news_api_key_missing": "News service: API key missing.",
    "news_api_error": "Could not fetch news at this time. The news service might be temporarily unavailable.",
    "news_api_no_articles": "I couldn't find any news articles for your query '{query}'.",
    "news_api_no_general_articles": "I couldn't find any general news articles right now.",
    "news_report_title_topic": "News on '{topic}':",
    "news_report_title_general": "Latest News:",
    "news_unexpected_error": "An unexpected error occurred while fetching news.",
    "news_read_more": "Read: {url}",
    "replied_message_default_question": "Analyze this text, identify the main idea, and share your opinion.",
    "llm_no_assistants": "Sorry, none of my AI assistants are available right now.",
    "analysis_error": "Sorry, an error occurred during the text analysis.",
    "analysis_client_not_configured": "The analysis function cannot be performed as the AI client is not configured.",
    "conversation_cancelled": "Okay, the current operation has been cancelled.",
    "image_generation_start": "On it! Imagining something for you...",
    "image_generation_error": "Sorry, I hit a snag and couldn't create your image. Please try again.",
    "image_generation_no_prompt": "I see you want an image, but I'm not sure what you want me to create. Could you please describe it?",
    "image_generation_success_single": "Here is the image you requested for: '{image_prompt}'",
    "image_generation_success_multiple": "Here are the images you requested for: '{image_prompt}'"
  },
  "weather_conditions_map": {
    "clear_sky": "Clear sky",
    "few_clouds": "Few clouds",
    "scattered_clouds": "Scattered clouds",
    "broken_clouds": "Broken clouds",
    "overcast_clouds": "Overcast clouds",
    "shower_rain": "Shower rain",
    "light_intensity_shower_rain": "Light intensity shower rain",
    "rain": "Rain",
    "light_rain": "Light rain",
    "moderate_rain": "Moderate rain",
    "heavy_intensity_rain": "Heavy intensity rain",
    "thunderstorm": "Thunderstorm",
    "snow": "Snow",
    "light_snow": "Light snow",
    "mist": "Mist",
    "fog": "Fog",
    "smoke": "Smoke",
    "haze": "Haze",
    "sand_dust_whirls": "Sand/Dust Whirls",
    "squalls": "Squalls",
    "tornado": "Tornado",
    "unknown_condition": "Condition unknown"
  },
  "days_of_week": {
    "Monday": "Monday",
    "Tuesday": "Tuesday",
    "Wednesday": "Wednesday",
    "Thursday": "Thursday",
    "Friday": "Friday",
    "Saturday": "Saturday",
    "Sunday": "Sunday"
  }
}



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/lang/ru.json ---
======================================================================

﻿{
  "prompts": {
    "master_intent_classifier": {
      "system": "Вы — AI-ассистент по маршрутизации запросов. Ваша задача — классифицировать намерение пользователя на основе его сообщения. Выберите ОДНУ из следующих предопределенных категорий, которая наилучшим образом описывает основную цель пользователя. Ответьте ТОЛЬКО валидным JSON-объектом, содержащим один ключ \"intent\" с названием категории в качестве значения (например, {\"intent\": \"WEATHER_QUERY\"}). Убедитесь, что весь ваш ответ — это единственная, валидная JSON-структура.\n\nДоступные категории:\n- WEATHER_QUERY: Пользователь спрашивает о погодных условиях, прогнозах, температуре и т.д.\n- NEWS_QUERY: Пользователь запрашивает новостные статьи, заголовки или обновления по текущим (срочным) событиям или конкретным новостным темам.\n- IMAGE_GENERATION_QUERY: Пользователь просит создать, нарисовать, сгенерировать картинку или изображение чего-либо.\n- USER_PROFILE_QUERY: Пользователь запрашивает информацию о конкретном человеке (например, 'кто такой [имя]?', 'расскажи о [имя]'), включая запросы на конкретные детали, факты, списки или анализ, связанные с этим человеком (например, 'несбывшиеся прогнозы Кийосаки', 'биография X', 'список достижений Y').\n- MESSAGE_ANALYSIS_QUERY: Пользователь отвечает на другое сообщение и просит вас (бота) проанализировать, резюмировать или прокомментировать это сообщение, на которое был дан ответ.\n- GENERAL_CHAT: Пользователь делает общее утверждение, задает общепознавательный вопрос, ищет информацию или анализ, не подходящие под другие специфические категории (например, 'объясни черные дыры', 'сравни X и Y'), или ведет непринужденную беседу.\n- UNKNOWN_INTENT: Если намерение очень неясно или не соответствует ни одной другой категории, несмотря на более широкие определения.",
      "user_template": "{text_to_classify}"
    },
    "language_detector_llm": {
      "system": "Ты — эксперт по определению языка. Проанализируй предоставленный текст, который включает 'Последнее сообщение пользователя' и, возможно, 'Недавнюю историю чата'. 'Последнее сообщение пользователя' наиболее важно для определения его основного языка. Ответь ТОЛЬКО валидным JSON-объектом следующего вида: {\"primary_lang\": \"ru\", \"confidence\": 0.95, \"other_detected_langs\": [\"en\", \"de\"]}. 'primary_lang' должен быть ISO 639-1 кодом языка последнего сообщения. 'confidence' — это твоя уверенность (от 0.0 до 1.0) в определении primary_lang *последнего сообщения*. 'other_detected_langs' — это необязательный список других ISO 639-1 кодов языков, значительно представленных во всем тексте, если таковые имеются. Весь твой вывод должен быть единым JSON-объектом.",
      "user_template_full_context": "Пожалуйста, определи язык 'Последнего сообщения пользователя', учитывая 'Недавнюю историю чата'.\n\nПоследнее сообщение пользователя:\n```text\n{latest_message}\n```\n\nНедавняя история чата (старые сообщения сначала):\n```text\n{history_context}\n```",
      "user_template_latest_only": "Пожалуйста, определи язык следующего 'Последнего сообщения пользователя':\n\nПоследнее сообщение пользователя:\n```text\n{latest_message}\n```"
    },
    "image_generation_prompt_extractor": {
      "system": "Ты — AI-ассистент, который извлекает основной объект из запроса пользователя для использования в качестве промпта для модели генерации изображений. Проанализируй текст пользователя и выведи только основную описательную часть запроса. Например, если пользователь говорит: 'Энки, можешь, пожалуйста, нарисовать величественного льва в саванне на закате?', ты должен вывести 'величественный лев в саванне на закате'. Если текст не содержит чёткого запроса на генерацию изображения, ответь одним словом 'None'.",
      "user_template": "Извлеки промпт для генерации изображения из этого текста:\n\n---\n{text}\n---"
    },
    "name_variation_generator": {
      "system": "Ты — эксперт по языкам, специализирующийся на русских и английских именах. Твоя задача — сгенерировать список лингвистических вариантов имени пользователя. Сосредоточься ТОЛЬКО на реалистичных, используемых людьми вариантах. НЕ генерируй технические имена пользователей с цифрами или суффиксами вроде '_dev'.\n\n**Цель:** Создать варианты для распознавания в тексте на естественном языке.\n\n**Категории для генерации:**\n1.  **Оригинальные формы:** Исходное имя, фамилия и их комбинации.\n2.  **Уменьшительно-ласкательные формы и прозвища:** Распространенные короткие и ласковые формы (например, 'Антонина' -> 'Тоня'; 'Роберт' -> 'Роб').\n3.  **Транслитерация (с вариантами):** Предоставь несколько распространенных латинских написаний для всех кириллических форм (оригинальных и уменьшительных). Пример для 'Тоня': 'tonya', 'tonia'.\n4.  **Обратная транслитерация:** Если исходное имя на латинице, предоставь правдоподобные кириллические версии. Пример для 'Yael': 'Яэль', 'Йаэль'.\n5.  **Русские склонения (падежи):** Для всех основных русских имен (полных и кратких форм) предоставь их формы в различных падежах (родительный, дательный, винительный, творительный, предложный). Пример для 'Саша': 'саши', 'саше', 'сашу', 'сашей', 'о саше'.\n\n**Формат вывода:** Верни единый JSON-объект: {\"variations\": [\"вариант1\", \"вариант2\", ...]}. Все варианты должны быть в нижнем регистре.",
      "user_template": "Сгенерируй лингвистические варианты для пользователя с информацией: {name_info}"
    },
    "replied_message_analyzer": {
      "system": "Ты — AI-аналитик. Твоя задача — проанализировать 'Исходный текст' и дать содержательный ответ на 'Вопрос пользователя' об этом тексте. Твой анализ должен быть объективным, кратким и по существу. Если вопрос общий (например, 'что думаешь?'), сделай краткое резюме, выделив ключевые тезисы или настроения в исходном тексте.",
      "user_template": "Исходный текст для анализа:\n---\n\"{original_text}\"\n---\n\nВопрос пользователя об этом тексте:\n---\n\"{user_question}\"\n---\n\nТвой анализ:"
    },
    "weather_intent_analyzer": {
      "system": "Вы эксперт по анализу запросов, связанных с погодой. Ваша задача — определить намерение пользователя. Хочет ли пользователь узнать 'текущую' погоду или 'прогноз' на несколько дней? Если это прогноз, также определите, на сколько дней. Ваш ответ ДОЛЖЕН быть действительным объектом JSON и ничем другим. Убедитесь, что вывод отформатирован как JSON.\nПримеры:\n- Текст пользователя: 'погода в Лондоне' -> Ваш ответ: {\"type\": \"current\"}\n- Текст пользователя: 'какая сейчас погода?' -> Ваш ответ: {\"type\": \"current\"}\n- Текст пользователя: 'погода в Тампе на неделю' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 7}\n- Текст пользователя: 'прогноз на 5 дней в Берлине' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 5}\n- Текст пользователя: 'какая погода будет завтра?' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 2}\n- Текст пользователя: 'дай прогноз на субботу' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 7}\n- Текст пользователя: 'просто дай прогноз погоды' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 5}\n- Текст пользователя: 'прогноз на выходные' -> Ваш ответ: {\"type\": \"forecast\", \"days\": 3}\nЕсли вы не уверены, всегда выбирайте 'current'.",
      "user_template": "{text}"
    },
    "location_extractor": {
      "system": "Вы — экспертный инструмент анализа текста. Ваша задача — извлечь название города или местоположения из текста пользователя. Проанализируйте следующий текст и определите упомянутое географическое местоположение (город, регион, страну). Верните ТОЛЬКО название местоположения на английском языке, подходящее для запроса к API погоды. Например, если текст 'какая погода в Санкт-Петербурге', вы должны вернуть 'Saint Petersburg'. Если текст 'покажи погоду в Астане', вы должны вернуть 'Astana'. Если конкретное местоположение не найдено, вы ДОЛЖНЫ вернуть одно слово: None",
      "user_template": "{text}"
    },
    "news_topic_extractor": {
      "system": "Вы — экспертный инструмент анализа текста. Ваша задача — извлечь основную тему, ключевое слово или местоположение из запроса пользователя на получение новостей. Проанализируйте текст. Если он содержит конкретную тему, вы ДОЛЖНЫ вернуть эту тему в ее основной (именительном) падеже и на языке оригинала запроса. Например, для запроса 'новости в москве', вы должны вернуть 'Москва'. Для 'news about cars', верните 'cars'. Если запрос общий (например, 'какие новости?', 'последние заголовки'), вы ДОЛЖНЫ вернуть одно слово: None",
      "user_template": "{text}"
    },
    "profile_creator": {
      "system": "Ты — AI-аналитик поведения по тексту. Твоя задача — создать первоначальное, предельно сжатое резюме наблюдаемых черт и паттернов, основываясь *исключительно* на предоставленном сообщении пользователя. Сконцентрируйся на:\n1.  **Стиль коммуникации:** (например, формальный/неформальный, ассертивный/пассивный, вопросительный, декларативный, использование сленга, многословность, лексическая сложность, наблюдаемая тональность высказывания).\n2.  **Когнитивные индикаторы:** (например, структурированность мышления, ассоциативное мышление, указанный подход к решению проблем, используемый уровень абстракции).\n3.  **Выраженные интересы/темы:** (Ключевые топики или предметы, явно упомянутые или сильно подразумеваемые).\n\n**Формат вывода:** Краткий маркированный список, использующий точную, объективную поведенческую или лингвистическую терминологию. Избегай субъективных интерпретаций, эмоциональной окраски, психологической демагогии или предсказания будущего поведения/предрасположенностей. Каждый пункт должен быть фактическим выводом из текста. Не добавляй вступительных или заключительных ремарок. Сосредоточься на плотности и ясности изложения.",
      "user_template": "Проанализируй следующее сообщение от нового пользователя и создай сжатое резюме поведенческих черт:\n\nСообщение пользователя:\n---\n\"{message_text}\"\n---\n\nТвоё резюме (маркированный список с использованием точной терминологии):"
    },
    "profile_updater": {
      "system": "Ты — AI-аналитик поведения по тексту, обновляющий существующее резюме черт. Проанализируй 'Существующее резюме' и 'Новое сообщение пользователя'. Синтезируй новое, предельно сжатое резюме, которое включает новые наблюдения и уточняет предыдущие, отдавая приоритет свежей информации в случае противоречий. Сохраняй объективность и используй точную, научную поведенческую или лингвистическую терминологию. Сосредоточься на последовательности, краткости и фактических выводах из текстов. Избегай субъективных интерпретаций, эмоциональной окраски или предсказания будущего поведения/предрасположенностей. Не добавляй вступительных или заключительных ремарок.\n\nФормат вывода: Краткий маркированный список.",
      "user_template": "Существующее резюме черт:\n---\n{current_profile_notes}\n---\n\nНовое сообщение пользователя для анализа:\n---\n\"{message_text}\"\n---\n\nТвоё обновленное и синтезированное сжатое резюме черт (маркированный список с использованием точной терминологии):"
    },
    "main_orchestrator": {
      "system": "Ты EnkiBot, умный и дружелюбный AI-ассистент в Telegram-чате, созданный Yael Demedetskaya. Твоя задача — помогать пользователям, отвечая на их вопросы. Ты обладаешь долгосрочной памятью о разговорах и профилях участников. Когда тебя просят рассказать о ком-то, твоя задача — СИНТЕЗИРОВАТЬ ИНФОРМАЦИЮ. Тебе будут предоставлены данные из профиля (досье) и набор последних 'сырых' сообщений от этого человека. Проанализируй ОБА источника и составь на их основе новый, краткий, но содержательный и актуальный ответ. **КРИТИЧЕСКИ ВАЖНО: весь твой ответ должен быть НА ЯЗЫКЕ {language_name} (код языка: {lang_code}). НЕ ИСПОЛЬЗУЙ английский или любые другие языки в своем ответе.** Отвечай кратко, естественно. Будь вежлив, но не слишком формален."
    },
    "weather_forecast_compiler": {
      "system": "Ты — отзывчивый и дружелюбный ведущий прогноза погоды. На основе предоставленных JSON-данных для многодневного прогноза погоды для {location}, создай краткое, легко читаемое и увлекательное описание на естественном языке для пользователя. Отвечай на {language_name}. Выдели любые значительные погодные явления, температурные тренды и общие условия. Для каждого дня можешь упомянуть название дня, ожидаемое состояние погоды и диапазон температур (мин/макс или среднюю). Старайся поддерживать разговорный тон. Не просто перечисляй данные, а интерпретируй их в приятный итоговый прогноз. Пользователю уже сообщили, что ты генерируешь прогноз.",
      "user_template": "Вот данные о погоде для {location}:\n```json\n{forecast_data_json}\n```\nПожалуйста, предоставь итоговый прогноз."
    },
    "news_compiler": {
      "system": "Ты — экспертный AI для составления сводок новостей. Тебе будет предоставлен список новостных статей в формате JSON, возможно, связанных с темой: '{topic}'. Твоя задача — сгенерировать краткую, информативную и увлекательную новостную сводку на {language_name}. Если прослеживается общая тема, обобщи ситуацию. Выдели 2-3 наиболее важные или интересные новости, кратко изложив их суть и источник. Не просто перечисляй все статьи, а предоставь синтезированный обзор. Если тема 'None' или общая, составь сводку по главным новостям в целом. Убедись, что вывод хорошо отформатирован для сообщения в чате.",
      "user_template": "Вот последние новостные статьи (тема: '{topic}'):\n```json\n{articles_json}\n```\nПожалуйста, предоставь новостную сводку."
    },
    "location_reply_extractor": {
      "system": "Пользователя ранее спросили 'Для какого города вы хотите узнать прогноз погоды?'. Ниже приведен его прямой ответ. Извлеки ТОЛЬКО название города из этого ответа и предоставь его на английском языке, подходящем для API погоды. Если ответ неоднозначен, не является городом или неясен, верни одно слово: None.",
      "user_template": "Ответ пользователя: \"{text}\""
    },
    "news_topic_reply_extractor": {
      "system": "Пользователя ранее спросили 'Какая тема новостей вас интересует?'. Ниже приведен его прямой ответ. Твоя задача — идентифицировать и извлечь основную именную группу или ключевое слово, представляющее тему новостей из этого ответа. Верни ТОЛЬКО эту извлеченную тему. Если ответ очень короткий (например, одно-два слова), этот ответ, скорее всего, и является темой. Не переводи тему. Если ответ явно не является темой (например, 'я не знаю', 'неважно') или слишком неоднозначен для определения темы, верни одно слово: None.",
      "user_template": "Ответ пользователя: \"{text}\""
    }
  },
  "responses": {
    "start": "Привет, {user_mention}! Я EnkiBot, создан Yael Demedetskaya. Чем могу помочь?",
    "help": "Я EnkiBot, AI-ассистент от Yael Demedetskaya.\nВ группах я отвечаю, когда вы упоминаете меня по имени (@EnkiBot, Энки) или отвечаете на мои сообщения.\nВы можете спросить меня 'расскажи о [имя/тема]', чтобы я поискал информацию в истории чата.\nЧтобы узнать погоду, спросите 'какая погода в [город]?'.\nЧтобы узнать новости, спросите 'какие новости?' или 'новости о [тема]?'.\nДля создания изображений, попросите 'нарисуй [описание]'.\n\n**Команды:**\n/start - Начало работы\n/help - Эта справка\n/news - Последние новости",
    "weather_ask_city": "Я могу узнать погоду, но для какого города?",
    "weather_ask_city_failed_extraction": "Извините, я не совсем понял название города. Не могли бы вы назвать город еще раз?",
    "llm_error_fallback": "Извините, не могу обработать ваш запрос прямо сейчас. Пожалуйста, попробуйте позже.",
    "generic_error_message": "Ой! Что-то пошло не так на моей стороне. Я уже записал ошибку, и мои разработчики её изучат.",
    "language_pack_creation_failed_fallback": "У меня небольшие трудности с полным пониманием этого языка прямо сейчас, но я постараюсь помочь на русском. Чем могу быть полезен?",
    "user_search_ambiguous_clarification": "Я нашел нескольких пользователей с таким именем: {user_options}. О ком именно вы спрашиваете? Пожалуйста, уточните (например, через @username).",
    "user_search_not_found_in_db": "Я не смог найти информацию о '{search_term}' в своих записях для этого чата.",
    "api_lang_code_openweathermap": "ru",
    "weather_api_key_missing": "Сервис погоды: отсутствует ключ API.",
    "weather_report_intro_current": "Текущая погода в г. {city}:",
    "weather_condition_label": "Состояние",
    "weather_temp_label": "Температура",
    "weather_feels_like_label": "ощущается как",
    "weather_wind_label": "Ветер",
    "weather_city_not_found": "Извините, я не смог найти город '{location}'.",
    "weather_server_error": "Не удалось получить данные о погоде из-за ошибки сервера.",
    "weather_unexpected_error": "Произошла непредвиденная ошибка при запросе погоды.",
    "weather_forecast_unavailable": "Данные прогноза для '{location}' недоступны.",
    "weather_report_intro_forecast": "Прогноз погоды для г. {city}:",
    "weather_city_not_found_forecast": "Извините, я не смог найти '{location}' для прогноза.",
    "weather_server_error_forecast": "Не удалось получить данные прогноза из-за ошибки сервера.",
    "weather_unexpected_error_forecast": "Произошла непредвиденная ошибка при запросе прогноза.",
    "weather_unknown_type": "Неизвестный тип запроса погоды.",
    "news_api_key_missing": "Новостной сервис: отсутствует ключ API.",
    "news_api_error": "Не удалось получить новости в данный момент. Новостной сервис может быть временно недоступен (код ошибки: {status_code}).",
    "news_api_no_articles": "Я не смог найти новости по вашему запросу '{query}'.",
    "news_api_no_general_articles": "Я не смог найти общие новости прямо сейчас.",
    "news_report_title_topic": "Новости по теме '{topic}':",
    "news_report_title_general": "Последние новости:",
    "news_unexpected_error": "Произошла непредвиденная ошибка при получении новостей.",
    "news_read_more": "Читать: {url}",
    "replied_message_default_question": "Проанализируй этот текст, выдели главную мысль и выскажи свое мнение.",
    "llm_no_assistants": "Извините, ни один из моих AI-помощников сейчас не доступен.",
    "analysis_error": "К сожалению, произошла ошибка во время анализа текста.",
    "analysis_client_not_configured": "Функция анализа не может быть выполнена, так как AI-клиент не настроен.",
    "news_ask_topic": "Конечно, могу подобрать для вас новости! Какая тема вас сегодня интересует?",
    "weather_api_data_error": "Я получил данные о погоде для {location}, но сейчас мне немного сложно их интерпретировать. Возможно, вам стоит проверить стандартное погодное приложение.",
    "news_api_data_error": "Я нашел несколько новостных статей, но сейчас мне немного сложно составить из них сводку. Вы можете попробовать посмотреть новости напрямую на новостном сайте.",
    "conversation_cancelled": "Хорошо, текущая операция отменена.",
    "image_generation_start": "Принял! Уже представляю кое-что для вас...",
    "image_generation_error": "Прошу прощения, что-то пошло не так, и я не смог создать изображение. Пожалуйста, попробуйте еще раз.",
    "image_generation_no_prompt": "Я понял, что вы хотите изображение, но не уверен, что именно мне создать. Не могли бы вы описать это?",
    "image_generation_success_single": "Вот изображение по вашему запросу: '{image_prompt}'",
    "image_generation_success_multiple": "Вот изображения по вашему запросу: '{image_prompt}'"
  },
  "weather_conditions_map": {
    "clear_sky": "Ясно",
    "few_clouds": "Малооблачно",
    "scattered_clouds": "Рассеянная облачность",
    "broken_clouds": "Переменная облачность",
    "overcast_clouds": "Пасмурно",
    "shower_rain": "Ливень",
    "light_intensity_shower_rain": "Небольшой ливень",
    "rain": "Дождь",
    "light_rain": "Небольшой дождь",
    "moderate_rain": "Умеренный дождь",
    "heavy_intensity_rain": "Сильный дождь",
    "thunderstorm": "Гроза",
    "snow": "Снег",
    "light_snow": "Небольшой снег",
    "mist": "Дымка",
    "fog": "Туман",
    "smoke": "Смог",
    "haze": "Мгла",
    "sand_dust_whirls": "Песчаные/пыльные вихри",
    "squalls": "Шквалы",
    "tornado": "Торнадо",
    "unknown_condition": "Состояние неизвестно"
  },
  "days_of_week": {
    "Monday": "Понедельник",
    "Tuesday": "Вторник",
    "Wednesday": "Среда",
    "Thursday": "Четверг",
    "Friday": "Пятница",
    "Saturday": "Суббота",
    "Sunday": "Воскресенье"
  }
}



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/main.py ---
======================================================================

# enkibot/main.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import os
import logging
import asyncio
from typing import Optional 
from telegram import Update 
from telegram.ext import Application 

from enkibot import config
from enkibot.utils.logging_config import setup_logging
from enkibot.utils.database import initialize_database
# --- MODIFIED IMPORT ---
from enkibot.app import EnkiBotApplication 
# --- END MODIFICATION ---

logger: Optional[logging.Logger] = None 

def clear_terminal():
    os.system('cls' if os.name == 'nt' else 'clear')

# Commented out backfill for now, as it needs careful setup if run from here
# async def run_backfill_async(): ...

def main() -> None:
    global logger 
    clear_terminal()
    setup_logging() 
    logger = logging.getLogger(__name__) 

    logger.info("Initializing database schema...")
    initialize_database()

    if not config.TELEGRAM_BOT_TOKEN:
        logger.critical("FATAL: TELEGRAM_BOT_TOKEN missing. Bot cannot start.")
        return

    try:
        logger.info("Initializing Telegram PTB Application...")
        ptb_app = Application.builder().token(config.TELEGRAM_BOT_TOKEN).build()
        
        logger.info("Initializing EnkiBotApplication...")
        # --- MODIFIED BOT INSTANTIATION ---
        enkibot_app_instance = EnkiBotApplication(ptb_app) 
        enkibot_app_instance.register_handlers() # Call the method to register handlers
        # --- END MODIFICATION ---

        logger.info("Starting EnkiBot polling...")
        # The run method is now part of EnkiBotApplication, or keep polling here
        # For simplicity, keeping polling here:
        ptb_app.run_polling(allowed_updates=Update.ALL_TYPES) 
        # Alternatively, if you add a run() method to EnkiBotApplication:
        # enkibot_app_instance.run() 
        
        logger.info("EnkiBot has stopped.")
    except Exception as e:
        logger.critical(f"Unrecoverable error during bot setup or run: {e}", exc_info=True)

if __name__ == '__main__':
    main()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/__init__.py ---
======================================================================

# enkibot/modules/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file intentionally left blank.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/api_router.py ---
======================================================================

﻿# enkibot/modules/api_router.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot API Router ===
# ==================================================================================================

# (GPLv3 Header as in your files)
import logging
import httpx 
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List

from enkibot import config # For API Keys
# Removed LLMServices import as it's not directly used here anymore for extraction

logger = logging.getLogger(__name__)

class ApiRouter:
    def __init__(self, weather_api_key: str | None, news_api_key: str | None, llm_services: Any = None): # llm_services no longer strictly needed here
        self.weather_api_key = weather_api_key
        self.news_api_key = news_api_key
        # self.llm_services = llm_services # Not used directly in this version of ApiRouter

        self.lang_to_country_map = {
            "en": "us", "ru": "ru", "de": "de", "fr": "fr", "es": "es", 
            "it": "it", "ja": "jp", "ko": "kr", "zh": "cn", "bg": "bg",
            "ua": "ua", "pl": "pl", "tr": "tr", "pt": "pt", 
        }
        self.default_news_country = "us"

    def _get_localized_response_string_from_pack(self, lang_pack_full: Optional[Dict[str, Any]], key: str, default_value: str, **kwargs) -> str:
        """ Helper to get response strings directly from a full language pack. """
        if lang_pack_full and "responses" in lang_pack_full:
            raw_string = lang_pack_full["responses"].get(key, default_value)
        else: # Fallback if pack or responses section is missing
            raw_string = default_value
        try:
            return raw_string.format(**kwargs) if kwargs else raw_string
        except KeyError: return default_value


    async def get_weather_data_structured(self, location: str, lang_pack_full: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """
        Fetches 5-day weather forecast data and returns it in a structured format
        suitable for LLM processing. Includes city name and a list of daily forecasts.
        """
        if not self.weather_api_key:
            logger.warning("Weather API key is not configured.")
            return None

        api_lang = "en" # Default to English for weather data city name, LLM can localize description
        if lang_pack_full and "responses" in lang_pack_full:
            api_lang = lang_pack_full["responses"].get("api_lang_code_openweathermap", "en")

        # OpenWeatherMap 5 day / 3 hour forecast endpoint
        url = "https://api.openweathermap.org/data/2.5/forecast"
        # Requesting enough data points for 5 days (8 records per day * 5 days = 40)
        params = {"q": location, "appid": self.weather_api_key, "units": "metric", "lang": api_lang, "cnt": 40}
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
            data = response.json()

            city_name = data.get("city", {}).get("name", location)
            forecast_items = data.get("list", [])

            if not forecast_items:
                logger.warning(f"No forecast items received for {location}")
                return None

            daily_forecasts_processed = []
            temp_daily_data = {} # date_str -> {'temps': [], 'conditions': set(), 'icons': set()}

            for item in forecast_items:
                dt_object = datetime.fromtimestamp(item["dt"])
                date_str = dt_object.strftime("%Y-%m-%d")
                
                if date_str not in temp_daily_data:
                    temp_daily_data[date_str] = {'temps': [], 'conditions': set(), 'icons': set()}
                
                temp_daily_data[date_str]['temps'].append(item["main"]["temp"])
                if item.get("weather") and item["weather"][0]:
                    temp_daily_data[date_str]['conditions'].add(item["weather"][0].get("description", "N/A"))
                    temp_daily_data[date_str]['icons'].add(item["weather"][0].get("icon", "N/A"))

            for date_str, daily_data in sorted(temp_daily_data.items()):
                if not daily_data['temps']: continue
                
                # For simplicity, take the condition that appears most or first unique for the day.
                # LLM can make sense of multiple conditions if provided as a list.
                day_condition = list(daily_data['conditions'])[0] if daily_data['conditions'] else "N/A"
                
                daily_forecasts_processed.append({
                    "date": date_str,
                    "day_name": datetime.strptime(date_str, "%Y-%m-%d").strftime('%A'), # English day name
                    "temp_min": min(daily_data['temps']),
                    "temp_max": max(daily_data['temps']),
                    "avg_temp": sum(daily_data['temps']) / len(daily_data['temps']),
                    "condition_descriptions": list(daily_data['conditions']), # Provide all conditions
                    "primary_condition": day_condition # LLM can choose or summarize
                })
            
            if not daily_forecasts_processed:
                 logger.warning(f"Could not process daily forecast data for {location}")
                 return None

            return {
                "location": city_name,
                "forecast_days": daily_forecasts_processed[:7] # Return up to 7 days of processed data
            }

        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error fetching weather forecast for {location}: {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e:
            logger.error(f"Unexpected error fetching structured weather data for {location}: {e}", exc_info=True)
        return None

    async def get_latest_news_structured(self, query: Optional[str] = None, lang_code: str = "en", num: int = 5) -> Optional[List[Dict[str, Any]]]:
        """
        Fetches latest news and returns a list of article dictionaries.
        """
        if not self.news_api_key:
            logger.warning("News API key is not configured.")
            return None

        params: Dict[str, Any] = {"apiKey": self.news_api_key, "pageSize": num}
        base_url = "https://newsapi.org/v2/"
        endpoint: str

        if query:
            logger.info(f"Fetching news for query: '{query}', language: {lang_code}")
            endpoint = "everything"
            params.update({"q": query, "language": lang_code, "sortBy": "relevancy"})
        else:
            country = self.lang_to_country_map.get(lang_code, self.default_news_country)
            logger.info(f"Fetching top headlines for country: '{country}' (derived from lang: {lang_code})")
            endpoint = "top-headlines"
            params.update({"country": country, "category": "general"})
        
        url = base_url + endpoint
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(url, params=params)
                logger.debug(f"NewsAPI request URL: {resp.url}")
                resp.raise_for_status()
            data = resp.json()
            articles_raw = data.get("articles", [])
            
            logger.info(f"NewsAPI returned {len(articles_raw)} articles (totalResults: {data.get('totalResults')}) for params: {params}")

            if not articles_raw:
                return [] # Return empty list if no articles

            # Process articles into a cleaner structure for the LLM
            processed_articles = []
            for article in articles_raw:
                processed_articles.append({
                    "title": article.get("title"),
                    "source": article.get("source", {}).get("name"),
                    "description": article.get("description"), # Short description or snippet
                    "url": article.get("url"),
                    "published_at": article.get("publishedAt")
                })
            return processed_articles
            
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error fetching news ({url}): {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e:
            logger.error(f"Unexpected error fetching structured news: {e}", exc_info=True)
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/base_module.py ---
======================================================================

# enkibot/modules/base_module.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Base Module ===
# ==================================================================================================
# This file is intended to hold an Abstract Base Class (ABC) for all functional modules.
# In a future evolution, all modules (e.g., IntentRecognizer, FactExtractor) would inherit
# from this class to ensure a consistent interface, for example, requiring an `execute` method.
# For now, it serves as a structural placeholder.
# ==================================================================================================

class BaseModule:
    """
    Abstract Base Class for all EnkiBot modules.
    """
    def __init__(self, name: str):
        self.name = name

    def execute(self, *args, **kwargs):
        """
        The main method to be implemented by all subclasses.
        """
        raise NotImplementedError("Each module must implement the 'execute' method.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/fact_extractor.py ---
======================================================================

﻿# enkibot/modules/fact_extractor.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Fact Extractor ===
# ==================================================================================================
# This module uses linguistic rules and morphological analysis to extract specific
# pieces of information from user text, such as a person's name mentioned in a query.
# This is faster and cheaper than using an LLM for simple, well-defined extraction tasks.
# ==================================================================================================

import logging
import re
import pymorphy3

logger = logging.getLogger(__name__)

# Initialize the morphological analyzer for Russian.
try:
    morph = pymorphy3.MorphAnalyzer()
except Exception as e:
    logger.error(f"Could not initialize pymorphy3 MorphAnalyzer: {e}. Fact extraction might fail.")
    morph = None

def find_user_search_query_in_text(text: str) -> str | None:
    """
    Analyzes text by lemmatizing words to their base form and looks for a combination
    of trigger words and prepositions to extract a user name.

    This is a classic Natural Language Processing (NLP) technique for entity extraction.

    Args:
        text: The user's message text.

    Returns:
        The extracted name as a string, or None if no name is found.
    """
    if not morph:
        logger.warning("Morphological analyzer not available. Skipping user search query extraction.")
        return None
        
    # Dictionaries of trigger word lemmas (base forms).
    # This makes the system robust to different word forms (e.g., 'tell', 'tells', 'told').
    TELL_LEMMAS = {'рассказать', 'поведать', 'сообщить', 'описать'}
    INFO_LEMMAS = {'информация', 'инфо', 'справка', 'досье', 'данные'}
    WHO_LEMMAS = {'кто', 'что'}
    EXPLAIN_LEMMAS = {'пояснить', 'объяснить'}
    REMEMBER_LEMMAS = {'помнить', 'напомнить'}

    # Prepositions that typically follow trigger words before a name.
    PREPOSITIONS = {'о', 'про', 'за', 'на', 'по'}

    # Split the text into words
    words = re.findall(r"[\w'-]+", text.lower())
    
    for i, word in enumerate(words):
        try:
            # Get the lemma (normal form) of the word
            lemma = morph.parse(word)[0].normal_form
            
            # Check if the lemma is one of our triggers
            is_trigger = (lemma in TELL_LEMMAS or
                          lemma in INFO_LEMMAS or
                          lemma in WHO_LEMMAS or
                          lemma in EXPLAIN_LEMMAS or
                          lemma in REMEMBER_LEMMAS)

            if is_trigger:
                # We found a trigger word. The name should follow it.
                start_index = i + 1
                
                # If the next word is a preposition, skip it.
                if start_index < len(words) and words[start_index] in PREPOSITIONS:
                    start_index += 1
                
                # Everything that follows (up to 3 words) is considered the name.
                if start_index < len(words):
                    # Capture 1 to 3 words after the trigger/preposition.
                    name_parts = words[start_index : start_index + 3]
                    extracted_name = " ".join(name_parts)
                    logger.info(f"Extracted potential user search query: '{extracted_name}'")
                    return extracted_name

        except Exception as e:
            logger.error(f"Error during lemmatization of word '{word}': {e}")
            continue
            
    return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/image_generation_service.py ---
======================================================================

# enkibot/modules/image_generation_service.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.


import logging
from typing import Optional, TYPE_CHECKING

from telegram import Update
from telegram.ext import ContextTypes
from telegram.constants import ChatAction

# This import makes the image generation tool available to be called.
from TBD_tool_name import image_generation

if TYPE_CHECKING:
    from enkibot.core.language_service import LanguageService
    from enkibot.modules.intent_recognizer import IntentRecognizer

logger = logging.getLogger(__name__)

class ImageGenerationIntentHandler:
    def __init__(self, 
                 language_service: 'LanguageService', 
                 intent_recognizer: 'IntentRecognizer'):
        logger.info("ImageGenerationIntentHandler initialized.")
        self.language_service = language_service
        self.intent_recognizer = intent_recognizer

    async def handle_intent(self, update: Update, context: ContextTypes.DEFAULT_TYPE, user_msg_txt: str) -> None:
        """
        Handles the full workflow for an image generation request, from prompt
        extraction to sending the final image or an error message.
        """
        if not update.message or not update.effective_chat:
            return

        # 1. Acknowledge the request and send a "typing" or "uploading" action.
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.UPLOAD_PHOTO)
        # Send a preliminary "working on it" message that we can edit or delete later.
        preliminary_reply = await update.message.reply_text(self.language_service.get_response_string("image_generation_start"))

        # 2. Extract a clean, descriptive prompt from the user's full message.
        extractor_prompts = self.language_service.get_llm_prompt_set("image_generation_prompt_extractor")
        clean_prompt = None
        if extractor_prompts and "system" in extractor_prompts:
            clean_prompt = await self.intent_recognizer.extract_image_prompt_with_llm(
                text=user_msg_txt,
                lang_code=self.language_service.current_lang,
                system_prompt=extractor_prompts["system"],
                user_prompt_template=extractor_prompts.get("user", "{text}")
            )
        else:
            logger.error("Image generation prompt extractor prompts are missing!")

        # 3. Handle cases where a clear prompt could not be extracted.
        if not clean_prompt:
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=preliminary_reply.message_id,
                text=self.language_service.get_response_string("image_generation_no_prompt")
            )
            return

        # 4. Call the image generation tool and handle the result.
        try:
            # This is the direct call to the image generation tool.
            image_gen_result = image_generation.generate_images(
                prompts=[clean_prompt],
                image_generation_usecase=image_generation.ImageGenerationUsecase.ALTERNATIVES
            )
            
            content_id = None
            if image_gen_result and image_gen_result.results:
                first_result = image_gen_result.results[0]
                if first_result and first_result.generated_images:
                    content_id = first_result.content_id

            # Delete the preliminary "On it! Imagining something..." message.
            await context.bot.delete_message(chat_id=update.effective_chat.id, message_id=preliminary_reply.message_id)

            if content_id:
                # If successful, send the image by replying with its content_id.
                # The Telegram client will render this as the image.
                logger.info(f"Successfully generated image for prompt '{clean_prompt}'. Replying with content_id.")
                await update.message.reply_text(content_id)
            else:
                # Handle the case where the tool ran but failed to produce an image.
                logger.error(f"Image generation tool ran but failed for prompt: {clean_prompt}")
                await update.message.reply_text(self.language_service.get_response_string("image_generation_error"))

        except Exception as e:
            logger.error(f"An exception occurred during the image generation tool call: {e}", exc_info=True)
            # Edit the preliminary message to show an error instead of leaving the user hanging.
            await context.bot.edit_message_text(
                chat_id=update.effective_chat.id,
                message_id=preliminary_reply.message_id,
                text=self.language_service.get_response_string("image_generation_error")
            )


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/intent_recognizer.py ---
======================================================================

# enkibot/modules/intent_recognizer.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

import logging
import json
import re 
from typing import Dict, Any, Optional, TYPE_CHECKING

from enkibot import config 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices 

logger = logging.getLogger(__name__)

class IntentRecognizer:
    def __init__(self, llm_services: 'LLMServices'): 
        logger.info("IntentRecognizer __init__ STARTING")
        self.llm_services = llm_services
        logger.info("IntentRecognizer __init__ COMPLETED")

    async def classify_master_intent(self, text: str, lang_code: str, 
                                     system_prompt: str, user_prompt_template: str) -> str:
        logger.info(f"Classifying master intent (lang: {lang_code}): '{text[:100]}...'")
        user_prompt = user_prompt_template.format(text_to_classify=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        classification_model_id = config.OPENAI_CLASSIFICATION_MODEL_ID
        if classification_model_id and any(model_prefix in classification_model_id for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}

        classified_intent_value = "UNKNOWN_INTENT" 
        completion_str_for_log = "N/A"

        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=classification_model_id, 
                temperature=0.0, 
                max_tokens=100, 
                **response_format_arg 
            )
            completion_str_for_log = completion_str if completion_str is not None else "None"

            if completion_str:
                try:
                    clean_comp_str = completion_str.strip()
                    match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                    if match:
                        clean_comp_str = match.group(1).strip()
                    elif clean_comp_str.startswith("```"): 
                        clean_comp_str = clean_comp_str.strip("` \t\n\r")
                        if clean_comp_str.lower().startswith("json"): 
                            clean_comp_str = clean_comp_str[4:].strip() 
                    
                    data = json.loads(clean_comp_str)
                    intent_from_json = data.get("intent", data.get("INTENT")) 

                    if intent_from_json and isinstance(intent_from_json, str):
                        processed_intent = intent_from_json.strip().strip('_').upper().replace(" ", "_")
                        known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "IMAGE_GENERATION_QUERY", 
                                            "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", 
                                            "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                        if processed_intent in known_categories:
                            classified_intent_value = processed_intent
                            logger.info(f"Master intent classified as: {classified_intent_value} via JSON.")
                        else:
                            logger.warning(f"LLM JSON with unknown category '{processed_intent}'. Raw value: '{intent_from_json}'. Full data: {data}. Defaulting UNKNOWN.")
                    else:
                        logger.warning(f"LLM JSON for master intent missing 'intent' key or not string. Data: {data}. Defaulting UNKNOWN.")
                except json.JSONDecodeError:
                    logger.warning(f"Failed to decode JSON from master_intent_classifier. LLM raw: '{completion_str_for_log}'. Attempting direct parse.")
                    raw_intent = completion_str_for_log.strip().strip('_').upper().replace(" ", "_")
                    if raw_intent.startswith('{') and raw_intent.endswith('}'):
                        try:
                            temp_data = json.loads(raw_intent)
                            extracted_val = temp_data.get("intent", temp_data.get("INTENT", raw_intent))
                            raw_intent = str(extracted_val).strip().strip('_').upper().replace(" ", "_")
                        except json.JSONDecodeError: 
                            pass 

                    known_categories = ["WEATHER_QUERY", "NEWS_QUERY", "IMAGE_GENERATION_QUERY", 
                                        "USER_PROFILE_QUERY", "MESSAGE_ANALYSIS_QUERY", 
                                        "GENERAL_CHAT", "UNKNOWN_INTENT"] 
                    if raw_intent in known_categories:
                        classified_intent_value = raw_intent
                        logger.info(f"Master intent classified as: {classified_intent_value} via direct string parse fallback.")
                    else:
                         logger.warning(f"Direct string parse fallback failed for intent: '{raw_intent}'. Defaulting UNKNOWN.")
            else:
                logger.warning("Master intent classification LLM call returned no content.")
        except Exception as e: 
            logger.error(f"Error during master intent classification LLM call: {e}", exc_info=True)
        
        return classified_intent_value

    async def analyze_weather_request_with_llm(self, text: str, lang_code: str, 
                                               system_prompt: str, user_prompt_template: Optional[str]) -> Dict[str, Any]:
        logger.info(f"Analyzing weather request type (lang: {lang_code}): '{text}' with LLM.")
        user_prompt = (user_prompt_template or "{text}").format(text=text) 
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        response_format_arg = {}
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        if model_to_use and any(model_prefix in model_to_use for model_prefix in ["gpt-4", "gpt-3.5-turbo", "gpt-4o"]):
             response_format_arg = {"response_format": {"type": "json_object"}}
        
        default_response = {"type": "current"} 
        completion_str_for_error_log = "N/A"
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=model_to_use,
                temperature=0, 
                **response_format_arg
            )
            completion_str_for_error_log = completion_str if completion_str is not None else "None"
            if completion_str:
                logger.info(f"LLM response for weather analysis: {completion_str}")
                clean_comp_str = completion_str.strip()
                match = re.search(r"```json\s*(.*?)\s*```", clean_comp_str, re.DOTALL | re.IGNORECASE)
                if match: clean_comp_str = match.group(1).strip()
                elif clean_comp_str.startswith("```"): 
                    clean_comp_str = clean_comp_str.strip("` \t\n\r") 
                    if clean_comp_str.lower().startswith("json"): 
                        clean_comp_str = clean_comp_str[4:].strip()              
                return json.loads(clean_comp_str.strip())
            else:
                logger.warning("LLM returned no content for weather analysis.")
        except json.JSONDecodeError as e:
            logger.error(f"Error decoding JSON from LLM for weather analysis: '{completion_str_for_error_log}'. Error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error in LLM call during weather request analysis: {e}", exc_info=True)
        
        return default_response

    async def extract_location_with_llm(self, text: str, lang_code: str, 
                                        system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM location extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"):
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
            
            if not location and self.llm_services.is_provider_configured("groq"): 
                logger.info("OpenAI location extraction failed or not configured, trying Groq.")
                completion = await self.llm_services.call_llm_api(
                    "Groq", self.llm_services.groq_api_key, self.llm_services.groq_endpoint_url, 
                    self.llm_services.groq_model_id, messages_for_api,
                    temperature=0, max_tokens=50 )
                if completion: location = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM location extraction: {e}", exc_info=True)

        if location and location.lower().strip() not in ["none", "null", "n/a", ""]:
            logger.info(f"LLM successfully extracted location: '{location}'")
            return location
        logger.warning(f"LLM couldn't extract location from: '{text}'.")
        return None

    async def extract_location_from_reply(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: str) -> Optional[str]:
        logger.info(f"Extracting location from user's reply (lang: {lang_code}): '{text}'")
        user_prompt = user_prompt_template.format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        location = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID 
        try:
            completion = await self.llm_services.call_openai_llm(
                messages_for_api,
                model_id=model_to_use,
                temperature=0.0,
                max_tokens=30 
            )
            if completion and completion.lower().strip() not in ["none", "null", "n/a", ""]:
                location = completion.strip().strip('"')
                logger.info(f"LLM extracted location from reply: '{location}'")
                return location
            else:
                logger.warning(f"LLM indicated no location in reply or returned 'None' for: '{text}'")
        except Exception as e:
            logger.error(f"Error during LLM location extraction from reply: {e}", exc_info=True)
        
        logger.warning(f"Could not extract a clear location from reply: '{text}'")
        return None

    # In enkibot/modules/intent_recognizer.py
    async def extract_topic_from_reply(self, text: str, lang_code: str, 
                                       system_prompt: str, user_prompt_template: str) -> Optional[str]:
        logger.info(f"Extracting news topic from user's reply (lang: {lang_code}): '{text}'")
        user_prompt = user_prompt_template.format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        topic = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            completion = await self.llm_services.call_openai_llm(
                messages_for_api,
                model_id=model_to_use,
                temperature=0.0, # Be very deterministic
                max_tokens=50 
            )
            if completion:
                cleaned_completion = completion.strip().strip('"') 
                # Check if LLM explicitly returns "None" or if it's very short after cleaning
                if cleaned_completion.lower() not in ["none", "null", "n/a", ""] and len(cleaned_completion) > 1: # Min length for a topic
                    topic = cleaned_completion
                    logger.info(f"LLM extracted topic from reply: '{topic}'")
                    return topic # Return the extracted topic
                else:
                    logger.warning(f"LLM indicated no topic in reply or returned 'None'/'empty' for: '{text}'")
            else:
                logger.warning(f"LLM call for topic extraction from reply returned no content for: '{text}'")
            
        except Exception as e:
            logger.error(f"Error during LLM topic extraction from reply: {e}", exc_info=True)
        
        logger.warning(f"Could not extract a clear topic from reply: '{text}'. Defaulting to None.")
        return None # Return None if no clear topic extracted or error
    
    async def extract_image_prompt_with_llm(self, text: str, lang_code: str, 
                                             system_prompt: str, user_prompt_template: str) -> Optional[str]:
        logger.info(f"Attempting to extract image generation prompt from text (lang: {lang_code}): '{text}'")
        user_prompt = user_prompt_template.format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID 
        
        try:
            completion = await self.llm_services.call_openai_llm(
                messages_for_api, 
                model_id=model_to_use, 
                temperature=0.2, 
                max_tokens=150 
            )
            
            if completion and completion.lower().strip() not in ["none", "null", "n/a", ""]:
                clean_prompt = completion.strip().strip('"')
                # Further cleaning if LLM adds conversational wrappers
                clean_prompt = re.sub(r"^(Okay, here's the prompt:|The image prompt is:|Sure, the prompt is:)\s*", "", clean_prompt, flags=re.IGNORECASE).strip()
                logger.info(f"Extracted image prompt: '{clean_prompt}'")
                return clean_prompt
            else:
                logger.info(f"LLM indicated no specific image prompt could be extracted from: '{text}'")
                return None
        except Exception as e:
            logger.error(f"Error in LLM call during image prompt extraction: {e}", exc_info=True)
            return None

    async def extract_news_topic_with_llm(self, text: str, lang_code: str, 
                                          system_prompt: str, user_prompt_template: Optional[str]) -> Optional[str]:
        logger.info(f"Requesting LLM news topic extraction from text (lang: {lang_code}): '{text}'")
        user_prompt = (user_prompt_template or "{text}").format(text=text)
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        topic = None
        model_to_use = config.OPENAI_CLASSIFICATION_MODEL_ID
        try:
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, model_id=model_to_use,
                    temperature=0, max_tokens=50 )
                if completion: topic = completion.strip()
        except Exception as e:
            logger.error(f"Error during LLM news topic extraction: {e}", exc_info=True)

        if topic and topic.lower().strip() not in ["none", "null", "n/a", ""]:
            logger.info(f"LLM successfully extracted news topic: '{topic}'")
            return topic
        logger.info(f"LLM found no specific news topic in '{text}'.")
        return None


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/profile_manager.py ---
======================================================================

# enkibot/modules/profile_manager.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Profile Manager ===
# ==================================================================================================
# Manages user psychological profiles and name variations using LLMs and database interaction.
# ==================================================================================================
import logging
import json
from typing import Optional, Dict, TYPE_CHECKING

from enkibot.utils.database import DatabaseManager 

if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices # For type hinting

logger = logging.getLogger(__name__)

class ProfileManager:
    def __init__(self, llm_services: 'LLMServices', db_manager: DatabaseManager):
        self.llm_services = llm_services # Type hint is string literal
        self.db_manager = db_manager
        self.MAX_PROFILE_SIZE = 4000

    async def populate_name_variations_with_llm(self, user_id: int, first_name: Optional[str], 
                                                last_name: Optional[str], username: Optional[str],
                                                system_prompt: str, user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"): # Check specific provider
            logger.warning(f"Name variation for user {user_id} skipped: OpenAI not configured in LLMServices.")
            return

        name_parts = [part for part in [first_name, last_name, username] if part and str(part).strip()]
        if not name_parts:
            logger.info(f"No valid name parts for user {user_id}, skipping name variation.")
            return
            
        name_info = ", ".join(name_parts)
        logger.info(f"Requesting name variations for user {user_id} ({name_info}).")
        user_prompt = user_prompt_template.format(name_info=name_info)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        name_variations = set([p.lower() for p in name_parts])
        
        try:
            completion_str = await self.llm_services.call_openai_llm(
                messages, temperature=0.3, response_format={"type": "json_object"}
            ) # Removed model_id, should be handled by call_openai_llm default
            if completion_str:
                data = json.loads(completion_str)
                variations_list = data.get('variations')
                if isinstance(variations_list, list):
                    name_variations.update([str(v).lower().strip() for v in variations_list if v and str(v).strip()])
                    logger.info(f"LLM got {len(variations_list)} raw variations. Total unique: {len(name_variations)} for user {user_id}.")
                else:
                    logger.warning(f"LLM name variations for {user_id} no 'variations' list. Resp: {completion_str[:200]}")
            else:
                logger.warning(f"LLM no content for name variations for user {user_id}.")
        except Exception as e:
            logger.error(f"LLM name variation error (user {user_id}): {e}", exc_info=True)

        if name_variations:
            await self.db_manager.save_user_name_variations(user_id, list(name_variations))
        else:
            logger.info(f"No name variations to save for user {user_id}.")

    async def analyze_and_update_user_profile(self, user_id: int, message_text: str,
                                              create_system_prompt: str, create_user_prompt_template: str,
                                              update_system_prompt: str, update_user_prompt_template: str):
        if not self.llm_services.is_provider_configured("openai"):
            logger.warning(f"Profiling for {user_id} skipped: OpenAI not configured.")
            return
        logger.info(f"Starting/Updating profile analysis for user {user_id}...")
        current_notes = await self.db_manager.get_user_profile_notes(user_id)
        sys_prompt, user_prompt = "", ""

        if not current_notes:
            logger.info(f"No profile for {user_id}. Creating new.")
            sys_prompt, user_prompt = create_system_prompt, create_user_prompt_template.format(message_text=message_text)
        else:
            logger.info(f"Existing profile for {user_id}. Updating.")
            sys_prompt, user_prompt = update_system_prompt, update_user_prompt_template.format(
                current_profile_notes=current_notes, message_text=message_text)
        
        messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}]
        updated_notes_str: Optional[str] = None
        try:
            updated_notes_str = await self.llm_services.call_openai_llm(messages, temperature=0.5, max_tokens=1000)
        except Exception as e:
            logger.error(f"LLM profile analysis error for {user_id}: {e}", exc_info=True)

        if updated_notes_str and updated_notes_str.strip():
            await self.db_manager.update_user_profile_notes(user_id, updated_notes_str.strip()[:self.MAX_PROFILE_SIZE])
            logger.info(f"Profile updated for {user_id}.")
        else:
            logger.warning(f"Profile analysis no content for {user_id}.")


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/modules/response_generator.py ---
======================================================================

# enkibot/modules/response_generator.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Response Generator ===
# ==================================================================================================
# enkibot/modules/intent_recognizer.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Your GPLv3 Header)

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
# print(f"%%%%% EXECUTING INTENT_RECOGNIZER.PY - VERSION FROM: {__file__} %%%%%") # You can uncomment this if you still face import issues

import logging
import json
from typing import List, Dict, Any, Optional, TYPE_CHECKING

from telegram.ext import ContextTypes 

# TYPE_CHECKING imports to avoid circular dependencies at runtime
if TYPE_CHECKING:
    from enkibot.core.llm_services import LLMServices
    from enkibot.utils.database import DatabaseManager
    from enkibot.modules.intent_recognizer import IntentRecognizer 
    # LanguageService is not directly used here, prompts are passed in

from enkibot.modules.fact_extractor import find_user_search_query_in_text 
from enkibot import config # For model IDs

logger = logging.getLogger(__name__)

class ResponseGenerator:
    def __init__(self, 
                 llm_services: 'LLMServices', 
                 db_manager: 'DatabaseManager', 
                 intent_recognizer: 'IntentRecognizer'): # No LanguageService needed directly
        logger.info("ResponseGenerator __init__ STARTING")
        self.llm_services = llm_services
        self.db_manager = db_manager
        self.intent_recognizer = intent_recognizer 
        logger.info("ResponseGenerator __init__ COMPLETED")

    async def analyze_replied_message(self, 
                                      original_text: str, 
                                      user_question: str,
                                      system_prompt: str, 
                                      user_prompt_template: Optional[str]) -> str:
        logger.info(f"ResponseGenerator: Analyzing replied message. Original length: {len(original_text)}, Question: '{user_question}'")
        
        user_prompt = (user_prompt_template or "Original Text:\n---\n{original_text}\n---\n\nUser's Question:\n---\n{user_question}\n---\n\nYour Analysis:").format(
            original_text=original_text, user_question=user_question
        )
        messages_for_api = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        try:
            # Use a general purpose model for this analysis task, or a specific one if configured
            analysis_model_id = config.OPENAI_MODEL_ID # Default general model
            
            if self.llm_services.is_provider_configured("openai"): 
                completion = await self.llm_services.call_openai_llm(
                    messages_for_api, 
                    model_id=analysis_model_id, 
                    temperature=0.5, 
                    max_tokens=1000
                )
                if completion: 
                    return completion.strip()
            
            # Fallback message if OpenAI is not configured or call fails
            logger.warning("OpenAI not configured or call failed for replied message analysis.")
            # This response should ideally come from language_service via the calling handler
            return "I'm having trouble analyzing that right now. Please try again later."
            
        except Exception as e:
            logger.error(f"Error in LLM call during replied message analysis: {e}", exc_info=True)
            # This response should ideally come from language_service via the calling handler
            return "Sorry, an error occurred during the text analysis."

    async def compile_weather_forecast_response(self, 
                                                forecast_data_structured: Dict[str, Any], 
                                                lang_code: str, # For logging and potentially Babel
                                                system_prompt: str, # Already localized and formatted by caller
                                                user_prompt_template: str) -> str:
        location = forecast_data_structured.get("location", "the requested location")
        forecast_data_json_str = json.dumps(forecast_data_structured.get("forecast_days", []), indent=2, ensure_ascii=False)

        user_prompt = user_prompt_template.format(
            location=location, 
            forecast_data_json=forecast_data_json_str
        )
        # System prompt is assumed to be already formatted with language_name by the caller (TelegramHandlerService)
            
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        
        logger.info(f"Requesting LLM to compile weather forecast for {location} in {lang_code}")
        # Use general model for compilation, or configure a specific one if desired
        compilation_model_id = config.OPENAI_MODEL_ID 
        compiled_response = await self.llm_services.race_llm_calls(messages) # Pass specific model if needed

        return compiled_response or "I found some weather data, but couldn't summarize it for you right now."


    async def compile_news_response(self, 
                                    articles_structured: List[Dict[str, Any]], 
                                    topic: Optional[str], 
                                    lang_code: str, # For logging and potentially Babel
                                    system_prompt: str, # Already localized and formatted by caller
                                    user_prompt_template: str) -> str:
        
        articles_json_str = json.dumps(articles_structured, indent=2, ensure_ascii=False)
        display_topic = topic if topic else "general interest"
            
        user_prompt = user_prompt_template.format(
            topic=display_topic, 
            articles_json=articles_json_str
        )
        # System prompt is assumed to be already formatted with language_name and topic by the caller
            
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        logger.info(f"Requesting LLM to compile news for topic '{display_topic}' in {lang_code}")
        # Use general model for compilation
        compilation_model_id = config.OPENAI_MODEL_ID
        compiled_response = await self.llm_services.race_llm_calls(messages) # Pass specific model if needed

        return compiled_response or "I found some news articles, but couldn't summarize them for you at the moment."

    async def get_orchestrated_llm_response(
        self, prompt_text: str, chat_id: int, user_id: int, message_id: int,
        context: ContextTypes.DEFAULT_TYPE, lang_code: str,
        system_prompt_override: str, 
        user_search_ambiguous_response_template: str,
        user_search_not_found_response_template: str
    ) -> str:
        logger.info(f"Orchestrating LLM response for: '{prompt_text[:100]}...' in chat {chat_id} (lang: {lang_code})")
        
        history_from_db: List[Dict[str, str]] = []
        profile_context_messages: List[Dict[str, str]] = []
        keyword_context_messages: List[Dict[str, str]] = [] 

        history_from_db = await self.db_manager.get_conversation_history(chat_id, limit=10) # Fetch recent history

        search_term_original = find_user_search_query_in_text(prompt_text) 
        
        if search_term_original:
            logger.info(f"User profile query detected for term: '{search_term_original}'")
            matched_profiles_data = await self.db_manager.find_user_profiles_by_name_variation(search_term_original)

            if len(matched_profiles_data) == 1:
                profile_data = matched_profiles_data[0]
                target_user_id_found = profile_data.get("UserID") 
                user_identifier = profile_data.get("FirstName") or profile_data.get("Username") or f"User ID {target_user_id_found}"

                if profile_data.get("Notes"): 
                    profile_context_messages.append({
                        "role": "system",
                        "content": f"Important Context (User Dossier) for '{user_identifier}':\n---\n{profile_data['Notes']}\n---"
                    })
                
                if target_user_id_found: 
                    user_specific_messages = await self.db_manager.get_user_messages_from_chat_log(target_user_id_found, chat_id, limit=5)
                    if user_specific_messages:
                        valid_user_messages = [msg for msg in user_specific_messages if isinstance(msg, str) and msg.strip()]
                        if valid_user_messages:
                            formatted_msgs = "\n".join([f'- "{msg_text}"' for msg_text in valid_user_messages])
                            keyword_context_messages.append({
                                "role": "system",
                                "content": f"Additional Context (recent raw messages) from '{user_identifier}'. Use with dossier for freshest insights:\n---\n{formatted_msgs}\n---"
                            })
            elif len(matched_profiles_data) > 1:
                user_options = [ f"@{p.get('Username')}" if p.get('Username') else f"{p.get('FirstName') or ''} {p.get('LastName') or ''}".strip() for p in matched_profiles_data ]
                user_options_str = ", ".join(filter(None, user_options))
                # The response string fetching should happen in TelegramHandlerService
                return user_search_ambiguous_response_template.format(user_options=user_options_str)
            else: 
                logger.info(f"No specific user profile found for search term '{search_term_original}'.")
                # Potentially return a "not found" message here if the query was ONLY about a user
                # For now, it will proceed to general LLM call which might still use the search_term_original
        
        # Construct messages for the LLM
        messages_for_api = []
        if system_prompt_override and isinstance(system_prompt_override, str):
            messages_for_api.append({"role": "system", "content": system_prompt_override})
        else:
            logger.error(f"system_prompt_override is invalid or None: {system_prompt_override}. Using a default.")
            messages_for_api.append({"role": "system", "content": "You are a helpful AI assistant."}) # Basic default

        messages_for_api.extend(profile_context_messages) 
        messages_for_api.extend(keyword_context_messages) 
        # Add general conversation history, ensuring not to duplicate if already part of keyword context
        # Simple approach: just add it. LLM should handle some redundancy.
        messages_for_api.extend(history_from_db) 
        
        if prompt_text and isinstance(prompt_text, str):
            messages_for_api.append({"role": "user", "content": prompt_text})
        else: # Should not happen if called from handle_message
            logger.error(f"prompt_text is invalid or None: {prompt_text}. Appending a placeholder.")
            messages_for_api.append({"role": "user", "content": "Please provide a general response."})

        # Context Truncation (simplified)
        MAX_TOTAL_MESSAGES_FOR_LLM = 20 # Example limit
        if len(messages_for_api) > MAX_TOTAL_MESSAGES_FOR_LLM:
            system_msgs = [m for m in messages_for_api if m["role"] == "system"]
            user_assistant_msgs = [m for m in messages_for_api if m["role"] != "system"]
            
            num_user_assistant_to_keep = MAX_TOTAL_MESSAGES_FOR_LLM - len(system_msgs)
            if num_user_assistant_to_keep < 1: # Ensure at least one user/assistant message if possible
                num_user_assistant_to_keep = 1 
            
            messages_for_api = system_msgs + user_assistant_msgs[-num_user_assistant_to_keep:]
            logger.info(f"Message context truncated to {len(messages_for_api)} messages.")

        logger.debug(f"Final messages for API ({len(messages_for_api)}): {json.dumps(messages_for_api, indent=2, ensure_ascii=False)[:1000]}...")

        final_reply = await self.llm_services.race_llm_calls(messages_for_api)

        if not final_reply:
            # This response should ideally come from language_service via the calling handler
            final_reply = "I'm currently unable to get a response from my core AI. Please try again shortly."
            logger.error("All LLM providers failed in race_llm_calls. Final_reply set to fallback.")

        # Save to conversation history (user prompt and bot reply)
        await self.db_manager.save_to_conversation_history(
            chat_id, user_id, message_id, 'user', prompt_text
        )
        if context.bot: 
            await self.db_manager.save_to_conversation_history(
                chat_id, context.bot.id, None, 'assistant', final_reply
            )
        return final_reply



======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/__init__.py ---
======================================================================

# enkibot/utils/__init__.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# This file makes the 'utils' directory a Python package.


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/database.py ---
======================================================================

# enkibot/utils/database.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by

# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Ensure your GPLv3 header is here)

# <<<--- DIAGNOSTIC PRINT IR-1: VERY TOP OF INTENT_RECOGNIZER.PY --- >>>
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
# (Your GPLv3 Header)
# ==================================================================================================
# === EnkiBot Database Utilities ===
# ==================================================================================================
import logging
import pyodbc
from typing import List, Dict, Any, Optional

from enkibot import config # For DB_CONNECTION_STRING

logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self, connection_string: Optional[str]):
        self.connection_string = connection_string
        if not self.connection_string:
            logger.warning("Database connection string is not configured. Database operations will be disabled.")

    def get_db_connection(self) -> Optional[pyodbc.Connection]:
        if not self.connection_string: return None
        try:
            conn = pyodbc.connect(self.connection_string, autocommit=False)
            logger.debug("Database connection established.")
            return conn
        except pyodbc.Error as ex:
            logger.error(f"Database connection error: {ex.args[0] if ex.args else ''} - {ex}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"Unexpected error establishing database connection: {e}", exc_info=True)
            return None

    async def execute_query(self, query: str, params: Optional[tuple] = None, fetch_one: bool = False, fetch_all: bool = False, commit: bool = False):
        if not self.connection_string:
            logger.warning("Query execution skipped: Database not configured.")
            return None if fetch_one or fetch_all else (True if commit else False)
        conn = self.get_db_connection()
        if not conn:
            return None if fetch_one or fetch_all else (True if commit else False)
        try:
            with conn.cursor() as cursor:
                logger.debug(f"Executing query: {query[:150]}... with params: {params}")
                cursor.execute(query, params) if params else cursor.execute(query)
                if commit: conn.commit(); logger.debug("Query committed."); return True
                if fetch_one: row = cursor.fetchone(); logger.debug(f"Query fetch_one: {row}"); return row
                if fetch_all: rows = cursor.fetchall(); logger.debug(f"Query fetch_all count: {len(rows)}"); return rows
            return True
        except pyodbc.Error as ex:
            logger.error(f"DB query error on '{query[:100]}...': {ex}", exc_info=True)
            try: conn.rollback(); logger.info("Transaction rolled back.")
            except pyodbc.Error as rb_ex: logger.error(f"Error during rollback: {rb_ex}", exc_info=True)
            return None if fetch_one or fetch_all else False
        except Exception as e:
            logger.error(f"Unexpected error query execution '{query[:100]}...': {e}", exc_info=True)
            return None if fetch_one or fetch_all else False
        finally:
            if conn: conn.close(); logger.debug("DB connection closed post-exec.")

    async def get_recent_chat_texts(self, chat_id: int, limit: int = 3) -> List[str]:
        if not self.connection_string: return []
        query = """
            SELECT TOP (?) MessageText FROM ChatLog
            WHERE ChatID = ? AND MessageText IS NOT NULL AND RTRIM(LTRIM(MessageText)) != ''
            ORDER BY Timestamp DESC
        """
        actual_limit = max(1, limit) 
        rows = await self.execute_query(query, (actual_limit, chat_id), fetch_all=True)
        return [row.MessageText for row in reversed(rows) if row.MessageText] if rows else []

    async def log_chat_message_and_upsert_user(
        self, chat_id: int, user_id: int, username: Optional[str],
        first_name: Optional[str], last_name: Optional[str],
        message_id: int, message_text: str, preferred_language: Optional[str] = None
    ) -> Optional[str]:
        if not self.connection_string: return None
        upsert_user_sql = """
            MERGE UserProfiles AS t
            USING (VALUES(?,?,?,?,GETDATE(),?)) AS s(UserID,Username,FirstName,LastName,LastSeen,PreferredLanguage)
            ON t.UserID = s.UserID
            WHEN MATCHED THEN
                UPDATE SET Username=s.Username, FirstName=s.FirstName, LastName=s.LastName, LastSeen=s.LastSeen, MessageCount=ISNULL(t.MessageCount,0)+1, PreferredLanguage=ISNULL(s.PreferredLanguage, t.PreferredLanguage)
            WHEN NOT MATCHED THEN
                INSERT(UserID,Username,FirstName,LastName,LastSeen,MessageCount,ProfileLastUpdated, PreferredLanguage)
                VALUES(s.UserID,s.Username,s.FirstName,s.LastName,s.LastSeen,1,GETDATE(),s.PreferredLanguage)
            OUTPUT $action AS Action;
        """
        user_params = (user_id, username, first_name, last_name, preferred_language)
        chat_log_sql = "INSERT INTO ChatLog (ChatID, UserID, Username, FirstName, MessageID, MessageText, Timestamp) VALUES (?, ?, ?, ?, ?, ?, GETDATE())"
        chat_log_params = (chat_id, user_id, username, first_name, message_id, message_text)
        conn = self.get_db_connection()
        if not conn: return None
        action_taken = None
        try:
            with conn.cursor() as cursor:
                cursor.execute(upsert_user_sql, user_params)
                row = cursor.fetchone();
                if row: action_taken = row.Action
                cursor.execute(chat_log_sql, chat_log_params)
            conn.commit()
            logger.info(f"Logged message for user {user_id}. Profile action: {action_taken}")
            return action_taken
        except pyodbc.Error as ex:
            logger.error(f"DB error logging/upserting user {user_id}: {ex}", exc_info=True); conn.rollback()
        finally:
            if conn: conn.close()
        return None # Ensure a return path if try fails before commit

    async def get_conversation_history(self, chat_id: int, limit: int = 20) -> List[Dict[str, str]]:
        query = "SELECT TOP (?) Role, Content FROM ConversationHistory WHERE ChatID = ? ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, chat_id), fetch_all=True)
        return [{"role": row.Role.lower(), "content": row.Content} for row in reversed(rows)] if rows else []

    async def save_to_conversation_history(self, chat_id: int, entity_id: int, message_id_telegram: Optional[int], role: str, content: str):
        query = "INSERT INTO ConversationHistory (ChatID, UserID, MessageID, Role, Content, Timestamp) VALUES (?, ?, ?, ?, ?, GETDATE())"
        await self.execute_query(query, (chat_id, entity_id, message_id_telegram, role, content), commit=True)

    async def get_user_profile_notes(self, user_id: int) -> Optional[str]:
        row = await self.execute_query("SELECT Notes FROM UserProfiles WHERE UserID = ?", (user_id,), fetch_one=True)
        return row.Notes if row and row.Notes else None

    async def update_user_profile_notes(self, user_id: int, notes: str):
        await self.execute_query("UPDATE UserProfiles SET Notes = ?, ProfileLastUpdated = GETDATE() WHERE UserID = ?", (notes, user_id), commit=True)
        logger.info(f"Profile notes updated for user {user_id}.")

    async def save_user_name_variations(self, user_id: int, variations: List[str]):
        if not self.connection_string or not variations: return
        sql_merge = """
            MERGE INTO UserNameVariations AS t USING (SELECT ? AS UserID, ? AS NameVariation) AS s
            ON (t.UserID = s.UserID AND t.NameVariation = s.NameVariation)
            WHEN NOT MATCHED THEN INSERT (UserID, NameVariation) VALUES (s.UserID, s.NameVariation);
        """
        conn = self.get_db_connection()
        if not conn: return
        try:
            with conn.cursor() as cursor:
                params_to_insert = [(user_id, var) for var in variations if var and str(var).strip()]
                if params_to_insert: cursor.executemany(sql_merge, params_to_insert)
            conn.commit()
            logger.info(f"Saved/updated {len(params_to_insert)} name vars for user {user_id}.")
        except pyodbc.Error as ex:
            logger.error(f"DB error saving name vars for user {user_id}: {ex}", exc_info=True); conn.rollback()
        finally:
            if conn: conn.close()
            
    async def find_user_profiles_by_name_variation(self, name_variation_query: str) -> List[Dict[str, Any]]:
        query = """
            SELECT DISTINCT up.UserID, up.FirstName, up.LastName, up.Username, up.Notes
            FROM UserProfiles up JOIN UserNameVariations unv ON up.UserID = unv.UserID
            WHERE unv.NameVariation = ?
        """
        rows = await self.execute_query(query, (name_variation_query.lower(),), fetch_all=True)
        return [{"UserID": r.UserID, "FirstName": r.FirstName, "LastName": r.LastName, "Username": r.Username, "Notes": r.Notes} for r in rows] if rows else []

    async def get_user_messages_from_chat_log(self, user_id: int, chat_id: int, limit: int = 10) -> List[str]: # Kept from previous version
        query = "SELECT TOP (?) MessageText FROM ChatLog WHERE UserID = ? AND ChatID = ? AND MessageText IS NOT NULL AND RTRIM(LTRIM(MessageText)) != '' ORDER BY Timestamp DESC"
        rows = await self.execute_query(query, (limit, user_id, chat_id), fetch_all=True)
        return [row.MessageText for row in rows] if rows else []

def initialize_database(): # This function defines and uses DatabaseManager locally
    if not config.DB_CONNECTION_STRING:
        logger.warning("Cannot initialize database: Connection string not configured.")
        return
    
    db_mngr = DatabaseManager(config.DB_CONNECTION_STRING) 
    conn = db_mngr.get_db_connection() 
    if not conn: 
        logger.error("Failed to connect to database for initialization (initialize_database).")
        return
    
    conn.autocommit = True 
    table_queries = {
        "UserProfiles": "CREATE TABLE UserProfiles (UserID BIGINT PRIMARY KEY, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, LastName NVARCHAR(255) NULL, LastSeen DATETIME2 DEFAULT GETDATE(), MessageCount INT DEFAULT 0, PreferredLanguage NVARCHAR(10) NULL, Notes NVARCHAR(MAX) NULL, ProfileLastUpdated DATETIME2 DEFAULT GETDATE());",
        "UserNameVariations": "CREATE TABLE UserNameVariations (VariationID INT IDENTITY(1,1) PRIMARY KEY, UserID BIGINT NOT NULL, NameVariation NVARCHAR(255) NOT NULL, FOREIGN KEY (UserID) REFERENCES UserProfiles(UserID) ON DELETE CASCADE);",
        "IX_UserNameVariations_UserID_NameVariation": "CREATE UNIQUE INDEX IX_UserNameVariations_UserID_NameVariation ON UserNameVariations (UserID, NameVariation);",
        "ConversationHistory": "CREATE TABLE ConversationHistory (MessageDBID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, MessageID BIGINT NULL, Role NVARCHAR(50) NOT NULL, Content NVARCHAR(MAX) NOT NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ConversationHistory_ChatID_Timestamp": "CREATE INDEX IX_ConversationHistory_ChatID_Timestamp ON ConversationHistory (ChatID, Timestamp DESC);",
        "ChatLog": "CREATE TABLE ChatLog (LogID INT IDENTITY(1,1) PRIMARY KEY, ChatID BIGINT NOT NULL, UserID BIGINT NOT NULL, Username NVARCHAR(255) NULL, FirstName NVARCHAR(255) NULL, MessageID BIGINT NOT NULL, MessageText NVARCHAR(MAX) NULL, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL);",
        "IX_ChatLog_ChatID_Timestamp": "CREATE INDEX IX_ChatLog_ChatID_Timestamp ON ChatLog (ChatID, Timestamp DESC);",
        "IX_ChatLog_UserID": "CREATE INDEX IX_ChatLog_UserID ON ChatLog (UserID);",
        "ErrorLog": "CREATE TABLE ErrorLog (ErrorID INT IDENTITY(1,1) PRIMARY KEY, Timestamp DATETIME2 DEFAULT GETDATE() NOT NULL, LogLevel NVARCHAR(50) NOT NULL, LoggerName NVARCHAR(255) NULL, ModuleName NVARCHAR(255) NULL, FunctionName NVARCHAR(255) NULL, LineNumber INT NULL, ErrorMessage NVARCHAR(MAX) NOT NULL, ExceptionInfo NVARCHAR(MAX) NULL);",
        "IX_ErrorLog_Timestamp": "CREATE INDEX IX_ErrorLog_Timestamp ON ErrorLog (Timestamp DESC);",
    }
    try:
        with conn.cursor() as cursor:
            logger.info("Initializing database tables...")
            for name, query in table_queries.items():
                is_idx = name.startswith("IX_")
                obj_type = "INDEX" if is_idx else "TABLE"
                obj_name_to_check = name # For tables, this is the table name. For indexes, this is the index name.
                table_for_index = ""
                if is_idx:
                    # Attempt to parse table name from index name, e.g., IX_TableName_Column -> TableName
                    parts = name.split('_')
                    if len(parts) > 1: table_for_index = parts[1] # This is a heuristic
                    else: logger.warning(f"Could not determine table for index {name}"); continue 
                
                check_q = "SELECT name FROM sys.indexes WHERE name = ? AND object_id = OBJECT_ID(?)" if is_idx else "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = ?"
                params_check = (obj_name_to_check, table_for_index) if is_idx else (obj_name_to_check,)
                
                cursor.execute(check_q, params_check)
                if cursor.fetchone(): logger.info(f"{obj_type} '{obj_name_to_check}' already exists.")
                else: logger.info(f"{obj_type} '{obj_name_to_check}' not found. Creating..."); cursor.execute(query); logger.info(f"{obj_type} '{obj_name_to_check}' created.")
            logger.info("Database initialization check complete.")
    except Exception as e: logger.error(f"DB init error: {e}", exc_info=True)
    finally:
        if conn: conn.autocommit = False; conn.close()


======================================================================
--- File: c:/Projects/EnkiBot/EnkiBot/EnkiBot/utils/logging_config.py ---
======================================================================

# enkibot/utils/logging_config.py
# EnkiBot: Advanced Multilingual Telegram AI Assistant
# Copyright (C) 2025 Yael Demedetskaya <yaelkroy@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# ==================================================================================================
# === EnkiBot Logging Configuration ===
# ==================================================================================================
# Sets up application-wide structured logging to both file and console.
# Includes a custom logging handler to write critical errors to the SQL database
# and clears the previous log file on startup.
# ==================================================================================================

import logging
import traceback
import pyodbc
import os # <--- IMPORT OS MODULE
from enkibot import config

def setup_logging():
    """Initializes the logging configuration for the entire application."""

    # --- START: CLEAR PREVIOUS LOG FILE ---
    log_file_name = "bot_activity.log"
    try:
        if os.path.exists(log_file_name):
            os.remove(log_file_name)
            # This print goes to console before logging is fully set up for the bot's logger.
            print(f"INFO: Previous log file '{log_file_name}' removed successfully.")
    except OSError as e:
        # This print also goes to console.
        print(f"WARNING: Error removing previous log file '{log_file_name}': {e}")
    # --- END: CLEAR PREVIOUS LOG FILE ---
    
    # Define a custom handler for logging errors to the database
    class SQLDBLogHandler(logging.Handler):
        """
        A logging handler that writes log records with level ERROR or higher
        to a dedicated table in the SQL Server database.
        """
        def __init__(self):
            super().__init__()
            self.conn = None

        def _get_db_conn_for_logging(self):
            """Establishes a database connection specifically for logging."""
            if not config.DB_CONNECTION_STRING:
                return None
            try:
                # Use autocommit=True for logging to ensure errors are written immediately.
                return pyodbc.connect(config.DB_CONNECTION_STRING, autocommit=True)
            except pyodbc.Error:
                # If the DB is down, we can't log to it. Silently fail for now.
                # A print statement could be added here for immediate feedback if needed.
                # print("WARNING: SQLDBLogHandler could not connect to the database for logging.")
                return None

        def emit(self, record: logging.LogRecord):
            """
            Writes the log record to the ErrorLog table in the database.
            """
            if self.conn is None:
                self.conn = self._get_db_conn_for_logging()

            if self.conn:
                try:
                    msg = self.format(record)
                    exc_info_str = traceback.format_exc() if record.exc_info else None
                    sql = "INSERT INTO ErrorLog (LogLevel, LoggerName, ModuleName, FunctionName, LineNumber, ErrorMessage, ExceptionInfo) VALUES (?, ?, ?, ?, ?, ?, ?)"
                    with self.conn.cursor() as cursor:
                        cursor.execute(sql, record.levelname, record.name, record.module, record.funcName, record.lineno, msg, exc_info_str)
                except pyodbc.Error:
                    # If an error occurs during logging, handle it and sever the connection.
                    self.handleError(record)
                    self.conn = None # Reset connection to be re-established on next emit.
        
        def close(self):
            """Closes the database connection if it's open."""
            if self.conn:
                try:
                    self.conn.close()
                except pyodbc.Error:
                    pass
            super().close()

    # --- Main Logging Configuration ---
    log_level = logging.INFO
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s'
    
    # Configure root logger
    # By using basicConfig with force=True (Python 3.8+), we can reconfigure if needed.
    # For older Python, ensure basicConfig is called only once application-wide.
    # Given setup_logging() is called once from main.py, this should be fine.
    logging.basicConfig(
        format=log_format,
        level=log_level,
        handlers=[
            logging.FileHandler(log_file_name, encoding='utf-8'), # Use variable
            logging.StreamHandler()
        ]
        # force=True # Add this if using Python 3.8+ and re-running setup_logging,
                   # but it should not be necessary with current structure.
    )
    
    module_logger = logging.getLogger(__name__) # Logger for this specific module (logging_config.py)

    # Add the custom DB handler if the database is configured
    if config.DB_CONNECTION_STRING:
        db_log_handler = SQLDBLogHandler()
        db_log_handler.setLevel(logging.ERROR) # Only log ERROR and CRITICAL to DB
        formatter = logging.Formatter(log_format)
        db_log_handler.setFormatter(formatter)
        logging.getLogger().addHandler(db_log_handler) # Add to the root logger
        module_logger.info("Configured logging of ERROR-level messages to the SQL database.")
    else:
        module_logger.warning("Logging to SQL database is NOT configured (DB_CONNECTION_STRING is missing).")


